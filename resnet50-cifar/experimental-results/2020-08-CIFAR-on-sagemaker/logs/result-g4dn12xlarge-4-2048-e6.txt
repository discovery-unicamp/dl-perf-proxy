wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:37
   204800/170498071 [..............................] - ETA: 1:16
  1204224/170498071 [..............................] - ETA: 20s 
  3612672/170498071 [..............................] - ETA: 8s 
  6971392/170498071 [>.............................] - ETA: 5s
 10346496/170498071 [>.............................] - ETA: 4s
 13721600/170498071 [=>............................] - ETA: 3s
 17014784/170498071 [=>............................] - ETA: 3s
 20193280/170498071 [==>...........................] - ETA: 3s
 23453696/170498071 [===>..........................] - ETA: 3s
 26828800/170498071 [===>..........................] - ETA: 2s
 30203904/170498071 [====>.........................] - ETA: 2s
 33546240/170498071 [====>.........................] - ETA: 2s
 36675584/170498071 [=====>........................] - ETA: 2s
 39911424/170498071 [======>.......................] - ETA: 2s
 43163648/170498071 [======>.......................] - ETA: 2s
 46530560/170498071 [=======>......................] - ETA: 2s
 49913856/170498071 [=======>......................] - ETA: 2s
 53141504/170498071 [========>.....................] - ETA: 2s
 56467456/170498071 [========>.....................] - ETA: 2s
 59777024/170498071 [=========>....................] - ETA: 1s
 63152128/170498071 [==========>...................] - ETA: 1s
 66396160/170498071 [==========>...................] - ETA: 1s
 69689344/170498071 [===========>..................] - ETA: 1s
 72966144/170498071 [===========>..................] - ETA: 1s
 76283904/170498071 [============>.................] - ETA: 1s
 79618048/170498071 [=============>................] - ETA: 1s
 82927616/170498071 [=============>................] - ETA: 1s
 86269952/170498071 [==============>...............] - ETA: 1s
 89530368/170498071 [==============>...............] - ETA: 1s
 92872704/170498071 [===============>..............] - ETA: 1s
 95854592/170498071 [===============>..............] - ETA: 1s
 99246080/170498071 [================>.............] - ETA: 1s
102604800/170498071 [=================>............] - ETA: 1s
105963520/170498071 [=================>............] - ETA: 1s
109256704/170498071 [==================>...........] - ETA: 1s
112582656/170498071 [==================>...........] - ETA: 0s
115744768/170498071 [===================>..........] - ETA: 0s
119103488/170498071 [===================>..........] - ETA: 0s
122363904/170498071 [====================>.........] - ETA: 0s
125558784/170498071 [=====================>........] - ETA: 0s
128933888/170498071 [=====================>........] - ETA: 0s
132268032/170498071 [======================>.......] - ETA: 0s
135626752/170498071 [======================>.......] - ETA: 0s
138977280/170498071 [=======================>......] - ETA: 0s
142286848/170498071 [========================>.....] - ETA: 0s
145661952/170498071 [========================>.....] - ETA: 0s
148905984/170498071 [=========================>....] - ETA: 0s
151707648/170498071 [=========================>....] - ETA: 0s
154378240/170498071 [==========================>...] - ETA: 0s
157089792/170498071 [==========================>...] - ETA: 0s
159752192/170498071 [===========================>..] - ETA: 0s
162455552/170498071 [===========================>..] - ETA: 0s
165142528/170498071 [============================>.] - ETA: 0s
167796736/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 1s
 7979008/94765736 [=>............................] - ETA: 0s
 9453568/94765736 [=>............................] - ETA: 0s
15409152/94765736 [===>..........................] - ETA: 0s
18841600/94765736 [====>.........................] - ETA: 0s
20021248/94765736 [=====>........................] - ETA: 1s
28532736/94765736 [========>.....................] - ETA: 0s
37093376/94765736 [==========>...................] - ETA: 0s
37683200/94765736 [==========>...................] - ETA: 0s
40509440/94765736 [===========>..................] - ETA: 0s
47136768/94765736 [=============>................] - ETA: 0s
55410688/94765736 [================>.............] - ETA: 0s
56598528/94765736 [================>.............] - ETA: 0s
59088896/94765736 [=================>............] - ETA: 0s
62439424/94765736 [==================>...........] - ETA: 0s
65986560/94765736 [===================>..........] - ETA: 0s
74604544/94765736 [======================>.......] - ETA: 0s
75440128/94765736 [======================>.......] - ETA: 0s
81313792/94765736 [========================>.....] - ETA: 0s
81870848/94765736 [========================>.....] - ETA: 0s
83525632/94765736 [=========================>....] - ETA: 0s
88129536/94765736 [==========================>...] - ETA: 0s
94281728/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 2s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 18.599600315093994
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1615862988.151614s

Real time: 1615862988.1516464
Epoch 1/5

on_train_batch_begin: 1615862988.948260s

on_train_batch_end: 1615863036.691178s

 2048/50000 [>.............................] - ETA: 18:56 - loss: 17.5275 - accuracy: 1.7738e-04
on_train_batch_begin: 1615863036.691792s

1 step training time: 47.743532s

on_train_batch_end: 1615863036.905300s

 4096/50000 [=>............................] - ETA: 9:06 - loss: 15.0528 - accuracy: 2.3222e-04 
on_train_batch_begin: 1615863036.905603s

2 step training time: 0.213811s

on_train_batch_end: 1615863037.119099s

 6144/50000 [==>...........................] - ETA: 5:49 - loss: 13.1643 - accuracy: 7.1875e-04
on_train_batch_begin: 1615863037.119409s

3 step training time: 0.213806s

on_train_batch_end: 1615863037.329812s

 8192/50000 [===>..........................] - ETA: 4:10 - loss: 11.9383 - accuracy: 0.0023    
on_train_batch_begin: 1615863037.330098s

4 step training time: 0.210689s

on_train_batch_end: 1615863037.539093s

10240/50000 [=====>........................] - ETA: 3:11 - loss: 11.1293 - accuracy: 0.0047
on_train_batch_begin: 1615863037.539379s

5 step training time: 0.209280s

on_train_batch_end: 1615863037.747569s

12288/50000 [======>.......................] - ETA: 2:32 - loss: 10.5414 - accuracy: 0.0076
on_train_batch_begin: 1615863037.747876s

6 step training time: 0.208497s

on_train_batch_end: 1615863037.958131s

14336/50000 [=======>......................] - ETA: 2:03 - loss: 10.0870 - accuracy: 0.0113
on_train_batch_begin: 1615863037.958433s

7 step training time: 0.210557s

on_train_batch_end: 1615863038.168305s

16384/50000 [========>.....................] - ETA: 1:42 - loss: 9.7269 - accuracy: 0.0154 
on_train_batch_begin: 1615863038.168604s

8 step training time: 0.210171s

on_train_batch_end: 1615863038.377342s

18432/50000 [==========>...................] - ETA: 1:26 - loss: 9.4076 - accuracy: 0.0197
on_train_batch_begin: 1615863038.377638s

9 step training time: 0.209033s

on_train_batch_end: 1615863038.590023s

20480/50000 [===========>..................] - ETA: 1:12 - loss: 9.1514 - accuracy: 0.0233
on_train_batch_begin: 1615863038.590312s

10 step training time: 0.212674s

on_train_batch_end: 1615863038.799529s

22528/50000 [============>.................] - ETA: 1:01 - loss: 8.9399 - accuracy: 0.0275
on_train_batch_begin: 1615863038.799820s

11 step training time: 0.209508s

on_train_batch_end: 1615863039.011057s

24576/50000 [=============>................] - ETA: 52s - loss: 8.7512 - accuracy: 0.0306 
on_train_batch_begin: 1615863039.011356s

12 step training time: 0.211536s

on_train_batch_end: 1615863039.224318s

26624/50000 [==============>...............] - ETA: 44s - loss: 8.5812 - accuracy: 0.0337
on_train_batch_begin: 1615863039.224610s

13 step training time: 0.213254s

on_train_batch_end: 1615863039.434991s

28672/50000 [================>.............] - ETA: 38s - loss: 8.4229 - accuracy: 0.0364
on_train_batch_begin: 1615863039.435274s

14 step training time: 0.210664s

on_train_batch_end: 1615863039.645036s

30720/50000 [=================>............] - ETA: 32s - loss: 8.2807 - accuracy: 0.0390
on_train_batch_begin: 1615863039.645323s

15 step training time: 0.210049s

on_train_batch_end: 1615863039.857427s

32768/50000 [==================>...........] - ETA: 27s - loss: 8.1549 - accuracy: 0.0416
on_train_batch_begin: 1615863039.857743s

16 step training time: 0.212420s

on_train_batch_end: 1615863040.068065s

34816/50000 [===================>..........] - ETA: 22s - loss: 8.0349 - accuracy: 0.0435
on_train_batch_begin: 1615863040.068365s

17 step training time: 0.210622s

on_train_batch_end: 1615863040.279009s

36864/50000 [=====================>........] - ETA: 18s - loss: 7.9177 - accuracy: 0.0452
on_train_batch_begin: 1615863040.279306s

18 step training time: 0.210940s

on_train_batch_end: 1615863040.490914s

38912/50000 [======================>.......] - ETA: 14s - loss: 7.8173 - accuracy: 0.0467
on_train_batch_begin: 1615863040.491194s

19 step training time: 0.211888s

on_train_batch_end: 1615863040.701052s

40960/50000 [=======================>......] - ETA: 11s - loss: 7.7139 - accuracy: 0.0480
on_train_batch_begin: 1615863040.701339s

20 step training time: 0.210145s

on_train_batch_end: 1615863040.913867s

43008/50000 [========================>.....] - ETA: 8s - loss: 7.6176 - accuracy: 0.0491 
on_train_batch_begin: 1615863040.914162s

21 step training time: 0.212823s

on_train_batch_end: 1615863041.124217s

45056/50000 [==========================>...] - ETA: 5s - loss: 7.5233 - accuracy: 0.0500
on_train_batch_begin: 1615863041.124505s

22 step training time: 0.210343s

on_train_batch_end: 1615863041.334773s

47104/50000 [===========================>..] - ETA: 3s - loss: 7.4278 - accuracy: 0.0506
on_train_batch_begin: 1615863041.335068s

23 step training time: 0.210563s

on_train_batch_end: 1615863041.544047s

49152/50000 [============================>.] - ETA: 0s - loss: 7.3359 - accuracy: 0.0511
on_train_batch_begin: 1615863041.544329s

24 step training time: 0.209261s

on_train_batch_end: 1615863044.179111s

on_test_batch_begin: 1615863044.416554s

25 step training time: 2.872225s

on_epoch_end: 1615863049.846043s

Validation time: 5.429474s

Real time: 1615863049.846043s

Epoch time: 61.69441556930542s

50000/50000 [==============================] - 62s 1ms/sample - loss: 7.2933 - accuracy: 0.0512 - val_loss: 23.7690 - val_accuracy: 0.1001

on_epoch_begin: 1615863049.846231s

Real time: 1615863049.8462358
Epoch 2/5

on_train_batch_begin: 1615863049.850641s

on_train_batch_end: 1615863050.060545s

 2048/50000 [>.............................] - ETA: 5s - loss: 4.8008 - accuracy: 0.0647
on_train_batch_begin: 1615863050.060828s

1 step training time: 0.210187s

on_train_batch_end: 1615863050.271522s

 4096/50000 [=>............................] - ETA: 4s - loss: 4.7214 - accuracy: 0.0659
on_train_batch_begin: 1615863050.271808s

2 step training time: 0.210980s

on_train_batch_end: 1615863050.482723s

 6144/50000 [==>...........................] - ETA: 4s - loss: 4.6431 - accuracy: 0.0665
on_train_batch_begin: 1615863050.483034s

3 step training time: 0.211226s

on_train_batch_end: 1615863050.695778s

 8192/50000 [===>..........................] - ETA: 4s - loss: 4.5601 - accuracy: 0.0683
on_train_batch_begin: 1615863050.696058s

4 step training time: 0.213024s

on_train_batch_end: 1615863050.908459s

10240/50000 [=====>........................] - ETA: 4s - loss: 4.4445 - accuracy: 0.0700
on_train_batch_begin: 1615863050.908744s

5 step training time: 0.212686s

on_train_batch_end: 1615863051.119667s

12288/50000 [======>.......................] - ETA: 3s - loss: 4.3184 - accuracy: 0.0715
on_train_batch_begin: 1615863051.119949s

6 step training time: 0.211205s

on_train_batch_end: 1615863051.331141s

14336/50000 [=======>......................] - ETA: 3s - loss: 4.2035 - accuracy: 0.0730
on_train_batch_begin: 1615863051.331420s

7 step training time: 0.211471s

on_train_batch_end: 1615863051.540526s

16384/50000 [========>.....................] - ETA: 3s - loss: 4.0975 - accuracy: 0.0746
on_train_batch_begin: 1615863051.540822s

8 step training time: 0.209402s

on_train_batch_end: 1615863051.752275s

18432/50000 [==========>...................] - ETA: 3s - loss: 3.9969 - accuracy: 0.0762
on_train_batch_begin: 1615863051.752560s

9 step training time: 0.211738s

on_train_batch_end: 1615863051.966383s

20480/50000 [===========>..................] - ETA: 3s - loss: 3.9091 - accuracy: 0.0776
on_train_batch_begin: 1615863051.966697s

10 step training time: 0.214137s

on_train_batch_end: 1615863052.178159s

22528/50000 [============>.................] - ETA: 2s - loss: 3.8263 - accuracy: 0.0790
on_train_batch_begin: 1615863052.178463s

11 step training time: 0.211767s

on_train_batch_end: 1615863052.392319s

24576/50000 [=============>................] - ETA: 2s - loss: 3.7413 - accuracy: 0.0804
on_train_batch_begin: 1615863052.392613s

12 step training time: 0.214149s

on_train_batch_end: 1615863052.604571s

26624/50000 [==============>...............] - ETA: 2s - loss: 3.6743 - accuracy: 0.0816
on_train_batch_begin: 1615863052.604855s

13 step training time: 0.212242s

on_train_batch_end: 1615863052.818218s

28672/50000 [================>.............] - ETA: 2s - loss: 3.6003 - accuracy: 0.0828
on_train_batch_begin: 1615863052.818501s

14 step training time: 0.213646s

on_train_batch_end: 1615863053.029957s

30720/50000 [=================>............] - ETA: 1s - loss: 3.5344 - accuracy: 0.0837
on_train_batch_begin: 1615863053.030245s

15 step training time: 0.211743s

on_train_batch_end: 1615863053.243263s

32768/50000 [==================>...........] - ETA: 1s - loss: 3.4720 - accuracy: 0.0847
on_train_batch_begin: 1615863053.243547s

16 step training time: 0.213303s

on_train_batch_end: 1615863053.455267s

34816/50000 [===================>..........] - ETA: 1s - loss: 3.4229 - accuracy: 0.0855
on_train_batch_begin: 1615863053.455553s

17 step training time: 0.212006s

on_train_batch_end: 1615863053.667928s

36864/50000 [=====================>........] - ETA: 1s - loss: 3.3783 - accuracy: 0.0863
on_train_batch_begin: 1615863053.668206s

18 step training time: 0.212653s

on_train_batch_end: 1615863053.879877s

38912/50000 [======================>.......] - ETA: 1s - loss: 3.3320 - accuracy: 0.0870
on_train_batch_begin: 1615863053.880158s

19 step training time: 0.211952s

on_train_batch_end: 1615863054.093006s

40960/50000 [=======================>......] - ETA: 0s - loss: 3.2932 - accuracy: 0.0877
on_train_batch_begin: 1615863054.093287s

20 step training time: 0.213129s

on_train_batch_end: 1615863054.305276s

43008/50000 [========================>.....] - ETA: 0s - loss: 3.2497 - accuracy: 0.0883
on_train_batch_begin: 1615863054.305560s

21 step training time: 0.212273s

on_train_batch_end: 1615863054.516182s

45056/50000 [==========================>...] - ETA: 0s - loss: 3.2112 - accuracy: 0.0888
on_train_batch_begin: 1615863054.516468s

22 step training time: 0.210908s

on_train_batch_end: 1615863054.726375s

47104/50000 [===========================>..] - ETA: 0s - loss: 3.1774 - accuracy: 0.0893
on_train_batch_begin: 1615863054.726692s

23 step training time: 0.210225s

on_train_batch_end: 1615863054.938484s

49152/50000 [============================>.] - ETA: 0s - loss: 3.1443 - accuracy: 0.0897
on_train_batch_begin: 1615863054.938772s

24 step training time: 0.212080s

on_train_batch_end: 1615863055.060533s

on_test_batch_begin: 1615863055.152510s

25 step training time: 0.213738s

on_epoch_end: 1615863055.447611s

Validation time: 0.295085s

Real time: 1615863055.447611s

Epoch time: 5.601392507553101s

50000/50000 [==============================] - 6s 112us/sample - loss: 3.1322 - accuracy: 0.0898 - val_loss: 8.5941 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615863055.447819s

Real time: 1615863055.4478245
Epoch 3/5

on_train_batch_begin: 1615863055.452199s

on_train_batch_end: 1615863055.663841s

 2048/50000 [>.............................] - ETA: 5s - loss: 2.4269 - accuracy: 0.0999
on_train_batch_begin: 1615863055.664128s

1 step training time: 0.211929s

on_train_batch_end: 1615863055.875872s

 4096/50000 [=>............................] - ETA: 4s - loss: 2.4265 - accuracy: 0.1003
on_train_batch_begin: 1615863055.876170s

2 step training time: 0.212042s

on_train_batch_end: 1615863056.087678s

 6144/50000 [==>...........................] - ETA: 4s - loss: 2.4666 - accuracy: 0.1001
on_train_batch_begin: 1615863056.088009s

3 step training time: 0.211839s

on_train_batch_end: 1615863056.299802s

 8192/50000 [===>..........................] - ETA: 4s - loss: 2.4841 - accuracy: 0.1002
on_train_batch_begin: 1615863056.300086s

4 step training time: 0.212077s

on_train_batch_end: 1615863056.512610s

10240/50000 [=====>........................] - ETA: 4s - loss: 2.4838 - accuracy: 0.1003
on_train_batch_begin: 1615863056.512893s

5 step training time: 0.212807s

on_train_batch_end: 1615863056.725443s

12288/50000 [======>.......................] - ETA: 3s - loss: 2.4571 - accuracy: 0.1003
on_train_batch_begin: 1615863056.725764s

6 step training time: 0.212871s

on_train_batch_end: 1615863056.936997s

14336/50000 [=======>......................] - ETA: 3s - loss: 2.4484 - accuracy: 0.1002
on_train_batch_begin: 1615863056.937289s

7 step training time: 0.211525s

on_train_batch_end: 1615863057.147922s

16384/50000 [========>.....................] - ETA: 3s - loss: 2.4350 - accuracy: 0.1003
on_train_batch_begin: 1615863057.148207s

8 step training time: 0.210918s

on_train_batch_end: 1615863057.359328s

18432/50000 [==========>...................] - ETA: 3s - loss: 2.4230 - accuracy: 0.1002
on_train_batch_begin: 1615863057.359624s

9 step training time: 0.211416s

on_train_batch_end: 1615863057.577286s

20480/50000 [===========>..................] - ETA: 3s - loss: 2.4186 - accuracy: 0.1002
on_train_batch_begin: 1615863057.577574s

10 step training time: 0.217951s

on_train_batch_end: 1615863057.792321s

22528/50000 [============>.................] - ETA: 2s - loss: 2.4078 - accuracy: 0.1002
on_train_batch_begin: 1615863057.792606s

11 step training time: 0.215031s

on_train_batch_end: 1615863058.005891s

24576/50000 [=============>................] - ETA: 2s - loss: 2.3922 - accuracy: 0.1002
on_train_batch_begin: 1615863058.006176s

12 step training time: 0.213570s

on_train_batch_end: 1615863058.218326s

26624/50000 [==============>...............] - ETA: 2s - loss: 2.3763 - accuracy: 0.1003
on_train_batch_begin: 1615863058.218615s

13 step training time: 0.212440s

on_train_batch_end: 1615863058.430134s

28672/50000 [================>.............] - ETA: 2s - loss: 2.3711 - accuracy: 0.1003
on_train_batch_begin: 1615863058.430415s

14 step training time: 0.211799s

on_train_batch_end: 1615863058.645108s

30720/50000 [=================>............] - ETA: 2s - loss: 2.3597 - accuracy: 0.1003
on_train_batch_begin: 1615863058.645399s

15 step training time: 0.214985s

on_train_batch_end: 1615863058.858355s

32768/50000 [==================>...........] - ETA: 1s - loss: 2.3461 - accuracy: 0.1003
on_train_batch_begin: 1615863058.858643s

16 step training time: 0.213243s

on_train_batch_end: 1615863059.070833s

34816/50000 [===================>..........] - ETA: 1s - loss: 2.3344 - accuracy: 0.1003
on_train_batch_begin: 1615863059.071120s

17 step training time: 0.212477s

on_train_batch_end: 1615863059.283939s

36864/50000 [=====================>........] - ETA: 1s - loss: 2.3294 - accuracy: 0.1003
on_train_batch_begin: 1615863059.284224s

18 step training time: 0.213104s

on_train_batch_end: 1615863059.495210s

38912/50000 [======================>.......] - ETA: 1s - loss: 2.3237 - accuracy: 0.1003
on_train_batch_begin: 1615863059.495491s

19 step training time: 0.211267s

on_train_batch_end: 1615863059.708493s

40960/50000 [=======================>......] - ETA: 0s - loss: 2.3215 - accuracy: 0.1003
on_train_batch_begin: 1615863059.708774s

20 step training time: 0.213283s

on_train_batch_end: 1615863059.921410s

43008/50000 [========================>.....] - ETA: 0s - loss: 2.3083 - accuracy: 0.1003
on_train_batch_begin: 1615863059.921714s

21 step training time: 0.212940s

on_train_batch_end: 1615863060.143208s

45056/50000 [==========================>...] - ETA: 0s - loss: 2.3026 - accuracy: 0.1003
on_train_batch_begin: 1615863060.143492s

22 step training time: 0.221778s

on_train_batch_end: 1615863060.356121s

47104/50000 [===========================>..] - ETA: 0s - loss: 2.3009 - accuracy: 0.1003
on_train_batch_begin: 1615863060.356407s

23 step training time: 0.212915s

on_train_batch_end: 1615863060.566665s

49152/50000 [============================>.] - ETA: 0s - loss: 2.2990 - accuracy: 0.1003
on_train_batch_begin: 1615863060.566953s

24 step training time: 0.210546s

on_train_batch_end: 1615863060.690911s

on_test_batch_begin: 1615863060.786345s

25 step training time: 0.219391s

on_epoch_end: 1615863061.080639s

Validation time: 0.294278s

Real time: 1615863061.080639s

Epoch time: 5.632831573486328s

50000/50000 [==============================] - 6s 113us/sample - loss: 2.2969 - accuracy: 0.1003 - val_loss: 7.3585 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615863061.080823s

Real time: 1615863061.0808282
Epoch 4/5

on_train_batch_begin: 1615863061.085138s

on_train_batch_end: 1615863061.299648s

 2048/50000 [>.............................] - ETA: 5s - loss: 2.0955 - accuracy: 0.1002
on_train_batch_begin: 1615863061.299930s

1 step training time: 0.214792s

on_train_batch_end: 1615863061.514057s

 4096/50000 [=>............................] - ETA: 4s - loss: 2.1176 - accuracy: 0.1003
on_train_batch_begin: 1615863061.514337s

2 step training time: 0.214407s

on_train_batch_end: 1615863061.728731s

 6144/50000 [==>...........................] - ETA: 4s - loss: 2.0665 - accuracy: 0.1003
on_train_batch_begin: 1615863061.729002s

3 step training time: 0.214665s

on_train_batch_end: 1615863061.940464s

 8192/50000 [===>..........................] - ETA: 4s - loss: 2.0445 - accuracy: 0.1002
on_train_batch_begin: 1615863061.940746s

4 step training time: 0.211744s

on_train_batch_end: 1615863062.151836s

10240/50000 [=====>........................] - ETA: 4s - loss: 2.0020 - accuracy: 0.1002
on_train_batch_begin: 1615863062.152120s

5 step training time: 0.211374s

on_train_batch_end: 1615863062.363539s

12288/50000 [======>.......................] - ETA: 3s - loss: 1.9790 - accuracy: 0.1003
on_train_batch_begin: 1615863062.363816s

6 step training time: 0.211696s

on_train_batch_end: 1615863062.575712s

14336/50000 [=======>......................] - ETA: 3s - loss: 1.9753 - accuracy: 0.1003
on_train_batch_begin: 1615863062.576011s

7 step training time: 0.212195s

on_train_batch_end: 1615863062.789634s

16384/50000 [========>.....................] - ETA: 3s - loss: 1.9724 - accuracy: 0.1003
on_train_batch_begin: 1615863062.789943s

8 step training time: 0.213933s

on_train_batch_end: 1615863063.004248s

18432/50000 [==========>...................] - ETA: 3s - loss: 1.9631 - accuracy: 0.1003
on_train_batch_begin: 1615863063.004527s

9 step training time: 0.214584s

on_train_batch_end: 1615863063.222197s

20480/50000 [===========>..................] - ETA: 3s - loss: 1.9523 - accuracy: 0.1003
on_train_batch_begin: 1615863063.222483s

10 step training time: 0.217955s

on_train_batch_end: 1615863063.436433s

22528/50000 [============>.................] - ETA: 2s - loss: 1.9238 - accuracy: 0.1003
on_train_batch_begin: 1615863063.436754s

11 step training time: 0.214272s

on_train_batch_end: 1615863063.648071s

24576/50000 [=============>................] - ETA: 2s - loss: 1.9206 - accuracy: 0.1003
on_train_batch_begin: 1615863063.648362s

12 step training time: 0.211608s

on_train_batch_end: 1615863063.859133s

26624/50000 [==============>...............] - ETA: 2s - loss: 1.9194 - accuracy: 0.1003
on_train_batch_begin: 1615863063.859423s

13 step training time: 0.211061s

on_train_batch_end: 1615863064.071888s

28672/50000 [================>.............] - ETA: 2s - loss: 1.9122 - accuracy: 0.1003
on_train_batch_begin: 1615863064.072169s

14 step training time: 0.212746s

on_train_batch_end: 1615863064.283377s

30720/50000 [=================>............] - ETA: 2s - loss: 1.9051 - accuracy: 0.1004
on_train_batch_begin: 1615863064.283663s

15 step training time: 0.211493s

on_train_batch_end: 1615863064.496367s

32768/50000 [==================>...........] - ETA: 1s - loss: 1.8871 - accuracy: 0.1004
on_train_batch_begin: 1615863064.496682s

16 step training time: 0.213019s

on_train_batch_end: 1615863064.709431s

34816/50000 [===================>..........] - ETA: 1s - loss: 1.8739 - accuracy: 0.1004
on_train_batch_begin: 1615863064.709759s

17 step training time: 0.213077s

on_train_batch_end: 1615863064.923444s

36864/50000 [=====================>........] - ETA: 1s - loss: 1.8645 - accuracy: 0.1004
on_train_batch_begin: 1615863064.923728s

18 step training time: 0.213969s

on_train_batch_end: 1615863065.135635s

38912/50000 [======================>.......] - ETA: 1s - loss: 1.8559 - accuracy: 0.1004
on_train_batch_begin: 1615863065.135921s

19 step training time: 0.212194s

on_train_batch_end: 1615863065.348330s

40960/50000 [=======================>......] - ETA: 0s - loss: 1.8415 - accuracy: 0.1004
on_train_batch_begin: 1615863065.348632s

20 step training time: 0.212711s

on_train_batch_end: 1615863065.560415s

43008/50000 [========================>.....] - ETA: 0s - loss: 1.8271 - accuracy: 0.1004
on_train_batch_begin: 1615863065.560701s

21 step training time: 0.212069s

on_train_batch_end: 1615863065.773126s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.8294 - accuracy: 0.1004
on_train_batch_begin: 1615863065.773410s

22 step training time: 0.212709s

on_train_batch_end: 1615863065.985163s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.8194 - accuracy: 0.1004
on_train_batch_begin: 1615863065.985456s

23 step training time: 0.212045s

on_train_batch_end: 1615863066.197868s

49152/50000 [============================>.] - ETA: 0s - loss: 1.8085 - accuracy: 0.1004
on_train_batch_begin: 1615863066.198153s

24 step training time: 0.212697s

on_train_batch_end: 1615863066.322840s

on_test_batch_begin: 1615863066.418370s

25 step training time: 0.220218s

on_epoch_end: 1615863066.737765s

Validation time: 0.319379s

Real time: 1615863066.737765s

Epoch time: 5.656954050064087s

50000/50000 [==============================] - 6s 113us/sample - loss: 1.8076 - accuracy: 0.1004 - val_loss: 6.8752 - val_accuracy: 0.1000

on_epoch_begin: 1615863066.737950s

Real time: 1615863066.7379556
Epoch 5/5

on_train_batch_begin: 1615863066.742490s

on_train_batch_end: 1615863066.957060s

 2048/50000 [>.............................] - ETA: 5s - loss: 1.5791 - accuracy: 0.1005
on_train_batch_begin: 1615863066.957352s

1 step training time: 0.214863s

on_train_batch_end: 1615863067.169405s

 4096/50000 [=>............................] - ETA: 4s - loss: 1.5370 - accuracy: 0.1005
on_train_batch_begin: 1615863067.169710s

2 step training time: 0.212358s

on_train_batch_end: 1615863067.382251s

 6144/50000 [==>...........................] - ETA: 4s - loss: 1.5319 - accuracy: 0.1005
on_train_batch_begin: 1615863067.382549s

3 step training time: 0.212839s

on_train_batch_end: 1615863067.593546s

 8192/50000 [===>..........................] - ETA: 4s - loss: 1.5080 - accuracy: 0.1005
on_train_batch_begin: 1615863067.593859s

4 step training time: 0.211309s

on_train_batch_end: 1615863067.806164s

10240/50000 [=====>........................] - ETA: 4s - loss: 1.5171 - accuracy: 0.1005
on_train_batch_begin: 1615863067.806451s

5 step training time: 0.212592s

on_train_batch_end: 1615863068.018448s

12288/50000 [======>.......................] - ETA: 3s - loss: 1.5174 - accuracy: 0.1005
on_train_batch_begin: 1615863068.018755s

6 step training time: 0.212304s

on_train_batch_end: 1615863068.230912s

14336/50000 [=======>......................] - ETA: 3s - loss: 1.4971 - accuracy: 0.1004
on_train_batch_begin: 1615863068.231193s

7 step training time: 0.212438s

on_train_batch_end: 1615863068.444126s

16384/50000 [========>.....................] - ETA: 3s - loss: 1.4744 - accuracy: 0.1005
on_train_batch_begin: 1615863068.444408s

8 step training time: 0.213214s

on_train_batch_end: 1615863068.656708s

18432/50000 [==========>...................] - ETA: 3s - loss: 1.4709 - accuracy: 0.1005
on_train_batch_begin: 1615863068.656999s

9 step training time: 0.212591s

on_train_batch_end: 1615863068.869360s

20480/50000 [===========>..................] - ETA: 3s - loss: 1.4559 - accuracy: 0.1005
on_train_batch_begin: 1615863068.869645s

10 step training time: 0.212646s

on_train_batch_end: 1615863069.085325s

22528/50000 [============>.................] - ETA: 2s - loss: 1.4369 - accuracy: 0.1005
on_train_batch_begin: 1615863069.085611s

11 step training time: 0.215966s

on_train_batch_end: 1615863069.299298s

24576/50000 [=============>................] - ETA: 2s - loss: 1.4296 - accuracy: 0.1005
on_train_batch_begin: 1615863069.299587s

12 step training time: 0.213976s

on_train_batch_end: 1615863069.513023s

26624/50000 [==============>...............] - ETA: 2s - loss: 1.4183 - accuracy: 0.1005
on_train_batch_begin: 1615863069.513312s

13 step training time: 0.213725s

on_train_batch_end: 1615863069.728936s

28672/50000 [================>.............] - ETA: 2s - loss: 1.4188 - accuracy: 0.1005
on_train_batch_begin: 1615863069.729224s

14 step training time: 0.215912s

on_train_batch_end: 1615863069.942345s

30720/50000 [=================>............] - ETA: 2s - loss: 1.4125 - accuracy: 0.1005
on_train_batch_begin: 1615863069.942642s

15 step training time: 0.213418s

on_train_batch_end: 1615863070.156967s

32768/50000 [==================>...........] - ETA: 1s - loss: 1.4055 - accuracy: 0.1005
on_train_batch_begin: 1615863070.157258s

16 step training time: 0.214615s

on_train_batch_end: 1615863070.371457s

34816/50000 [===================>..........] - ETA: 1s - loss: 1.4039 - accuracy: 0.1005
on_train_batch_begin: 1615863070.371746s

17 step training time: 0.214488s

on_train_batch_end: 1615863070.588017s

36864/50000 [=====================>........] - ETA: 1s - loss: 1.3909 - accuracy: 0.1005
on_train_batch_begin: 1615863070.588325s

18 step training time: 0.216579s

on_train_batch_end: 1615863070.800797s

38912/50000 [======================>.......] - ETA: 1s - loss: 1.3917 - accuracy: 0.1005
on_train_batch_begin: 1615863070.801095s

19 step training time: 0.212770s

on_train_batch_end: 1615863071.014491s

40960/50000 [=======================>......] - ETA: 0s - loss: 1.3859 - accuracy: 0.1005
on_train_batch_begin: 1615863071.014794s

20 step training time: 0.213699s

on_train_batch_end: 1615863071.229263s

43008/50000 [========================>.....] - ETA: 0s - loss: 1.3835 - accuracy: 0.1005
on_train_batch_begin: 1615863071.229541s

21 step training time: 0.214747s

on_train_batch_end: 1615863071.447139s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.3819 - accuracy: 0.1005
on_train_batch_begin: 1615863071.447425s

22 step training time: 0.217884s

on_train_batch_end: 1615863071.659384s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.3757 - accuracy: 0.1005
on_train_batch_begin: 1615863071.659682s

23 step training time: 0.212257s

on_train_batch_end: 1615863071.871819s

49152/50000 [============================>.] - ETA: 0s - loss: 1.3655 - accuracy: 0.1005
on_train_batch_begin: 1615863071.872103s

24 step training time: 0.212421s

on_train_batch_end: 1615863071.996708s

on_test_batch_begin: 1615863072.088301s

25 step training time: 0.216198s

on_epoch_end: 1615863072.400393s

Validation time: 0.312077s

Real time: 1615863072.400393s

Epoch time: 5.662455081939697s

50000/50000 [==============================] - 6s 113us/sample - loss: 1.3668 - accuracy: 0.1005 - val_loss: 7.6130 - val_accuracy: 0.1002
Tempo do fit: 87.72403573989868