wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:49
   221184/170498071 [..............................] - ETA: 1:12
  1155072/170498071 [..............................] - ETA: 21s 
  3842048/170498071 [..............................] - ETA: 8s 
  7159808/170498071 [>.............................] - ETA: 5s
 10461184/170498071 [>.............................] - ETA: 4s
 13778944/170498071 [=>............................] - ETA: 3s
 17080320/170498071 [==>...........................] - ETA: 3s
 20357120/170498071 [==>...........................] - ETA: 3s
 23642112/170498071 [===>..........................] - ETA: 3s
 26845184/170498071 [===>..........................] - ETA: 2s
 29990912/170498071 [====>.........................] - ETA: 2s
 33202176/170498071 [====>.........................] - ETA: 2s
 36380672/170498071 [=====>........................] - ETA: 2s
 38043648/170498071 [=====>........................] - ETA: 2s
 41320448/170498071 [======>.......................] - ETA: 2s
 44630016/170498071 [======>.......................] - ETA: 2s
 47931392/170498071 [=======>......................] - ETA: 2s
 51257344/170498071 [========>.....................] - ETA: 2s
 54116352/170498071 [========>.....................] - ETA: 2s
 57376768/170498071 [=========>....................] - ETA: 2s
 60686336/170498071 [=========>....................] - ETA: 1s
 64004096/170498071 [==========>...................] - ETA: 1s
 67280896/170498071 [==========>...................] - ETA: 1s
 70606848/170498071 [===========>..................] - ETA: 1s
 73891840/170498071 [============>.................] - ETA: 1s
 76931072/170498071 [============>.................] - ETA: 1s
 80240640/170498071 [=============>................] - ETA: 1s
 83369984/170498071 [=============>................] - ETA: 1s
 86564864/170498071 [==============>...............] - ETA: 1s
 89792512/170498071 [==============>...............] - ETA: 1s
 93085696/170498071 [===============>..............] - ETA: 1s
 96370688/170498071 [===============>..............] - ETA: 1s
 99663872/170498071 [================>.............] - ETA: 1s
102965248/170498071 [=================>............] - ETA: 1s
106250240/170498071 [=================>............] - ETA: 1s
109551616/170498071 [==================>...........] - ETA: 1s
112836608/170498071 [==================>...........] - ETA: 0s
116137984/170498071 [===================>..........] - ETA: 0s
119201792/170498071 [===================>..........] - ETA: 0s
122445824/170498071 [====================>.........] - ETA: 0s
125067264/170498071 [=====================>........] - ETA: 0s
127459328/170498071 [=====================>........] - ETA: 0s
129687552/170498071 [=====================>........] - ETA: 0s
131833856/170498071 [======================>.......] - ETA: 0s
133767168/170498071 [======================>.......] - ETA: 0s
135798784/170498071 [======================>.......] - ETA: 0s
137879552/170498071 [=======================>......] - ETA: 0s
140042240/170498071 [=======================>......] - ETA: 0s
141959168/170498071 [=======================>......] - ETA: 0s
143810560/170498071 [========================>.....] - ETA: 0s
145760256/170498071 [========================>.....] - ETA: 0s
147922944/170498071 [=========================>....] - ETA: 0s
150118400/170498071 [=========================>....] - ETA: 0s
152248320/170498071 [=========================>....] - ETA: 0s
154148864/170498071 [==========================>...] - ETA: 0s
156180480/170498071 [==========================>...] - ETA: 0s
158326784/170498071 [==========================>...] - ETA: 0s
160604160/170498071 [===========================>..] - ETA: 0s
162684928/170498071 [===========================>..] - ETA: 0s
164765696/170498071 [===========================>..] - ETA: 0s
166682624/170498071 [============================>.] - ETA: 0s
168558592/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 3s
 6307840/94765736 [>.............................] - ETA: 0s
 9388032/94765736 [=>............................] - ETA: 1s
14114816/94765736 [===>..........................] - ETA: 1s
18841600/94765736 [====>.........................] - ETA: 1s
21200896/94765736 [=====>........................] - ETA: 1s
28286976/94765736 [=======>......................] - ETA: 1s
35176448/94765736 [==========>...................] - ETA: 1s
37683200/94765736 [==========>...................] - ETA: 1s
44769280/94765736 [=============>................] - ETA: 0s
47136768/94765736 [=============>................] - ETA: 1s
53731328/94765736 [================>.............] - ETA: 0s
56598528/94765736 [================>.............] - ETA: 0s
62095360/94765736 [==================>...........] - ETA: 0s
65986560/94765736 [===================>..........] - ETA: 0s
71892992/94765736 [=====================>........] - ETA: 0s
75440128/94765736 [======================>.......] - ETA: 0s
81256448/94765736 [========================>.....] - ETA: 0s
86556672/94765736 [==========================>...] - ETA: 0s
94773248/94765736 [==============================] - 2s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 18.20851421356201
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1609164166.320057s

Real time: 1609164166.3200796
Epoch 1/5

on_train_batch_begin: 1609164167.212815s

on_train_batch_end: 1609164195.309460s

 2048/50000 [>.............................] - ETA: 11:18 - loss: 17.7587 - accuracy: 3.6526e-04
on_train_batch_begin: 1609164195.310222s

1 step training time: 28.097407s

on_train_batch_end: 1609164196.708398s

 4096/50000 [=>............................] - ETA: 5:40 - loss: 13.9684 - accuracy: 2.9552e-04 
on_train_batch_begin: 1609164196.708841s

2 step training time: 1.398618s

on_train_batch_end: 1609164198.098775s

 6144/50000 [==>...........................] - ETA: 3:46 - loss: 12.0888 - accuracy: 0.0011    
on_train_batch_begin: 1609164198.099177s

3 step training time: 1.390336s

on_train_batch_end: 1609164199.496328s

 8192/50000 [===>..........................] - ETA: 2:49 - loss: 11.1079 - accuracy: 0.0018
on_train_batch_begin: 1609164199.496750s

4 step training time: 1.397573s

on_train_batch_end: 1609164200.897770s

10240/50000 [=====>........................] - ETA: 2:14 - loss: 10.4649 - accuracy: 0.0043
on_train_batch_begin: 1609164200.898169s

5 step training time: 1.401419s

on_train_batch_end: 1609164202.284696s

12288/50000 [======>.......................] - ETA: 1:50 - loss: 10.0138 - accuracy: 0.0070
on_train_batch_begin: 1609164202.285094s

6 step training time: 1.386926s

on_train_batch_end: 1609164203.666534s

14336/50000 [=======>......................] - ETA: 1:32 - loss: 9.6860 - accuracy: 0.0091 
on_train_batch_begin: 1609164203.666926s

7 step training time: 1.381831s

on_train_batch_end: 1609164205.072019s

16384/50000 [========>.....................] - ETA: 1:19 - loss: 9.4202 - accuracy: 0.0118
on_train_batch_begin: 1609164205.072415s

8 step training time: 1.405489s

on_train_batch_end: 1609164206.473713s

18432/50000 [==========>...................] - ETA: 1:08 - loss: 9.1971 - accuracy: 0.0156
on_train_batch_begin: 1609164206.474121s

9 step training time: 1.401706s

on_train_batch_end: 1609164207.860573s

20480/50000 [===========>..................] - ETA: 59s - loss: 9.0115 - accuracy: 0.0199 
on_train_batch_begin: 1609164207.861011s

10 step training time: 1.386890s

on_train_batch_end: 1609164209.251918s

22528/50000 [============>.................] - ETA: 52s - loss: 8.8427 - accuracy: 0.0244
on_train_batch_begin: 1609164209.252319s

11 step training time: 1.391308s

on_train_batch_end: 1609164210.646199s

24576/50000 [=============>................] - ETA: 45s - loss: 8.7050 - accuracy: 0.0291
on_train_batch_begin: 1609164210.646590s

12 step training time: 1.394270s

on_train_batch_end: 1609164212.047510s

26624/50000 [==============>...............] - ETA: 40s - loss: 8.5792 - accuracy: 0.0326
on_train_batch_begin: 1609164212.047901s

13 step training time: 1.401312s

on_train_batch_end: 1609164213.441597s

28672/50000 [================>.............] - ETA: 35s - loss: 8.4629 - accuracy: 0.0357
on_train_batch_begin: 1609164213.442000s

14 step training time: 1.394099s

on_train_batch_end: 1609164214.833524s

30720/50000 [=================>............] - ETA: 30s - loss: 8.3592 - accuracy: 0.0382
on_train_batch_begin: 1609164214.833931s

15 step training time: 1.391931s

on_train_batch_end: 1609164216.228833s

32768/50000 [==================>...........] - ETA: 26s - loss: 8.2569 - accuracy: 0.0407
on_train_batch_begin: 1609164216.229228s

16 step training time: 1.395297s

on_train_batch_end: 1609164217.622315s

34816/50000 [===================>..........] - ETA: 22s - loss: 8.1610 - accuracy: 0.0427
on_train_batch_begin: 1609164217.622706s

17 step training time: 1.393478s

on_train_batch_end: 1609164219.026280s

36864/50000 [=====================>........] - ETA: 18s - loss: 8.0702 - accuracy: 0.0442
on_train_batch_begin: 1609164219.026679s

18 step training time: 1.403972s

on_train_batch_end: 1609164220.431350s

38912/50000 [======================>.......] - ETA: 15s - loss: 7.9837 - accuracy: 0.0459
on_train_batch_begin: 1609164220.431765s

19 step training time: 1.405087s

on_train_batch_end: 1609164221.827563s

40960/50000 [=======================>......] - ETA: 12s - loss: 7.9072 - accuracy: 0.0470
on_train_batch_begin: 1609164221.827970s

20 step training time: 1.396204s

on_train_batch_end: 1609164223.225459s

43008/50000 [========================>.....] - ETA: 9s - loss: 7.8318 - accuracy: 0.0482 
on_train_batch_begin: 1609164223.225852s

21 step training time: 1.397883s

on_train_batch_end: 1609164224.629091s

45056/50000 [==========================>...] - ETA: 6s - loss: 7.7608 - accuracy: 0.0497
on_train_batch_begin: 1609164224.629483s

22 step training time: 1.403631s

on_train_batch_end: 1609164226.039833s

47104/50000 [===========================>..] - ETA: 3s - loss: 7.6945 - accuracy: 0.0509
on_train_batch_begin: 1609164226.040246s

23 step training time: 1.410763s

on_train_batch_end: 1609164227.443313s

49152/50000 [============================>.] - ETA: 1s - loss: 7.6261 - accuracy: 0.0524
on_train_batch_begin: 1609164227.443703s

24 step training time: 1.403456s

on_train_batch_end: 1609164235.348753s

on_test_batch_begin: 1609164235.567734s

25 step training time: 8.124031s

on_epoch_end: 1609164242.262086s

Validation time: 6.694332s

Real time: 1609164242.262086s

Epoch time: 75.94202828407288s

50000/50000 [==============================] - 76s 2ms/sample - loss: 7.5967 - accuracy: 0.0526 - val_loss: 3600.2585 - val_accuracy: 0.0000e+00

on_epoch_begin: 1609164242.262402s

Real time: 1609164242.2624152
Epoch 2/5

on_train_batch_begin: 1609164242.266746s

on_train_batch_end: 1609164243.681398s

 2048/50000 [>.............................] - ETA: 33s - loss: 5.8887 - accuracy: 0.0770
on_train_batch_begin: 1609164243.681813s

1 step training time: 1.415067s

on_train_batch_end: 1609164245.088454s

 4096/50000 [=>............................] - ETA: 31s - loss: 5.9315 - accuracy: 0.0767
on_train_batch_begin: 1609164245.088873s

2 step training time: 1.407060s

on_train_batch_end: 1609164246.499287s

 6144/50000 [==>...........................] - ETA: 30s - loss: 5.8940 - accuracy: 0.0767
on_train_batch_begin: 1609164246.499689s

3 step training time: 1.410816s

on_train_batch_end: 1609164247.902845s

 8192/50000 [===>..........................] - ETA: 28s - loss: 5.8466 - accuracy: 0.0791
on_train_batch_begin: 1609164247.903254s

4 step training time: 1.403565s

on_train_batch_end: 1609164249.309588s

10240/50000 [=====>........................] - ETA: 27s - loss: 5.8039 - accuracy: 0.0797
on_train_batch_begin: 1609164249.309988s

5 step training time: 1.406734s

on_train_batch_end: 1609164250.720145s

12288/50000 [======>.......................] - ETA: 25s - loss: 5.7706 - accuracy: 0.0803
on_train_batch_begin: 1609164250.720542s

6 step training time: 1.410554s

on_train_batch_end: 1609164252.134899s

14336/50000 [=======>......................] - ETA: 24s - loss: 5.7385 - accuracy: 0.0808
on_train_batch_begin: 1609164252.135440s

7 step training time: 1.414898s

on_train_batch_end: 1609164253.560205s

16384/50000 [========>.....................] - ETA: 23s - loss: 5.7149 - accuracy: 0.0817
on_train_batch_begin: 1609164253.560596s

8 step training time: 1.425156s

on_train_batch_end: 1609164254.973214s

18432/50000 [==========>...................] - ETA: 21s - loss: 5.6661 - accuracy: 0.0825
on_train_batch_begin: 1609164254.973631s

9 step training time: 1.413035s

on_train_batch_end: 1609164256.373773s

20480/50000 [===========>..................] - ETA: 20s - loss: 5.6330 - accuracy: 0.0827
on_train_batch_begin: 1609164256.374174s

10 step training time: 1.400543s

on_train_batch_end: 1609164257.780856s

22528/50000 [============>.................] - ETA: 18s - loss: 5.5982 - accuracy: 0.0835
on_train_batch_begin: 1609164257.781283s

11 step training time: 1.407109s

on_train_batch_end: 1609164259.192703s

24576/50000 [=============>................] - ETA: 17s - loss: 5.5590 - accuracy: 0.0842
on_train_batch_begin: 1609164259.193129s

12 step training time: 1.411846s

on_train_batch_end: 1609164260.591612s

26624/50000 [==============>...............] - ETA: 16s - loss: 5.5362 - accuracy: 0.0845
on_train_batch_begin: 1609164260.592008s

13 step training time: 1.398879s

on_train_batch_end: 1609164262.005521s

28672/50000 [================>.............] - ETA: 14s - loss: 5.5171 - accuracy: 0.0843
on_train_batch_begin: 1609164262.006109s

14 step training time: 1.414101s

on_train_batch_end: 1609164263.416151s

30720/50000 [=================>............] - ETA: 13s - loss: 5.4877 - accuracy: 0.0846
on_train_batch_begin: 1609164263.416574s

15 step training time: 1.410464s

on_train_batch_end: 1609164264.834155s

32768/50000 [==================>...........] - ETA: 11s - loss: 5.4659 - accuracy: 0.0849
on_train_batch_begin: 1609164264.834549s

16 step training time: 1.417976s

on_train_batch_end: 1609164266.239779s

34816/50000 [===================>..........] - ETA: 10s - loss: 5.4381 - accuracy: 0.0852
on_train_batch_begin: 1609164266.240172s

17 step training time: 1.405622s

on_train_batch_end: 1609164267.654746s

36864/50000 [=====================>........] - ETA: 9s - loss: 5.4109 - accuracy: 0.0854 
on_train_batch_begin: 1609164267.655315s

18 step training time: 1.415143s

on_train_batch_end: 1609164269.061769s

38912/50000 [======================>.......] - ETA: 7s - loss: 5.3851 - accuracy: 0.0852
on_train_batch_begin: 1609164269.062186s

19 step training time: 1.406871s

on_train_batch_end: 1609164270.465565s

40960/50000 [=======================>......] - ETA: 6s - loss: 5.3616 - accuracy: 0.0852
on_train_batch_begin: 1609164270.466007s

20 step training time: 1.403822s

on_train_batch_end: 1609164271.869275s

43008/50000 [========================>.....] - ETA: 4s - loss: 5.3271 - accuracy: 0.0851
on_train_batch_begin: 1609164271.869676s

21 step training time: 1.403669s

on_train_batch_end: 1609164273.277391s

45056/50000 [==========================>...] - ETA: 3s - loss: 5.3004 - accuracy: 0.0851
on_train_batch_begin: 1609164273.277784s

22 step training time: 1.408108s

on_train_batch_end: 1609164274.685627s

47104/50000 [===========================>..] - ETA: 1s - loss: 5.2711 - accuracy: 0.0850
on_train_batch_begin: 1609164274.686022s

23 step training time: 1.408238s

on_train_batch_end: 1609164276.093724s

49152/50000 [============================>.] - ETA: 0s - loss: 5.2419 - accuracy: 0.0848
on_train_batch_begin: 1609164276.094112s

24 step training time: 1.408090s

on_train_batch_end: 1609164276.721005s

on_test_batch_begin: 1609164276.745984s

25 step training time: 0.651871s

on_epoch_end: 1609164278.647847s

Validation time: 1.901846s

Real time: 1609164278.647847s

Epoch time: 36.3854501247406s

50000/50000 [==============================] - 36s 728us/sample - loss: 5.2321 - accuracy: 0.0847 - val_loss: 13.4163 - val_accuracy: 0.0000e+00

on_epoch_begin: 1609164278.648102s

Real time: 1609164278.6481118
Epoch 3/5

on_train_batch_begin: 1609164278.652541s

on_train_batch_end: 1609164280.066622s

 2048/50000 [>.............................] - ETA: 33s - loss: 4.4430 - accuracy: 0.0784
on_train_batch_begin: 1609164280.067037s

1 step training time: 1.414495s

on_train_batch_end: 1609164281.491416s

 4096/50000 [=>............................] - ETA: 31s - loss: 4.4459 - accuracy: 0.0758
on_train_batch_begin: 1609164281.491802s

2 step training time: 1.424766s

on_train_batch_end: 1609164282.912483s

 6144/50000 [==>...........................] - ETA: 30s - loss: 4.3816 - accuracy: 0.0740
on_train_batch_begin: 1609164282.912916s

3 step training time: 1.421113s

on_train_batch_end: 1609164284.326665s

 8192/50000 [===>..........................] - ETA: 28s - loss: 4.3260 - accuracy: 0.0724
on_train_batch_begin: 1609164284.327071s

4 step training time: 1.414156s

on_train_batch_end: 1609164285.735382s

10240/50000 [=====>........................] - ETA: 27s - loss: 4.3096 - accuracy: 0.0706
on_train_batch_begin: 1609164285.735773s

5 step training time: 1.408701s

on_train_batch_end: 1609164287.145571s

12288/50000 [======>.......................] - ETA: 26s - loss: 4.2389 - accuracy: 0.0702
on_train_batch_begin: 1609164287.145963s

6 step training time: 1.410190s

on_train_batch_end: 1609164288.553116s

14336/50000 [=======>......................] - ETA: 24s - loss: 4.2135 - accuracy: 0.0698
on_train_batch_begin: 1609164288.553504s

7 step training time: 1.407541s

on_train_batch_end: 1609164289.971461s

16384/50000 [========>.....................] - ETA: 23s - loss: 4.1860 - accuracy: 0.0695
on_train_batch_begin: 1609164289.971856s

8 step training time: 1.418352s

on_train_batch_end: 1609164291.387076s

18432/50000 [==========>...................] - ETA: 21s - loss: 4.1601 - accuracy: 0.0694
on_train_batch_begin: 1609164291.387501s

9 step training time: 1.415645s

on_train_batch_end: 1609164292.819847s

20480/50000 [===========>..................] - ETA: 20s - loss: 4.1224 - accuracy: 0.0697
on_train_batch_begin: 1609164292.820235s

10 step training time: 1.432734s

on_train_batch_end: 1609164294.238000s

22528/50000 [============>.................] - ETA: 19s - loss: 4.0648 - accuracy: 0.0711
on_train_batch_begin: 1609164294.238391s

11 step training time: 1.418155s

on_train_batch_end: 1609164295.657571s

24576/50000 [=============>................] - ETA: 17s - loss: 4.0078 - accuracy: 0.0726
on_train_batch_begin: 1609164295.657977s

12 step training time: 1.419586s

on_train_batch_end: 1609164297.083499s

26624/50000 [==============>...............] - ETA: 16s - loss: 3.9554 - accuracy: 0.0741
on_train_batch_begin: 1609164297.083932s

13 step training time: 1.425955s

on_train_batch_end: 1609164298.497689s

28672/50000 [================>.............] - ETA: 14s - loss: 3.9050 - accuracy: 0.0752
on_train_batch_begin: 1609164298.498096s

14 step training time: 1.414164s

on_train_batch_end: 1609164299.919193s

30720/50000 [=================>............] - ETA: 13s - loss: 3.8519 - accuracy: 0.0764
on_train_batch_begin: 1609164299.919583s

15 step training time: 1.421486s

on_train_batch_end: 1609164301.346934s

32768/50000 [==================>...........] - ETA: 11s - loss: 3.8005 - accuracy: 0.0775
on_train_batch_begin: 1609164301.347323s

16 step training time: 1.427740s

on_train_batch_end: 1609164302.776359s

34816/50000 [===================>..........] - ETA: 10s - loss: 3.7570 - accuracy: 0.0785
on_train_batch_begin: 1609164302.776800s

17 step training time: 1.429477s

on_train_batch_end: 1609164304.208486s

36864/50000 [=====================>........] - ETA: 9s - loss: 3.7100 - accuracy: 0.0795 
on_train_batch_begin: 1609164304.208906s

18 step training time: 1.432106s

on_train_batch_end: 1609164305.624960s

38912/50000 [======================>.......] - ETA: 7s - loss: 3.6710 - accuracy: 0.0804
on_train_batch_begin: 1609164305.625350s

19 step training time: 1.416444s

on_train_batch_end: 1609164307.036808s

40960/50000 [=======================>......] - ETA: 6s - loss: 3.6317 - accuracy: 0.0813
on_train_batch_begin: 1609164307.037202s

20 step training time: 1.411852s

on_train_batch_end: 1609164308.448981s

43008/50000 [========================>.....] - ETA: 4s - loss: 3.5886 - accuracy: 0.0822
on_train_batch_begin: 1609164308.449397s

21 step training time: 1.412195s

on_train_batch_end: 1609164309.879083s

45056/50000 [==========================>...] - ETA: 3s - loss: 3.5595 - accuracy: 0.0829
on_train_batch_begin: 1609164309.879488s

22 step training time: 1.430091s

on_train_batch_end: 1609164311.306180s

47104/50000 [===========================>..] - ETA: 2s - loss: 3.5277 - accuracy: 0.0836
on_train_batch_begin: 1609164311.306597s

23 step training time: 1.427109s

on_train_batch_end: 1609164312.730489s

49152/50000 [============================>.] - ETA: 0s - loss: 3.4984 - accuracy: 0.0842
on_train_batch_begin: 1609164312.730878s

24 step training time: 1.424281s

on_train_batch_end: 1609164313.360308s

on_test_batch_begin: 1609164313.385095s

25 step training time: 0.654217s

on_epoch_end: 1609164315.288939s

Validation time: 1.903828s

Real time: 1609164315.288939s

Epoch time: 36.640849590301514s

50000/50000 [==============================] - 37s 733us/sample - loss: 3.4843 - accuracy: 0.0843 - val_loss: 7.2230 - val_accuracy: 0.1001

on_epoch_begin: 1609164315.289233s

Real time: 1609164315.289243
Epoch 4/5

on_train_batch_begin: 1609164315.293621s

on_train_batch_end: 1609164316.709316s

 2048/50000 [>.............................] - ETA: 33s - loss: 2.7297 - accuracy: 0.0995
on_train_batch_begin: 1609164316.709723s

1 step training time: 1.416102s

on_train_batch_end: 1609164318.132939s

 4096/50000 [=>............................] - ETA: 31s - loss: 2.6269 - accuracy: 0.1001
on_train_batch_begin: 1609164318.133347s

2 step training time: 1.423624s

on_train_batch_end: 1609164319.559738s

 6144/50000 [==>...........................] - ETA: 30s - loss: 2.6071 - accuracy: 0.0999
on_train_batch_begin: 1609164319.560178s

3 step training time: 1.426831s

on_train_batch_end: 1609164320.979334s

 8192/50000 [===>..........................] - ETA: 29s - loss: 2.5543 - accuracy: 0.0997
on_train_batch_begin: 1609164320.979723s

4 step training time: 1.419546s

on_train_batch_end: 1609164322.397662s

10240/50000 [=====>........................] - ETA: 27s - loss: 2.5278 - accuracy: 0.0998
on_train_batch_begin: 1609164322.398049s

5 step training time: 1.418326s

on_train_batch_end: 1609164323.814344s

12288/50000 [======>.......................] - ETA: 26s - loss: 2.5396 - accuracy: 0.0997
on_train_batch_begin: 1609164323.814744s

6 step training time: 1.416695s

on_train_batch_end: 1609164325.237109s

14336/50000 [=======>......................] - ETA: 24s - loss: 2.5221 - accuracy: 0.0997
on_train_batch_begin: 1609164325.237506s

7 step training time: 1.422762s

on_train_batch_end: 1609164326.674626s

16384/50000 [========>.....................] - ETA: 23s - loss: 2.5145 - accuracy: 0.0997
on_train_batch_begin: 1609164326.675023s

8 step training time: 1.437517s

on_train_batch_end: 1609164328.093832s

18432/50000 [==========>...................] - ETA: 21s - loss: 2.4959 - accuracy: 0.0998
on_train_batch_begin: 1609164328.094226s

9 step training time: 1.419203s

on_train_batch_end: 1609164329.528513s

20480/50000 [===========>..................] - ETA: 20s - loss: 2.4670 - accuracy: 0.0997
on_train_batch_begin: 1609164329.528938s

10 step training time: 1.434712s

on_train_batch_end: 1609164330.942228s

22528/50000 [============>.................] - ETA: 19s - loss: 2.4376 - accuracy: 0.0998
on_train_batch_begin: 1609164330.942630s

11 step training time: 1.413692s

on_train_batch_end: 1609164332.367818s

24576/50000 [=============>................] - ETA: 17s - loss: 2.4122 - accuracy: 0.0998
on_train_batch_begin: 1609164332.368219s

12 step training time: 1.425589s

on_train_batch_end: 1609164333.793669s

26624/50000 [==============>...............] - ETA: 16s - loss: 2.3889 - accuracy: 0.0998
on_train_batch_begin: 1609164333.794074s

13 step training time: 1.425855s

on_train_batch_end: 1609164335.226980s

28672/50000 [================>.............] - ETA: 14s - loss: 2.3639 - accuracy: 0.0998
on_train_batch_begin: 1609164335.227377s

14 step training time: 1.433303s

on_train_batch_end: 1609164336.646615s

30720/50000 [=================>............] - ETA: 13s - loss: 2.3467 - accuracy: 0.0998
on_train_batch_begin: 1609164336.647008s

15 step training time: 1.419631s

on_train_batch_end: 1609164338.061357s

32768/50000 [==================>...........] - ETA: 11s - loss: 2.3210 - accuracy: 0.0998
on_train_batch_begin: 1609164338.061753s

16 step training time: 1.414745s

on_train_batch_end: 1609164339.477133s

34816/50000 [===================>..........] - ETA: 10s - loss: 2.3057 - accuracy: 0.0998
on_train_batch_begin: 1609164339.477526s

17 step training time: 1.415773s

on_train_batch_end: 1609164340.894189s

36864/50000 [=====================>........] - ETA: 9s - loss: 2.2909 - accuracy: 0.0998 
on_train_batch_begin: 1609164340.894617s

18 step training time: 1.417091s

on_train_batch_end: 1609164342.313525s

38912/50000 [======================>.......] - ETA: 7s - loss: 2.2747 - accuracy: 0.0999
on_train_batch_begin: 1609164342.313939s

19 step training time: 1.419322s

on_train_batch_end: 1609164343.737912s

40960/50000 [=======================>......] - ETA: 6s - loss: 2.2525 - accuracy: 0.0999
on_train_batch_begin: 1609164343.738316s

20 step training time: 1.424376s

on_train_batch_end: 1609164345.171360s

43008/50000 [========================>.....] - ETA: 4s - loss: 2.2376 - accuracy: 0.0999
on_train_batch_begin: 1609164345.171751s

21 step training time: 1.433436s

on_train_batch_end: 1609164346.599470s

45056/50000 [==========================>...] - ETA: 3s - loss: 2.2189 - accuracy: 0.0999
on_train_batch_begin: 1609164346.599867s

22 step training time: 1.428116s

on_train_batch_end: 1609164348.013817s

47104/50000 [===========================>..] - ETA: 2s - loss: 2.2062 - accuracy: 0.0999
on_train_batch_begin: 1609164348.014209s

23 step training time: 1.414342s

on_train_batch_end: 1609164349.423583s

49152/50000 [============================>.] - ETA: 0s - loss: 2.1925 - accuracy: 0.0999
on_train_batch_begin: 1609164349.424012s

24 step training time: 1.409803s

on_train_batch_end: 1609164350.059378s

on_test_batch_begin: 1609164350.083897s

25 step training time: 0.659885s

on_epoch_end: 1609164352.007710s

Validation time: 1.923793s

Real time: 1609164352.007710s

Epoch time: 36.71848821640015s

50000/50000 [==============================] - 37s 734us/sample - loss: 2.1895 - accuracy: 0.0999 - val_loss: 8.3842 - val_accuracy: 0.0998

on_epoch_begin: 1609164352.007974s

Real time: 1609164352.007987
Epoch 5/5

on_train_batch_begin: 1609164352.012330s

on_train_batch_end: 1609164353.433822s

 2048/50000 [>.............................] - ETA: 33s - loss: 1.9196 - accuracy: 0.1000
on_train_batch_begin: 1609164353.434393s

1 step training time: 1.422064s

on_train_batch_end: 1609164354.855335s

 4096/50000 [=>............................] - ETA: 31s - loss: 1.8704 - accuracy: 0.0998
on_train_batch_begin: 1609164354.855721s

2 step training time: 1.421328s

on_train_batch_end: 1609164356.299575s

 6144/50000 [==>...........................] - ETA: 30s - loss: 1.8779 - accuracy: 0.0996
on_train_batch_begin: 1609164356.299967s

3 step training time: 1.444246s

on_train_batch_end: 1609164357.734943s

 8192/50000 [===>..........................] - ETA: 29s - loss: 1.9102 - accuracy: 0.0997
on_train_batch_begin: 1609164357.735334s

4 step training time: 1.435367s

on_train_batch_end: 1609164359.159816s

10240/50000 [=====>........................] - ETA: 27s - loss: 1.9282 - accuracy: 0.0997
on_train_batch_begin: 1609164359.160210s

5 step training time: 1.424875s

on_train_batch_end: 1609164360.578869s

12288/50000 [======>.......................] - ETA: 26s - loss: 1.9594 - accuracy: 0.0998
on_train_batch_begin: 1609164360.579287s

6 step training time: 1.419077s

on_train_batch_end: 1609164362.004721s

14336/50000 [=======>......................] - ETA: 24s - loss: 1.9792 - accuracy: 0.0999
on_train_batch_begin: 1609164362.005119s

7 step training time: 1.425832s

on_train_batch_end: 1609164363.436953s

16384/50000 [========>.....................] - ETA: 23s - loss: 2.0135 - accuracy: 0.0999
on_train_batch_begin: 1609164363.437350s

8 step training time: 1.432231s

on_train_batch_end: 1609164364.864221s

18432/50000 [==========>...................] - ETA: 22s - loss: 2.0296 - accuracy: 0.0999
on_train_batch_begin: 1609164364.864618s

9 step training time: 1.427268s

on_train_batch_end: 1609164366.289240s

20480/50000 [===========>..................] - ETA: 20s - loss: 2.0358 - accuracy: 0.0999
on_train_batch_begin: 1609164366.289634s

10 step training time: 1.425017s

on_train_batch_end: 1609164367.718781s

22528/50000 [============>.................] - ETA: 19s - loss: 2.0324 - accuracy: 0.1000
on_train_batch_begin: 1609164367.719168s

11 step training time: 1.429534s

on_train_batch_end: 1609164369.150008s

24576/50000 [=============>................] - ETA: 17s - loss: 2.0384 - accuracy: 0.1000
on_train_batch_begin: 1609164369.150403s

12 step training time: 1.431235s

on_train_batch_end: 1609164370.585154s

26624/50000 [==============>...............] - ETA: 16s - loss: 2.0391 - accuracy: 0.1000
on_train_batch_begin: 1609164370.585553s

13 step training time: 1.435150s

on_train_batch_end: 1609164372.006135s

28672/50000 [================>.............] - ETA: 14s - loss: 2.0423 - accuracy: 0.1000
on_train_batch_begin: 1609164372.006527s

14 step training time: 1.420974s

on_train_batch_end: 1609164373.427392s

30720/50000 [=================>............] - ETA: 13s - loss: 2.0368 - accuracy: 0.1000
on_train_batch_begin: 1609164373.427788s

15 step training time: 1.421261s

on_train_batch_end: 1609164374.845378s

32768/50000 [==================>...........] - ETA: 12s - loss: 2.0324 - accuracy: 0.1000
on_train_batch_begin: 1609164374.845768s

16 step training time: 1.417980s

on_train_batch_end: 1609164376.284540s

34816/50000 [===================>..........] - ETA: 10s - loss: 2.0318 - accuracy: 0.1000
on_train_batch_begin: 1609164376.284971s

17 step training time: 1.439204s

on_train_batch_end: 1609164377.718666s

36864/50000 [=====================>........] - ETA: 9s - loss: 2.0272 - accuracy: 0.1000 
on_train_batch_begin: 1609164377.719065s

18 step training time: 1.434093s

on_train_batch_end: 1609164379.146043s

38912/50000 [======================>.......] - ETA: 7s - loss: 2.0258 - accuracy: 0.1000
on_train_batch_begin: 1609164379.146448s

19 step training time: 1.427383s

on_train_batch_end: 1609164380.571384s

40960/50000 [=======================>......] - ETA: 6s - loss: 2.0206 - accuracy: 0.1000
on_train_batch_begin: 1609164380.571802s

20 step training time: 1.425354s

on_train_batch_end: 1609164381.996744s

43008/50000 [========================>.....] - ETA: 4s - loss: 2.0177 - accuracy: 0.1000
on_train_batch_begin: 1609164381.997148s

21 step training time: 1.425346s

on_train_batch_end: 1609164383.444569s

45056/50000 [==========================>...] - ETA: 3s - loss: 2.0142 - accuracy: 0.1000
on_train_batch_begin: 1609164383.444999s

22 step training time: 1.447851s

on_train_batch_end: 1609164384.877295s

47104/50000 [===========================>..] - ETA: 2s - loss: 2.0138 - accuracy: 0.1000
on_train_batch_begin: 1609164384.877685s

23 step training time: 1.432686s

on_train_batch_end: 1609164386.306086s

49152/50000 [============================>.] - ETA: 0s - loss: 2.0183 - accuracy: 0.1000
on_train_batch_begin: 1609164386.306485s

24 step training time: 1.428800s

on_train_batch_end: 1609164386.945488s

on_test_batch_begin: 1609164386.969842s

25 step training time: 0.663357s

on_epoch_end: 1609164388.894539s

Validation time: 1.924682s

Real time: 1609164388.894539s

Epoch time: 36.886571645736694s

50000/50000 [==============================] - 37s 738us/sample - loss: 2.0155 - accuracy: 0.1000 - val_loss: 7.4501 - val_accuracy: 0.1001
Tempo do fit: 226.50917601585388