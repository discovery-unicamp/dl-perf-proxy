wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:00
   204800/170498071 [..............................] - ETA: 1:12
  1441792/170498071 [..............................] - ETA: 16s 
  4481024/170498071 [..............................] - ETA: 6s 
  7954432/170498071 [>.............................] - ETA: 4s
 11329536/170498071 [>.............................] - ETA: 4s
 14950400/170498071 [=>............................] - ETA: 3s
 18604032/170498071 [==>...........................] - ETA: 3s
 22011904/170498071 [==>...........................] - ETA: 2s
 25141248/170498071 [===>..........................] - ETA: 2s
 29097984/170498071 [====>.........................] - ETA: 2s
 32710656/170498071 [====>.........................] - ETA: 2s
 36069376/170498071 [=====>........................] - ETA: 2s
 39329792/170498071 [=====>........................] - ETA: 2s
 42852352/170498071 [======>.......................] - ETA: 2s
 46473216/170498071 [=======>......................] - ETA: 2s
 50110464/170498071 [=======>......................] - ETA: 2s
 53305344/170498071 [========>.....................] - ETA: 1s
 56631296/170498071 [========>.....................] - ETA: 1s
 60211200/170498071 [=========>....................] - ETA: 1s
 63823872/170498071 [==========>...................] - ETA: 1s
 67330048/170498071 [==========>...................] - ETA: 1s
 70557696/170498071 [===========>..................] - ETA: 1s
 73916416/170498071 [============>.................] - ETA: 1s
 77520896/170498071 [============>.................] - ETA: 1s
 81141760/170498071 [=============>................] - ETA: 1s
 84574208/170498071 [=============>................] - ETA: 1s
 87810048/170498071 [==============>...............] - ETA: 1s
 91258880/170498071 [===============>..............] - ETA: 1s
 94855168/170498071 [===============>..............] - ETA: 1s
 98426880/170498071 [================>.............] - ETA: 1s
101785600/170498071 [================>.............] - ETA: 1s
105078784/170498071 [=================>............] - ETA: 1s
108519424/170498071 [==================>...........] - ETA: 0s
112082944/170498071 [==================>...........] - ETA: 0s
115671040/170498071 [===================>..........] - ETA: 0s
119037952/170498071 [===================>..........] - ETA: 0s
122363904/170498071 [====================>.........] - ETA: 0s
125886464/170498071 [=====================>........] - ETA: 0s
129458176/170498071 [=====================>........] - ETA: 0s
133029888/170498071 [======================>.......] - ETA: 0s
136331264/170498071 [======================>.......] - ETA: 0s
139665408/170498071 [=======================>......] - ETA: 0s
143171584/170498071 [========================>.....] - ETA: 0s
146776064/170498071 [========================>.....] - ETA: 0s
150265856/170498071 [=========================>....] - ETA: 0s
153591808/170498071 [==========================>...] - ETA: 0s
157040640/170498071 [==========================>...] - ETA: 0s
160620544/170498071 [===========================>..] - ETA: 0s
164192256/170498071 [===========================>..] - ETA: 0s
167567360/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 0s
 4440064/94765736 [>.............................] - ETA: 1s
 9388032/94765736 [=>............................] - ETA: 1s
16130048/94765736 [====>.........................] - ETA: 1s
18841600/94765736 [====>.........................] - ETA: 1s
24805376/94765736 [======>.......................] - ETA: 1s
28286976/94765736 [=======>......................] - ETA: 1s
30089216/94765736 [========>.....................] - ETA: 1s
34127872/94765736 [=========>....................] - ETA: 1s
37683200/94765736 [==========>...................] - ETA: 1s
42590208/94765736 [============>.................] - ETA: 0s
47136768/94765736 [=============>................] - ETA: 0s
51093504/94765736 [===============>..............] - ETA: 0s
56598528/94765736 [================>.............] - ETA: 0s
60612608/94765736 [==================>...........] - ETA: 0s
65986560/94765736 [===================>..........] - ETA: 0s
67518464/94765736 [====================>.........] - ETA: 0s
71852032/94765736 [=====================>........] - ETA: 0s
75440128/94765736 [======================>.......] - ETA: 0s
79314944/94765736 [========================>.....] - ETA: 0s
85458944/94765736 [==========================>...] - ETA: 0s
91406336/94765736 [===========================>..] - ETA: 0s
94281728/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 2s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 21.239986658096313
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1607808356.761290s

Real time: 1607808356.761312
Epoch 1/5

on_train_batch_begin: 1607808357.742240s

on_train_batch_end: 1607808417.161332s

 1024/50000 [..............................] - ETA: 48:08 - loss: 17.4637 - accuracy: 1.7548e-04
on_train_batch_begin: 1607808417.162119s

1 step training time: 59.419880s

on_train_batch_end: 1607808417.229827s

 2048/50000 [>.............................] - ETA: 23:35 - loss: 15.1506 - accuracy: 1.6403e-04
on_train_batch_begin: 1607808417.230242s

2 step training time: 0.068123s

on_train_batch_end: 1607808417.295284s

 3072/50000 [>.............................] - ETA: 15:24 - loss: 13.1489 - accuracy: 4.5268e-04
on_train_batch_begin: 1607808417.295688s

3 step training time: 0.065446s

on_train_batch_end: 1607808417.359646s

 4096/50000 [=>............................] - ETA: 11:19 - loss: 12.0015 - accuracy: 8.8310e-04
on_train_batch_begin: 1607808417.360057s

4 step training time: 0.064369s

on_train_batch_end: 1607808417.424379s

 5120/50000 [==>...........................] - ETA: 8:51 - loss: 11.2414 - accuracy: 0.0019     
on_train_batch_begin: 1607808417.424786s

5 step training time: 0.064729s

on_train_batch_end: 1607808417.488381s

 6144/50000 [==>...........................] - ETA: 7:13 - loss: 10.7256 - accuracy: 0.0043
on_train_batch_begin: 1607808417.488797s

6 step training time: 0.064011s

on_train_batch_end: 1607808417.552083s

 7168/50000 [===>..........................] - ETA: 6:03 - loss: 10.3523 - accuracy: 0.0071
on_train_batch_begin: 1607808417.552476s

7 step training time: 0.063679s

on_train_batch_end: 1607808417.620925s

 8192/50000 [===>..........................] - ETA: 5:10 - loss: 10.0496 - accuracy: 0.0108
on_train_batch_begin: 1607808417.621325s

8 step training time: 0.068850s

on_train_batch_end: 1607808417.686950s

 9216/50000 [====>.........................] - ETA: 4:29 - loss: 9.8006 - accuracy: 0.0144 
on_train_batch_begin: 1607808417.687347s

9 step training time: 0.066021s

on_train_batch_end: 1607808417.749991s

10240/50000 [=====>........................] - ETA: 3:56 - loss: 9.6034 - accuracy: 0.0179
on_train_batch_begin: 1607808417.750415s

10 step training time: 0.063068s

on_train_batch_end: 1607808417.813155s

11264/50000 [=====>........................] - ETA: 3:29 - loss: 9.4221 - accuracy: 0.0211
on_train_batch_begin: 1607808417.813547s

11 step training time: 0.063133s

on_train_batch_end: 1607808417.876732s

12288/50000 [======>.......................] - ETA: 3:07 - loss: 9.2598 - accuracy: 0.0248
on_train_batch_begin: 1607808417.877124s

12 step training time: 0.063576s

on_train_batch_end: 1607808417.941457s

13312/50000 [======>.......................] - ETA: 2:48 - loss: 9.1229 - accuracy: 0.0283
on_train_batch_begin: 1607808417.941842s

13 step training time: 0.064718s

on_train_batch_end: 1607808418.006896s

14336/50000 [=======>......................] - ETA: 2:32 - loss: 8.9914 - accuracy: 0.0312
on_train_batch_begin: 1607808418.007282s

14 step training time: 0.065440s

on_train_batch_end: 1607808418.070933s

15360/50000 [========>.....................] - ETA: 2:18 - loss: 8.8796 - accuracy: 0.0333
on_train_batch_begin: 1607808418.071321s

15 step training time: 0.064039s

on_train_batch_end: 1607808418.134062s

16384/50000 [========>.....................] - ETA: 2:05 - loss: 8.7759 - accuracy: 0.0356
on_train_batch_begin: 1607808418.134478s

16 step training time: 0.063157s

on_train_batch_end: 1607808418.200301s

17408/50000 [=========>....................] - ETA: 1:55 - loss: 8.6798 - accuracy: 0.0371
on_train_batch_begin: 1607808418.200690s

17 step training time: 0.066212s

on_train_batch_end: 1607808418.264774s

18432/50000 [==========>...................] - ETA: 1:45 - loss: 8.5959 - accuracy: 0.0390
on_train_batch_begin: 1607808418.265160s

18 step training time: 0.064469s

on_train_batch_end: 1607808418.333440s

19456/50000 [==========>...................] - ETA: 1:36 - loss: 8.5117 - accuracy: 0.0408
on_train_batch_begin: 1607808418.333832s

19 step training time: 0.068672s

on_train_batch_end: 1607808418.397175s

20480/50000 [===========>..................] - ETA: 1:28 - loss: 8.4384 - accuracy: 0.0420
on_train_batch_begin: 1607808418.397564s

20 step training time: 0.063732s

on_train_batch_end: 1607808418.465792s

21504/50000 [===========>..................] - ETA: 1:21 - loss: 8.3704 - accuracy: 0.0433
on_train_batch_begin: 1607808418.466177s

21 step training time: 0.068613s

on_train_batch_end: 1607808418.531164s

22528/50000 [============>.................] - ETA: 1:15 - loss: 8.3026 - accuracy: 0.0446
on_train_batch_begin: 1607808418.531549s

22 step training time: 0.065372s

on_train_batch_end: 1607808418.595814s

23552/50000 [=============>................] - ETA: 1:09 - loss: 8.2373 - accuracy: 0.0458
on_train_batch_begin: 1607808418.596200s

23 step training time: 0.064650s

on_train_batch_end: 1607808418.658980s

24576/50000 [=============>................] - ETA: 1:04 - loss: 8.1764 - accuracy: 0.0471
on_train_batch_begin: 1607808418.659408s

24 step training time: 0.063208s

on_train_batch_end: 1607808418.723897s

25600/50000 [==============>...............] - ETA: 59s - loss: 8.1171 - accuracy: 0.0480 
on_train_batch_begin: 1607808418.724285s

25 step training time: 0.064878s

on_train_batch_end: 1607808418.792155s

26624/50000 [==============>...............] - ETA: 54s - loss: 8.0627 - accuracy: 0.0490
on_train_batch_begin: 1607808418.792541s

26 step training time: 0.068256s

on_train_batch_end: 1607808418.855228s

27648/50000 [===============>..............] - ETA: 50s - loss: 8.0109 - accuracy: 0.0497
on_train_batch_begin: 1607808418.855621s

27 step training time: 0.063080s

on_train_batch_end: 1607808418.921899s

28672/50000 [================>.............] - ETA: 46s - loss: 7.9648 - accuracy: 0.0505
on_train_batch_begin: 1607808418.922299s

28 step training time: 0.066677s

on_train_batch_end: 1607808418.985125s

29696/50000 [================>.............] - ETA: 42s - loss: 7.9164 - accuracy: 0.0516
on_train_batch_begin: 1607808418.985503s

29 step training time: 0.063205s

on_train_batch_end: 1607808419.052506s

30720/50000 [=================>............] - ETA: 39s - loss: 7.8688 - accuracy: 0.0524
on_train_batch_begin: 1607808419.052890s

30 step training time: 0.067387s

on_train_batch_end: 1607808419.121272s

31744/50000 [==================>...........] - ETA: 35s - loss: 7.8230 - accuracy: 0.0535
on_train_batch_begin: 1607808419.121657s

31 step training time: 0.068767s

on_train_batch_end: 1607808419.188640s

32768/50000 [==================>...........] - ETA: 32s - loss: 7.7835 - accuracy: 0.0544
on_train_batch_begin: 1607808419.189030s

32 step training time: 0.067373s

on_train_batch_end: 1607808419.251941s

33792/50000 [===================>..........] - ETA: 29s - loss: 7.7461 - accuracy: 0.0550
on_train_batch_begin: 1607808419.252332s

33 step training time: 0.063302s

on_train_batch_end: 1607808419.318712s

34816/50000 [===================>..........] - ETA: 27s - loss: 7.7060 - accuracy: 0.0561
on_train_batch_begin: 1607808419.319098s

34 step training time: 0.066765s

on_train_batch_end: 1607808419.381971s

35840/50000 [====================>.........] - ETA: 24s - loss: 7.6668 - accuracy: 0.0570
on_train_batch_begin: 1607808419.382394s

35 step training time: 0.063296s

on_train_batch_end: 1607808419.445435s

36864/50000 [=====================>........] - ETA: 22s - loss: 7.6311 - accuracy: 0.0576
on_train_batch_begin: 1607808419.445829s

36 step training time: 0.063435s

on_train_batch_end: 1607808419.508071s

37888/50000 [=====================>........] - ETA: 20s - loss: 7.5950 - accuracy: 0.0585
on_train_batch_begin: 1607808419.508454s

37 step training time: 0.062625s

on_train_batch_end: 1607808419.572375s

38912/50000 [======================>.......] - ETA: 17s - loss: 7.5628 - accuracy: 0.0589
on_train_batch_begin: 1607808419.572758s

38 step training time: 0.064305s

on_train_batch_end: 1607808419.636062s

39936/50000 [======================>.......] - ETA: 15s - loss: 7.5319 - accuracy: 0.0597
on_train_batch_begin: 1607808419.636444s

39 step training time: 0.063685s

on_train_batch_end: 1607808419.701279s

40960/50000 [=======================>......] - ETA: 13s - loss: 7.5047 - accuracy: 0.0604
on_train_batch_begin: 1607808419.701661s

40 step training time: 0.065218s

on_train_batch_end: 1607808419.765043s

41984/50000 [========================>.....] - ETA: 12s - loss: 7.4764 - accuracy: 0.0608
on_train_batch_begin: 1607808419.765429s

41 step training time: 0.063767s

on_train_batch_end: 1607808419.827186s

43008/50000 [========================>.....] - ETA: 10s - loss: 7.4476 - accuracy: 0.0613
on_train_batch_begin: 1607808419.827570s

42 step training time: 0.062142s

on_train_batch_end: 1607808419.895035s

44032/50000 [=========================>....] - ETA: 8s - loss: 7.4199 - accuracy: 0.0615 
on_train_batch_begin: 1607808419.895440s

43 step training time: 0.067869s

on_train_batch_end: 1607808419.963926s

45056/50000 [==========================>...] - ETA: 6s - loss: 7.3917 - accuracy: 0.0617
on_train_batch_begin: 1607808419.964324s

44 step training time: 0.068884s

on_train_batch_end: 1607808420.028676s

46080/50000 [==========================>...] - ETA: 5s - loss: 7.3663 - accuracy: 0.0619
on_train_batch_begin: 1607808420.029071s

45 step training time: 0.064747s

on_train_batch_end: 1607808420.099510s

47104/50000 [===========================>..] - ETA: 3s - loss: 7.3400 - accuracy: 0.0623
on_train_batch_begin: 1607808420.099909s

46 step training time: 0.070838s

on_train_batch_end: 1607808420.165068s

48128/50000 [===========================>..] - ETA: 2s - loss: 7.3159 - accuracy: 0.0624
on_train_batch_begin: 1607808420.165467s

47 step training time: 0.065558s

on_train_batch_end: 1607808420.228929s

49152/50000 [============================>.] - ETA: 1s - loss: 7.2932 - accuracy: 0.0627
on_train_batch_begin: 1607808420.229328s

48 step training time: 0.063861s

on_train_batch_end: 1607808421.225305s

on_test_batch_begin: 1607808421.515159s

49 step training time: 1.285831s

on_epoch_end: 1607808428.255727s

Validation time: 6.740548s

Real time: 1607808428.255727s

Epoch time: 71.49444007873535s

50000/50000 [==============================] - 71s 1ms/sample - loss: 7.2723 - accuracy: 0.0628 - val_loss: 3576.8856 - val_accuracy: 0.0000e+00

on_epoch_begin: 1607808428.256000s

Real time: 1607808428.25601
Epoch 2/5

on_train_batch_begin: 1607808428.261781s

on_train_batch_end: 1607808428.325891s

 1024/50000 [..............................] - ETA: 3s - loss: 6.0495 - accuracy: 0.0757
on_train_batch_begin: 1607808428.326303s

1 step training time: 0.064522s

on_train_batch_end: 1607808428.390543s

 2048/50000 [>.............................] - ETA: 3s - loss: 6.0738 - accuracy: 0.0736
on_train_batch_begin: 1607808428.390923s

2 step training time: 0.064620s

on_train_batch_end: 1607808428.455880s

 3072/50000 [>.............................] - ETA: 3s - loss: 5.9650 - accuracy: 0.0730
on_train_batch_begin: 1607808428.456261s

3 step training time: 0.065339s

on_train_batch_end: 1607808428.522554s

 4096/50000 [=>............................] - ETA: 2s - loss: 5.9294 - accuracy: 0.0725
on_train_batch_begin: 1607808428.522938s

4 step training time: 0.066677s

on_train_batch_end: 1607808428.587922s

 5120/50000 [==>...........................] - ETA: 2s - loss: 5.9072 - accuracy: 0.0707
on_train_batch_begin: 1607808428.588300s

5 step training time: 0.065361s

on_train_batch_end: 1607808428.653806s

 6144/50000 [==>...........................] - ETA: 2s - loss: 5.8612 - accuracy: 0.0691
on_train_batch_begin: 1607808428.654185s

6 step training time: 0.065886s

on_train_batch_end: 1607808428.717036s

 7168/50000 [===>..........................] - ETA: 2s - loss: 5.8632 - accuracy: 0.0669
on_train_batch_begin: 1607808428.717421s

7 step training time: 0.063235s

on_train_batch_end: 1607808428.783813s

 8192/50000 [===>..........................] - ETA: 2s - loss: 5.8379 - accuracy: 0.0648
on_train_batch_begin: 1607808428.784202s

8 step training time: 0.066781s

on_train_batch_end: 1607808428.851905s

 9216/50000 [====>.........................] - ETA: 2s - loss: 5.8082 - accuracy: 0.0630
on_train_batch_begin: 1607808428.852287s

9 step training time: 0.068085s

on_train_batch_end: 1607808428.920112s

10240/50000 [=====>........................] - ETA: 2s - loss: 5.7674 - accuracy: 0.0620
on_train_batch_begin: 1607808428.920503s

10 step training time: 0.068217s

on_train_batch_end: 1607808428.987916s

11264/50000 [=====>........................] - ETA: 2s - loss: 5.7383 - accuracy: 0.0611
on_train_batch_begin: 1607808428.988295s

11 step training time: 0.067791s

on_train_batch_end: 1607808429.051468s

12288/50000 [======>.......................] - ETA: 2s - loss: 5.6882 - accuracy: 0.0608
on_train_batch_begin: 1607808429.051854s

12 step training time: 0.063559s

on_train_batch_end: 1607808429.114817s

13312/50000 [======>.......................] - ETA: 2s - loss: 5.6384 - accuracy: 0.0607
on_train_batch_begin: 1607808429.115215s

13 step training time: 0.063362s

on_train_batch_end: 1607808429.180195s

14336/50000 [=======>......................] - ETA: 2s - loss: 5.5893 - accuracy: 0.0607
on_train_batch_begin: 1607808429.180580s

14 step training time: 0.065365s

on_train_batch_end: 1607808429.245769s

15360/50000 [========>.....................] - ETA: 2s - loss: 5.5397 - accuracy: 0.0609
on_train_batch_begin: 1607808429.246156s

15 step training time: 0.065575s

on_train_batch_end: 1607808429.311442s

16384/50000 [========>.....................] - ETA: 2s - loss: 5.4903 - accuracy: 0.0612
on_train_batch_begin: 1607808429.311828s

16 step training time: 0.065672s

on_train_batch_end: 1607808429.378046s

17408/50000 [=========>....................] - ETA: 2s - loss: 5.4319 - accuracy: 0.0615
on_train_batch_begin: 1607808429.378458s

17 step training time: 0.066631s

on_train_batch_end: 1607808429.442200s

18432/50000 [==========>...................] - ETA: 2s - loss: 5.3756 - accuracy: 0.0622
on_train_batch_begin: 1607808429.442619s

18 step training time: 0.064160s

on_train_batch_end: 1607808429.509519s

19456/50000 [==========>...................] - ETA: 1s - loss: 5.3161 - accuracy: 0.0631
on_train_batch_begin: 1607808429.509903s

19 step training time: 0.067285s

on_train_batch_end: 1607808429.576334s

20480/50000 [===========>..................] - ETA: 1s - loss: 5.2730 - accuracy: 0.0637
on_train_batch_begin: 1607808429.576717s

20 step training time: 0.066814s

on_train_batch_end: 1607808429.640758s

21504/50000 [===========>..................] - ETA: 1s - loss: 5.2282 - accuracy: 0.0645
on_train_batch_begin: 1607808429.641138s

21 step training time: 0.064421s

on_train_batch_end: 1607808429.708171s

22528/50000 [============>.................] - ETA: 1s - loss: 5.1834 - accuracy: 0.0650
on_train_batch_begin: 1607808429.708551s

22 step training time: 0.067413s

on_train_batch_end: 1607808429.776900s

23552/50000 [=============>................] - ETA: 1s - loss: 5.1369 - accuracy: 0.0657
on_train_batch_begin: 1607808429.777326s

23 step training time: 0.068774s

on_train_batch_end: 1607808429.842398s

24576/50000 [=============>................] - ETA: 1s - loss: 5.0964 - accuracy: 0.0667
on_train_batch_begin: 1607808429.842798s

24 step training time: 0.065472s

on_train_batch_end: 1607808429.908916s

25600/50000 [==============>...............] - ETA: 1s - loss: 5.0501 - accuracy: 0.0676
on_train_batch_begin: 1607808429.909296s

25 step training time: 0.066498s

on_train_batch_end: 1607808429.972989s

26624/50000 [==============>...............] - ETA: 1s - loss: 5.0088 - accuracy: 0.0686
on_train_batch_begin: 1607808429.973366s

26 step training time: 0.064070s

on_train_batch_end: 1607808430.037834s

27648/50000 [===============>..............] - ETA: 1s - loss: 4.9636 - accuracy: 0.0696
on_train_batch_begin: 1607808430.038216s

27 step training time: 0.064850s

on_train_batch_end: 1607808430.103518s

28672/50000 [================>.............] - ETA: 1s - loss: 4.9176 - accuracy: 0.0706
on_train_batch_begin: 1607808430.103901s

28 step training time: 0.065685s

on_train_batch_end: 1607808430.167483s

29696/50000 [================>.............] - ETA: 1s - loss: 4.8724 - accuracy: 0.0714
on_train_batch_begin: 1607808430.167861s

29 step training time: 0.063961s

on_train_batch_end: 1607808430.231992s

30720/50000 [=================>............] - ETA: 1s - loss: 4.8285 - accuracy: 0.0722
on_train_batch_begin: 1607808430.232372s

30 step training time: 0.064511s

on_train_batch_end: 1607808430.294771s

31744/50000 [==================>...........] - ETA: 1s - loss: 4.7924 - accuracy: 0.0729
on_train_batch_begin: 1607808430.295147s

31 step training time: 0.062775s

on_train_batch_end: 1607808430.364726s

32768/50000 [==================>...........] - ETA: 1s - loss: 4.7523 - accuracy: 0.0736
on_train_batch_begin: 1607808430.365119s

32 step training time: 0.069971s

on_train_batch_end: 1607808430.427696s

33792/50000 [===================>..........] - ETA: 1s - loss: 4.7133 - accuracy: 0.0742
on_train_batch_begin: 1607808430.428074s

33 step training time: 0.062955s

on_train_batch_end: 1607808430.492703s

34816/50000 [===================>..........] - ETA: 0s - loss: 4.6708 - accuracy: 0.0749
on_train_batch_begin: 1607808430.493092s

34 step training time: 0.065017s

on_train_batch_end: 1607808430.556252s

35840/50000 [====================>.........] - ETA: 0s - loss: 4.6321 - accuracy: 0.0755
on_train_batch_begin: 1607808430.556634s

35 step training time: 0.063542s

on_train_batch_end: 1607808430.618650s

36864/50000 [=====================>........] - ETA: 0s - loss: 4.5896 - accuracy: 0.0761
on_train_batch_begin: 1607808430.619037s

36 step training time: 0.062403s

on_train_batch_end: 1607808430.687361s

37888/50000 [=====================>........] - ETA: 0s - loss: 4.5437 - accuracy: 0.0767
on_train_batch_begin: 1607808430.687741s

37 step training time: 0.068704s

on_train_batch_end: 1607808430.749894s

38912/50000 [======================>.......] - ETA: 0s - loss: 4.5012 - accuracy: 0.0773
on_train_batch_begin: 1607808430.750302s

38 step training time: 0.062561s

on_train_batch_end: 1607808430.818207s

39936/50000 [======================>.......] - ETA: 0s - loss: 4.4608 - accuracy: 0.0779
on_train_batch_begin: 1607808430.818624s

39 step training time: 0.068321s

on_train_batch_end: 1607808430.883913s

40960/50000 [=======================>......] - ETA: 0s - loss: 4.4221 - accuracy: 0.0784
on_train_batch_begin: 1607808430.884297s

40 step training time: 0.065674s

on_train_batch_end: 1607808430.948569s

41984/50000 [========================>.....] - ETA: 0s - loss: 4.3868 - accuracy: 0.0789
on_train_batch_begin: 1607808430.948947s

41 step training time: 0.064650s

on_train_batch_end: 1607808431.015365s

43008/50000 [========================>.....] - ETA: 0s - loss: 4.3468 - accuracy: 0.0793
on_train_batch_begin: 1607808431.015758s

42 step training time: 0.066811s

on_train_batch_end: 1607808431.081288s

44032/50000 [=========================>....] - ETA: 0s - loss: 4.3119 - accuracy: 0.0798
on_train_batch_begin: 1607808431.081664s

43 step training time: 0.065906s

on_train_batch_end: 1607808431.144474s

45056/50000 [==========================>...] - ETA: 0s - loss: 4.2751 - accuracy: 0.0802
on_train_batch_begin: 1607808431.144858s

44 step training time: 0.063194s

on_train_batch_end: 1607808431.209256s

46080/50000 [==========================>...] - ETA: 0s - loss: 4.2395 - accuracy: 0.0806
on_train_batch_begin: 1607808431.209645s

45 step training time: 0.064787s

on_train_batch_end: 1607808431.273631s

47104/50000 [===========================>..] - ETA: 0s - loss: 4.2084 - accuracy: 0.0810
on_train_batch_begin: 1607808431.274013s

46 step training time: 0.064368s

on_train_batch_end: 1607808431.337475s

48128/50000 [===========================>..] - ETA: 0s - loss: 4.1728 - accuracy: 0.0814
on_train_batch_begin: 1607808431.337869s

47 step training time: 0.063856s

on_train_batch_end: 1607808431.403334s

49152/50000 [============================>.] - ETA: 0s - loss: 4.1424 - accuracy: 0.0818
on_train_batch_begin: 1607808431.403723s

48 step training time: 0.065854s

on_train_batch_end: 1607808431.465980s

on_test_batch_begin: 1607808431.492225s

49 step training time: 0.088502s

on_epoch_end: 1607808431.738537s

Validation time: 0.246296s

Real time: 1607808431.738537s

Epoch time: 3.482548475265503s

50000/50000 [==============================] - 3s 70us/sample - loss: 4.1133 - accuracy: 0.0820 - val_loss: 9.3584 - val_accuracy: 0.0000e+00

on_epoch_begin: 1607808431.738813s

Real time: 1607808431.7388237
Epoch 3/5

on_train_batch_begin: 1607808431.744720s

on_train_batch_end: 1607808431.809865s

 1024/50000 [..............................] - ETA: 3s - loss: 2.2571 - accuracy: 0.1021
on_train_batch_begin: 1607808431.810263s

1 step training time: 0.065542s

on_train_batch_end: 1607808431.873650s

 2048/50000 [>.............................] - ETA: 3s - loss: 2.4515 - accuracy: 0.1023
on_train_batch_begin: 1607808431.874038s

2 step training time: 0.063776s

on_train_batch_end: 1607808431.939297s

 3072/50000 [>.............................] - ETA: 3s - loss: 2.4044 - accuracy: 0.1015
on_train_batch_begin: 1607808431.939688s

3 step training time: 0.065650s

on_train_batch_end: 1607808432.008839s

 4096/50000 [=>............................] - ETA: 3s - loss: 2.4291 - accuracy: 0.1013
on_train_batch_begin: 1607808432.009221s

4 step training time: 0.069533s

on_train_batch_end: 1607808432.071519s

 5120/50000 [==>...........................] - ETA: 2s - loss: 2.4347 - accuracy: 0.1011
on_train_batch_begin: 1607808432.071902s

5 step training time: 0.062681s

on_train_batch_end: 1607808432.139229s

 6144/50000 [==>...........................] - ETA: 2s - loss: 2.4221 - accuracy: 0.1010
on_train_batch_begin: 1607808432.139613s

6 step training time: 0.067712s

on_train_batch_end: 1607808432.203929s

 7168/50000 [===>..........................] - ETA: 2s - loss: 2.4084 - accuracy: 0.1011
on_train_batch_begin: 1607808432.204341s

7 step training time: 0.064728s

on_train_batch_end: 1607808432.271032s

 8192/50000 [===>..........................] - ETA: 2s - loss: 2.4204 - accuracy: 0.1009
on_train_batch_begin: 1607808432.271415s

8 step training time: 0.067075s

on_train_batch_end: 1607808432.340261s

 9216/50000 [====>.........................] - ETA: 2s - loss: 2.4363 - accuracy: 0.1009
on_train_batch_begin: 1607808432.340646s

9 step training time: 0.069230s

on_train_batch_end: 1607808432.408545s

10240/50000 [=====>........................] - ETA: 2s - loss: 2.4467 - accuracy: 0.1009
on_train_batch_begin: 1607808432.408930s

10 step training time: 0.068284s

on_train_batch_end: 1607808432.472639s

11264/50000 [=====>........................] - ETA: 2s - loss: 2.4706 - accuracy: 0.1007
on_train_batch_begin: 1607808432.473040s

11 step training time: 0.064110s

on_train_batch_end: 1607808432.537949s

12288/50000 [======>.......................] - ETA: 2s - loss: 2.4741 - accuracy: 0.1007
on_train_batch_begin: 1607808432.538368s

12 step training time: 0.065328s

on_train_batch_end: 1607808432.605811s

13312/50000 [======>.......................] - ETA: 2s - loss: 2.4727 - accuracy: 0.1007
on_train_batch_begin: 1607808432.606195s

13 step training time: 0.067827s

on_train_batch_end: 1607808432.673153s

14336/50000 [=======>......................] - ETA: 2s - loss: 2.4749 - accuracy: 0.1006
on_train_batch_begin: 1607808432.673541s

14 step training time: 0.067345s

on_train_batch_end: 1607808432.736585s

15360/50000 [========>.....................] - ETA: 2s - loss: 2.4668 - accuracy: 0.1005
on_train_batch_begin: 1607808432.736971s

15 step training time: 0.063430s

on_train_batch_end: 1607808432.800835s

16384/50000 [========>.....................] - ETA: 2s - loss: 2.4545 - accuracy: 0.1005
on_train_batch_begin: 1607808432.801217s

16 step training time: 0.064246s

on_train_batch_end: 1607808432.864758s

17408/50000 [=========>....................] - ETA: 2s - loss: 2.4385 - accuracy: 0.1005
on_train_batch_begin: 1607808432.865138s

17 step training time: 0.063921s

on_train_batch_end: 1607808432.928165s

18432/50000 [==========>...................] - ETA: 2s - loss: 2.4279 - accuracy: 0.1005
on_train_batch_begin: 1607808432.928552s

18 step training time: 0.063414s

on_train_batch_end: 1607808432.993756s

19456/50000 [==========>...................] - ETA: 1s - loss: 2.4294 - accuracy: 0.1005
on_train_batch_begin: 1607808432.994147s

19 step training time: 0.065595s

on_train_batch_end: 1607808433.058563s

20480/50000 [===========>..................] - ETA: 1s - loss: 2.4224 - accuracy: 0.1005
on_train_batch_begin: 1607808433.058945s

20 step training time: 0.064799s

on_train_batch_end: 1607808433.123124s

21504/50000 [===========>..................] - ETA: 1s - loss: 2.4164 - accuracy: 0.1005
on_train_batch_begin: 1607808433.123514s

21 step training time: 0.064569s

on_train_batch_end: 1607808433.191567s

22528/50000 [============>.................] - ETA: 1s - loss: 2.4093 - accuracy: 0.1005
on_train_batch_begin: 1607808433.191951s

22 step training time: 0.068437s

on_train_batch_end: 1607808433.261334s

23552/50000 [=============>................] - ETA: 1s - loss: 2.4005 - accuracy: 0.1005
on_train_batch_begin: 1607808433.261714s

23 step training time: 0.069763s

on_train_batch_end: 1607808433.326227s

24576/50000 [=============>................] - ETA: 1s - loss: 2.3946 - accuracy: 0.1005
on_train_batch_begin: 1607808433.326637s

24 step training time: 0.064923s

on_train_batch_end: 1607808433.390270s

25600/50000 [==============>...............] - ETA: 1s - loss: 2.3823 - accuracy: 0.1005
on_train_batch_begin: 1607808433.390677s

25 step training time: 0.064040s

on_train_batch_end: 1607808433.458495s

26624/50000 [==============>...............] - ETA: 1s - loss: 2.3748 - accuracy: 0.1005
on_train_batch_begin: 1607808433.458877s

26 step training time: 0.068201s

on_train_batch_end: 1607808433.523480s

27648/50000 [===============>..............] - ETA: 1s - loss: 2.3589 - accuracy: 0.1005
on_train_batch_begin: 1607808433.523866s

27 step training time: 0.064989s

on_train_batch_end: 1607808433.592046s

28672/50000 [================>.............] - ETA: 1s - loss: 2.3477 - accuracy: 0.1005
on_train_batch_begin: 1607808433.592437s

28 step training time: 0.068571s

on_train_batch_end: 1607808433.660024s

29696/50000 [================>.............] - ETA: 1s - loss: 2.3359 - accuracy: 0.1005
on_train_batch_begin: 1607808433.660404s

29 step training time: 0.067967s

on_train_batch_end: 1607808433.730100s

30720/50000 [=================>............] - ETA: 1s - loss: 2.3251 - accuracy: 0.1006
on_train_batch_begin: 1607808433.730513s

30 step training time: 0.070109s

on_train_batch_end: 1607808433.795511s

31744/50000 [==================>...........] - ETA: 1s - loss: 2.3148 - accuracy: 0.1005
on_train_batch_begin: 1607808433.795897s

31 step training time: 0.065384s

on_train_batch_end: 1607808433.859435s

32768/50000 [==================>...........] - ETA: 1s - loss: 2.3041 - accuracy: 0.1005
on_train_batch_begin: 1607808433.859817s

32 step training time: 0.063920s

on_train_batch_end: 1607808433.923905s

33792/50000 [===================>..........] - ETA: 1s - loss: 2.2978 - accuracy: 0.1006
on_train_batch_begin: 1607808433.924286s

33 step training time: 0.064470s

on_train_batch_end: 1607808433.990827s

34816/50000 [===================>..........] - ETA: 0s - loss: 2.2957 - accuracy: 0.1006
on_train_batch_begin: 1607808433.991204s

34 step training time: 0.066917s

on_train_batch_end: 1607808434.054214s

35840/50000 [====================>.........] - ETA: 0s - loss: 2.2887 - accuracy: 0.1006
on_train_batch_begin: 1607808434.054625s

35 step training time: 0.063421s

on_train_batch_end: 1607808434.122394s

36864/50000 [=====================>........] - ETA: 0s - loss: 2.2804 - accuracy: 0.1006
on_train_batch_begin: 1607808434.122777s

36 step training time: 0.068152s

on_train_batch_end: 1607808434.189628s

37888/50000 [=====================>........] - ETA: 0s - loss: 2.2764 - accuracy: 0.1006
on_train_batch_begin: 1607808434.190024s

37 step training time: 0.067247s

on_train_batch_end: 1607808434.254934s

38912/50000 [======================>.......] - ETA: 0s - loss: 2.2651 - accuracy: 0.1006
on_train_batch_begin: 1607808434.255319s

38 step training time: 0.065296s

on_train_batch_end: 1607808434.318264s

39936/50000 [======================>.......] - ETA: 0s - loss: 2.2580 - accuracy: 0.1006
on_train_batch_begin: 1607808434.318676s

39 step training time: 0.063356s

on_train_batch_end: 1607808434.381387s

40960/50000 [=======================>......] - ETA: 0s - loss: 2.2481 - accuracy: 0.1006
on_train_batch_begin: 1607808434.381772s

40 step training time: 0.063096s

on_train_batch_end: 1607808434.448302s

41984/50000 [========================>.....] - ETA: 0s - loss: 2.2405 - accuracy: 0.1006
on_train_batch_begin: 1607808434.448687s

41 step training time: 0.066916s

on_train_batch_end: 1607808434.511846s

43008/50000 [========================>.....] - ETA: 0s - loss: 2.2294 - accuracy: 0.1006
on_train_batch_begin: 1607808434.512227s

42 step training time: 0.063539s

on_train_batch_end: 1607808434.577607s

44032/50000 [=========================>....] - ETA: 0s - loss: 2.2199 - accuracy: 0.1006
on_train_batch_begin: 1607808434.577999s

43 step training time: 0.065773s

on_train_batch_end: 1607808434.641599s

45056/50000 [==========================>...] - ETA: 0s - loss: 2.2136 - accuracy: 0.1006
on_train_batch_begin: 1607808434.641988s

44 step training time: 0.063988s

on_train_batch_end: 1607808434.711559s

46080/50000 [==========================>...] - ETA: 0s - loss: 2.2074 - accuracy: 0.1006
on_train_batch_begin: 1607808434.711948s

45 step training time: 0.069960s

on_train_batch_end: 1607808434.778063s

47104/50000 [===========================>..] - ETA: 0s - loss: 2.1959 - accuracy: 0.1006
on_train_batch_begin: 1607808434.778472s

46 step training time: 0.066525s

on_train_batch_end: 1607808434.840936s

48128/50000 [===========================>..] - ETA: 0s - loss: 2.1864 - accuracy: 0.1006
on_train_batch_begin: 1607808434.841333s

47 step training time: 0.062861s

on_train_batch_end: 1607808434.907183s

49152/50000 [============================>.] - ETA: 0s - loss: 2.1742 - accuracy: 0.1006
on_train_batch_begin: 1607808434.907564s

48 step training time: 0.066231s

on_train_batch_end: 1607808434.968565s

on_test_batch_begin: 1607808434.991175s

49 step training time: 0.083611s

on_epoch_end: 1607808435.238238s

Validation time: 0.247047s

Real time: 1607808435.238238s

Epoch time: 3.499436140060425s

50000/50000 [==============================] - 3s 70us/sample - loss: 2.1666 - accuracy: 0.1006 - val_loss: 7.6979 - val_accuracy: 0.0000e+00

on_epoch_begin: 1607808435.238522s

Real time: 1607808435.2385314
Epoch 4/5

on_train_batch_begin: 1607808435.244114s

on_train_batch_end: 1607808435.313298s

 1024/50000 [..............................] - ETA: 3s - loss: 1.5963 - accuracy: 0.1013
on_train_batch_begin: 1607808435.313690s

1 step training time: 0.069576s

on_train_batch_end: 1607808435.376459s

 2048/50000 [>.............................] - ETA: 3s - loss: 1.5328 - accuracy: 0.1013
on_train_batch_begin: 1607808435.376848s

2 step training time: 0.063159s

on_train_batch_end: 1607808435.441919s

 3072/50000 [>.............................] - ETA: 3s - loss: 1.5609 - accuracy: 0.1013
on_train_batch_begin: 1607808435.442327s

3 step training time: 0.065479s

on_train_batch_end: 1607808435.507211s

 4096/50000 [=>............................] - ETA: 3s - loss: 1.5674 - accuracy: 0.1010
on_train_batch_begin: 1607808435.507593s

4 step training time: 0.065266s

on_train_batch_end: 1607808435.575562s

 5120/50000 [==>...........................] - ETA: 2s - loss: 1.5626 - accuracy: 0.1010
on_train_batch_begin: 1607808435.575955s

5 step training time: 0.068362s

on_train_batch_end: 1607808435.640320s

 6144/50000 [==>...........................] - ETA: 2s - loss: 1.5740 - accuracy: 0.1011
on_train_batch_begin: 1607808435.640705s

6 step training time: 0.064750s

on_train_batch_end: 1607808435.707862s

 7168/50000 [===>..........................] - ETA: 2s - loss: 1.5615 - accuracy: 0.1011
on_train_batch_begin: 1607808435.708246s

7 step training time: 0.067541s

on_train_batch_end: 1607808435.775441s

 8192/50000 [===>..........................] - ETA: 2s - loss: 1.5867 - accuracy: 0.1010
on_train_batch_begin: 1607808435.775828s

8 step training time: 0.067582s

on_train_batch_end: 1607808435.843457s

 9216/50000 [====>.........................] - ETA: 2s - loss: 1.5911 - accuracy: 0.1011
on_train_batch_begin: 1607808435.843848s

9 step training time: 0.068020s

on_train_batch_end: 1607808435.907704s

10240/50000 [=====>........................] - ETA: 2s - loss: 1.5941 - accuracy: 0.1010
on_train_batch_begin: 1607808435.908084s

10 step training time: 0.064236s

on_train_batch_end: 1607808435.974561s

11264/50000 [=====>........................] - ETA: 2s - loss: 1.5989 - accuracy: 0.1009
on_train_batch_begin: 1607808435.974947s

11 step training time: 0.066862s

on_train_batch_end: 1607808436.037304s

12288/50000 [======>.......................] - ETA: 2s - loss: 1.5968 - accuracy: 0.1009
on_train_batch_begin: 1607808436.037696s

12 step training time: 0.062750s

on_train_batch_end: 1607808436.100675s

13312/50000 [======>.......................] - ETA: 2s - loss: 1.5995 - accuracy: 0.1009
on_train_batch_begin: 1607808436.101065s

13 step training time: 0.063369s

on_train_batch_end: 1607808436.163725s

14336/50000 [=======>......................] - ETA: 2s - loss: 1.6015 - accuracy: 0.1010
on_train_batch_begin: 1607808436.164134s

14 step training time: 0.063069s

on_train_batch_end: 1607808436.228173s

15360/50000 [========>.....................] - ETA: 2s - loss: 1.5951 - accuracy: 0.1010
on_train_batch_begin: 1607808436.228564s

15 step training time: 0.064430s

on_train_batch_end: 1607808436.298947s

16384/50000 [========>.....................] - ETA: 2s - loss: 1.5808 - accuracy: 0.1010
on_train_batch_begin: 1607808436.299328s

16 step training time: 0.070764s

on_train_batch_end: 1607808436.361512s

17408/50000 [=========>....................] - ETA: 2s - loss: 1.5707 - accuracy: 0.1010
on_train_batch_begin: 1607808436.361895s

17 step training time: 0.062567s

on_train_batch_end: 1607808436.425037s

18432/50000 [==========>...................] - ETA: 2s - loss: 1.5682 - accuracy: 0.1010
on_train_batch_begin: 1607808436.425423s

18 step training time: 0.063528s

on_train_batch_end: 1607808436.490317s

19456/50000 [==========>...................] - ETA: 1s - loss: 1.5638 - accuracy: 0.1010
on_train_batch_begin: 1607808436.490698s

19 step training time: 0.065275s

on_train_batch_end: 1607808436.555074s

20480/50000 [===========>..................] - ETA: 1s - loss: 1.5590 - accuracy: 0.1010
on_train_batch_begin: 1607808436.555464s

20 step training time: 0.064766s

on_train_batch_end: 1607808436.619761s

21504/50000 [===========>..................] - ETA: 1s - loss: 1.5489 - accuracy: 0.1010
on_train_batch_begin: 1607808436.620142s

21 step training time: 0.064678s

on_train_batch_end: 1607808436.689466s

22528/50000 [============>.................] - ETA: 1s - loss: 1.5424 - accuracy: 0.1010
on_train_batch_begin: 1607808436.689858s

22 step training time: 0.069716s

on_train_batch_end: 1607808436.757497s

23552/50000 [=============>................] - ETA: 1s - loss: 1.5323 - accuracy: 0.1010
on_train_batch_begin: 1607808436.757886s

23 step training time: 0.068028s

on_train_batch_end: 1607808436.821447s

24576/50000 [=============>................] - ETA: 1s - loss: 1.5321 - accuracy: 0.1011
on_train_batch_begin: 1607808436.821823s

24 step training time: 0.063937s

on_train_batch_end: 1607808436.885676s

25600/50000 [==============>...............] - ETA: 1s - loss: 1.5269 - accuracy: 0.1010
on_train_batch_begin: 1607808436.886060s

25 step training time: 0.064237s

on_train_batch_end: 1607808436.948450s

26624/50000 [==============>...............] - ETA: 1s - loss: 1.5242 - accuracy: 0.1010
on_train_batch_begin: 1607808436.948828s

26 step training time: 0.062768s

on_train_batch_end: 1607808437.014316s

27648/50000 [===============>..............] - ETA: 1s - loss: 1.5212 - accuracy: 0.1010
on_train_batch_begin: 1607808437.014704s

27 step training time: 0.065876s

on_train_batch_end: 1607808437.078464s

28672/50000 [================>.............] - ETA: 1s - loss: 1.5193 - accuracy: 0.1010
on_train_batch_begin: 1607808437.078843s

28 step training time: 0.064139s

on_train_batch_end: 1607808437.141885s

29696/50000 [================>.............] - ETA: 1s - loss: 1.5178 - accuracy: 0.1010
on_train_batch_begin: 1607808437.142301s

29 step training time: 0.063458s

on_train_batch_end: 1607808437.205534s

30720/50000 [=================>............] - ETA: 1s - loss: 1.5191 - accuracy: 0.1011
on_train_batch_begin: 1607808437.205921s

30 step training time: 0.063620s

on_train_batch_end: 1607808437.269371s

31744/50000 [==================>...........] - ETA: 1s - loss: 1.5147 - accuracy: 0.1011
on_train_batch_begin: 1607808437.269750s

31 step training time: 0.063829s

on_train_batch_end: 1607808437.333730s

32768/50000 [==================>...........] - ETA: 1s - loss: 1.5098 - accuracy: 0.1011
on_train_batch_begin: 1607808437.334118s

32 step training time: 0.064367s

on_train_batch_end: 1607808437.401901s

33792/50000 [===================>..........] - ETA: 1s - loss: 1.5042 - accuracy: 0.1011
on_train_batch_begin: 1607808437.402299s

33 step training time: 0.068182s

on_train_batch_end: 1607808437.466190s

34816/50000 [===================>..........] - ETA: 0s - loss: 1.4975 - accuracy: 0.1011
on_train_batch_begin: 1607808437.466601s

34 step training time: 0.064302s

on_train_batch_end: 1607808437.529181s

35840/50000 [====================>.........] - ETA: 0s - loss: 1.4945 - accuracy: 0.1011
on_train_batch_begin: 1607808437.529563s

35 step training time: 0.062962s

on_train_batch_end: 1607808437.593354s

36864/50000 [=====================>........] - ETA: 0s - loss: 1.4897 - accuracy: 0.1011
on_train_batch_begin: 1607808437.593742s

36 step training time: 0.064179s

on_train_batch_end: 1607808437.657893s

37888/50000 [=====================>........] - ETA: 0s - loss: 1.4834 - accuracy: 0.1011
on_train_batch_begin: 1607808437.658303s

37 step training time: 0.064561s

on_train_batch_end: 1607808437.725319s

38912/50000 [======================>.......] - ETA: 0s - loss: 1.4746 - accuracy: 0.1011
on_train_batch_begin: 1607808437.725714s

38 step training time: 0.067411s

on_train_batch_end: 1607808437.794219s

39936/50000 [======================>.......] - ETA: 0s - loss: 1.4726 - accuracy: 0.1011
on_train_batch_begin: 1607808437.794638s

39 step training time: 0.068924s

on_train_batch_end: 1607808437.858588s

40960/50000 [=======================>......] - ETA: 0s - loss: 1.4678 - accuracy: 0.1012
on_train_batch_begin: 1607808437.858976s

40 step training time: 0.064338s

on_train_batch_end: 1607808437.926470s

41984/50000 [========================>.....] - ETA: 0s - loss: 1.4661 - accuracy: 0.1012
on_train_batch_begin: 1607808437.926855s

41 step training time: 0.067878s

on_train_batch_end: 1607808437.991458s

43008/50000 [========================>.....] - ETA: 0s - loss: 1.4621 - accuracy: 0.1011
on_train_batch_begin: 1607808437.991848s

42 step training time: 0.064993s

on_train_batch_end: 1607808438.056337s

44032/50000 [=========================>....] - ETA: 0s - loss: 1.4610 - accuracy: 0.1011
on_train_batch_begin: 1607808438.056717s

43 step training time: 0.064869s

on_train_batch_end: 1607808438.119714s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.4581 - accuracy: 0.1011
on_train_batch_begin: 1607808438.120095s

44 step training time: 0.063378s

on_train_batch_end: 1607808438.183345s

46080/50000 [==========================>...] - ETA: 0s - loss: 1.4541 - accuracy: 0.1011
on_train_batch_begin: 1607808438.183732s

45 step training time: 0.063637s

on_train_batch_end: 1607808438.248166s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.4492 - accuracy: 0.1011
on_train_batch_begin: 1607808438.248549s

46 step training time: 0.064817s

on_train_batch_end: 1607808438.315490s

48128/50000 [===========================>..] - ETA: 0s - loss: 1.4459 - accuracy: 0.1011
on_train_batch_begin: 1607808438.315881s

47 step training time: 0.067332s

on_train_batch_end: 1607808438.378896s

49152/50000 [============================>.] - ETA: 0s - loss: 1.4457 - accuracy: 0.1011
on_train_batch_begin: 1607808438.379300s

48 step training time: 0.063419s

on_train_batch_end: 1607808438.440596s

on_test_batch_begin: 1607808438.464107s

49 step training time: 0.084806s

on_epoch_end: 1607808438.718483s

Validation time: 0.254361s

Real time: 1607808438.718483s

Epoch time: 3.4799747467041016s

50000/50000 [==============================] - 3s 70us/sample - loss: 1.4440 - accuracy: 0.1011 - val_loss: 7.2044 - val_accuracy: 0.0999

on_epoch_begin: 1607808438.718740s

Real time: 1607808438.7187493
Epoch 5/5

on_train_batch_begin: 1607808438.724416s

on_train_batch_end: 1607808438.789722s

 1024/50000 [..............................] - ETA: 3s - loss: 1.1281 - accuracy: 0.1009
on_train_batch_begin: 1607808438.790108s

1 step training time: 0.065693s

on_train_batch_end: 1607808438.856602s

 2048/50000 [>.............................] - ETA: 3s - loss: 1.1621 - accuracy: 0.1011
on_train_batch_begin: 1607808438.856986s

2 step training time: 0.066877s

on_train_batch_end: 1607808438.920215s

 3072/50000 [>.............................] - ETA: 3s - loss: 1.2025 - accuracy: 0.1011
on_train_batch_begin: 1607808438.920608s

3 step training time: 0.063623s

on_train_batch_end: 1607808438.985360s

 4096/50000 [=>............................] - ETA: 2s - loss: 1.1899 - accuracy: 0.1011
on_train_batch_begin: 1607808438.985746s

4 step training time: 0.065138s

on_train_batch_end: 1607808439.050654s

 5120/50000 [==>...........................] - ETA: 2s - loss: 1.1628 - accuracy: 0.1012
on_train_batch_begin: 1607808439.051034s

5 step training time: 0.065288s

on_train_batch_end: 1607808439.114963s

 6144/50000 [==>...........................] - ETA: 2s - loss: 1.1402 - accuracy: 0.1012
on_train_batch_begin: 1607808439.115338s

6 step training time: 0.064304s

on_train_batch_end: 1607808439.183091s

 7168/50000 [===>..........................] - ETA: 2s - loss: 1.1234 - accuracy: 0.1013
on_train_batch_begin: 1607808439.183496s

7 step training time: 0.068158s

on_train_batch_end: 1607808439.246496s

 8192/50000 [===>..........................] - ETA: 2s - loss: 1.1132 - accuracy: 0.1014
on_train_batch_begin: 1607808439.246887s

8 step training time: 0.063390s

on_train_batch_end: 1607808439.316157s

 9216/50000 [====>.........................] - ETA: 2s - loss: 1.1221 - accuracy: 0.1013
on_train_batch_begin: 1607808439.316542s

9 step training time: 0.069655s

on_train_batch_end: 1607808439.384291s

10240/50000 [=====>........................] - ETA: 2s - loss: 1.1242 - accuracy: 0.1013
on_train_batch_begin: 1607808439.384678s

10 step training time: 0.068136s

on_train_batch_end: 1607808439.447922s

11264/50000 [=====>........................] - ETA: 2s - loss: 1.1213 - accuracy: 0.1013
on_train_batch_begin: 1607808439.448303s

11 step training time: 0.063625s

on_train_batch_end: 1607808439.512403s

12288/50000 [======>.......................] - ETA: 2s - loss: 1.1217 - accuracy: 0.1012
on_train_batch_begin: 1607808439.512784s

12 step training time: 0.064481s

on_train_batch_end: 1607808439.575429s

13312/50000 [======>.......................] - ETA: 2s - loss: 1.1240 - accuracy: 0.1012
on_train_batch_begin: 1607808439.575826s

13 step training time: 0.063042s

on_train_batch_end: 1607808439.641827s

14336/50000 [=======>......................] - ETA: 2s - loss: 1.1254 - accuracy: 0.1012
on_train_batch_begin: 1607808439.642219s

14 step training time: 0.066393s

on_train_batch_end: 1607808439.706681s

15360/50000 [========>.....................] - ETA: 2s - loss: 1.1256 - accuracy: 0.1013
on_train_batch_begin: 1607808439.707061s

15 step training time: 0.064842s

on_train_batch_end: 1607808439.772125s

16384/50000 [========>.....................] - ETA: 2s - loss: 1.1243 - accuracy: 0.1013
on_train_batch_begin: 1607808439.772516s

16 step training time: 0.065455s

on_train_batch_end: 1607808439.835884s

17408/50000 [=========>....................] - ETA: 2s - loss: 1.1253 - accuracy: 0.1014
on_train_batch_begin: 1607808439.836270s

17 step training time: 0.063754s

on_train_batch_end: 1607808439.901393s

18432/50000 [==========>...................] - ETA: 2s - loss: 1.1259 - accuracy: 0.1014
on_train_batch_begin: 1607808439.901774s

18 step training time: 0.065504s

on_train_batch_end: 1607808439.968173s

19456/50000 [==========>...................] - ETA: 1s - loss: 1.1258 - accuracy: 0.1014
on_train_batch_begin: 1607808439.968555s

19 step training time: 0.066782s

on_train_batch_end: 1607808440.032360s

20480/50000 [===========>..................] - ETA: 1s - loss: 1.1199 - accuracy: 0.1013
on_train_batch_begin: 1607808440.032749s

20 step training time: 0.064193s

on_train_batch_end: 1607808440.097164s

21504/50000 [===========>..................] - ETA: 1s - loss: 1.1088 - accuracy: 0.1014
on_train_batch_begin: 1607808440.097558s

21 step training time: 0.064809s

on_train_batch_end: 1607808440.161472s

22528/50000 [============>.................] - ETA: 1s - loss: 1.1037 - accuracy: 0.1014
on_train_batch_begin: 1607808440.161853s

22 step training time: 0.064295s

on_train_batch_end: 1607808440.230700s

23552/50000 [=============>................] - ETA: 1s - loss: 1.1000 - accuracy: 0.1014
on_train_batch_begin: 1607808440.231084s

23 step training time: 0.069231s

on_train_batch_end: 1607808440.294770s

24576/50000 [=============>................] - ETA: 1s - loss: 1.0994 - accuracy: 0.1014
on_train_batch_begin: 1607808440.295157s

24 step training time: 0.064074s

on_train_batch_end: 1607808440.363107s

25600/50000 [==============>...............] - ETA: 1s - loss: 1.1030 - accuracy: 0.1014
on_train_batch_begin: 1607808440.363497s

25 step training time: 0.068340s

on_train_batch_end: 1607808440.427523s

26624/50000 [==============>...............] - ETA: 1s - loss: 1.1012 - accuracy: 0.1013
on_train_batch_begin: 1607808440.427917s

26 step training time: 0.064420s

on_train_batch_end: 1607808440.491401s

27648/50000 [===============>..............] - ETA: 1s - loss: 1.0930 - accuracy: 0.1013
on_train_batch_begin: 1607808440.491787s

27 step training time: 0.063870s

on_train_batch_end: 1607808440.555990s

28672/50000 [================>.............] - ETA: 1s - loss: 1.0866 - accuracy: 0.1013
on_train_batch_begin: 1607808440.556373s

28 step training time: 0.064586s

on_train_batch_end: 1607808440.618526s

29696/50000 [================>.............] - ETA: 1s - loss: 1.0824 - accuracy: 0.1013
on_train_batch_begin: 1607808440.618915s

29 step training time: 0.062542s

on_train_batch_end: 1607808440.687087s

30720/50000 [=================>............] - ETA: 1s - loss: 1.0830 - accuracy: 0.1013
on_train_batch_begin: 1607808440.687480s

30 step training time: 0.068565s

on_train_batch_end: 1607808440.751647s

31744/50000 [==================>...........] - ETA: 1s - loss: 1.0760 - accuracy: 0.1013
on_train_batch_begin: 1607808440.752035s

31 step training time: 0.064555s

on_train_batch_end: 1607808440.815736s

32768/50000 [==================>...........] - ETA: 1s - loss: 1.0727 - accuracy: 0.1013
on_train_batch_begin: 1607808440.816117s

32 step training time: 0.064081s

on_train_batch_end: 1607808440.880917s

33792/50000 [===================>..........] - ETA: 1s - loss: 1.0710 - accuracy: 0.1013
on_train_batch_begin: 1607808440.881307s

33 step training time: 0.065191s

on_train_batch_end: 1607808440.948581s

34816/50000 [===================>..........] - ETA: 0s - loss: 1.0691 - accuracy: 0.1013
on_train_batch_begin: 1607808440.948972s

34 step training time: 0.067664s

on_train_batch_end: 1607808441.012759s

35840/50000 [====================>.........] - ETA: 0s - loss: 1.0656 - accuracy: 0.1013
on_train_batch_begin: 1607808441.013139s

35 step training time: 0.064168s

on_train_batch_end: 1607808441.076923s

36864/50000 [=====================>........] - ETA: 0s - loss: 1.0654 - accuracy: 0.1013
on_train_batch_begin: 1607808441.077305s

36 step training time: 0.064166s

on_train_batch_end: 1607808441.140851s

37888/50000 [=====================>........] - ETA: 0s - loss: 1.0642 - accuracy: 0.1013
on_train_batch_begin: 1607808441.141236s

37 step training time: 0.063931s

on_train_batch_end: 1607808441.208877s

38912/50000 [======================>.......] - ETA: 0s - loss: 1.0621 - accuracy: 0.1013
on_train_batch_begin: 1607808441.209261s

38 step training time: 0.068025s

on_train_batch_end: 1607808441.276403s

39936/50000 [======================>.......] - ETA: 0s - loss: 1.0586 - accuracy: 0.1013
on_train_batch_begin: 1607808441.276791s

39 step training time: 0.067530s

on_train_batch_end: 1607808441.341563s

40960/50000 [=======================>......] - ETA: 0s - loss: 1.0603 - accuracy: 0.1014
on_train_batch_begin: 1607808441.341949s

40 step training time: 0.065159s

on_train_batch_end: 1607808441.406514s

41984/50000 [========================>.....] - ETA: 0s - loss: 1.0592 - accuracy: 0.1014
on_train_batch_begin: 1607808441.406902s

41 step training time: 0.064952s

on_train_batch_end: 1607808441.471767s

43008/50000 [========================>.....] - ETA: 0s - loss: 1.0569 - accuracy: 0.1014
on_train_batch_begin: 1607808441.472147s

42 step training time: 0.065246s

on_train_batch_end: 1607808441.535300s

44032/50000 [=========================>....] - ETA: 0s - loss: 1.0568 - accuracy: 0.1014
on_train_batch_begin: 1607808441.535697s

43 step training time: 0.063550s

on_train_batch_end: 1607808441.599799s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.0548 - accuracy: 0.1014
on_train_batch_begin: 1607808441.600190s

44 step training time: 0.064493s

on_train_batch_end: 1607808441.664425s

46080/50000 [==========================>...] - ETA: 0s - loss: 1.0521 - accuracy: 0.1014
on_train_batch_begin: 1607808441.664816s

45 step training time: 0.064626s

on_train_batch_end: 1607808441.729367s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.0505 - accuracy: 0.1014
on_train_batch_begin: 1607808441.729745s

46 step training time: 0.064929s

on_train_batch_end: 1607808441.798874s

48128/50000 [===========================>..] - ETA: 0s - loss: 1.0474 - accuracy: 0.1014
on_train_batch_begin: 1607808441.799268s

47 step training time: 0.069523s

on_train_batch_end: 1607808441.863386s

49152/50000 [============================>.] - ETA: 0s - loss: 1.0469 - accuracy: 0.1014
on_train_batch_begin: 1607808441.863766s

48 step training time: 0.064498s

on_train_batch_end: 1607808441.929400s

on_test_batch_begin: 1607808441.953832s

49 step training time: 0.090065s

on_epoch_end: 1607808442.203123s

Validation time: 0.249274s

Real time: 1607808442.203123s

Epoch time: 3.484395742416382s

50000/50000 [==============================] - 3s 70us/sample - loss: 1.0461 - accuracy: 0.1014 - val_loss: 7.3406 - val_accuracy: 0.0999
Tempo do fit: 90.43958640098572