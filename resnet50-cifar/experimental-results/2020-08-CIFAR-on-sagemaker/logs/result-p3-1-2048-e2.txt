wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 8:00
   204800/170498071 [..............................] - ETA: 1:01
   794624/170498071 [..............................] - ETA: 26s 
  2285568/170498071 [..............................] - ETA: 12s
  4202496/170498071 [..............................] - ETA: 8s 
  7233536/170498071 [>.............................] - ETA: 6s
 10559488/170498071 [>.............................] - ETA: 4s
 13623296/170498071 [=>............................] - ETA: 4s
 16801792/170498071 [=>............................] - ETA: 3s
 20127744/170498071 [==>...........................] - ETA: 3s
 23568384/170498071 [===>..........................] - ETA: 3s
 27189248/170498071 [===>..........................] - ETA: 3s
 30744576/170498071 [====>.........................] - ETA: 2s
 34119680/170498071 [=====>........................] - ETA: 2s
 37560320/170498071 [=====>........................] - ETA: 2s
 41123840/170498071 [======>.......................] - ETA: 2s
 44785664/170498071 [======>.......................] - ETA: 2s
 48177152/170498071 [=======>......................] - ETA: 2s
 51617792/170498071 [========>.....................] - ETA: 2s
 55140352/170498071 [========>.....................] - ETA: 2s
 58810368/170498071 [=========>....................] - ETA: 1s
 62218240/170498071 [=========>....................] - ETA: 1s
 65675264/170498071 [==========>...................] - ETA: 1s
 69230592/170498071 [===========>..................] - ETA: 1s
 72818688/170498071 [===========>..................] - ETA: 1s
 76292096/170498071 [============>.................] - ETA: 1s
 79732736/170498071 [=============>................] - ETA: 1s
 83214336/170498071 [=============>................] - ETA: 1s
 86761472/170498071 [==============>...............] - ETA: 1s
 90333184/170498071 [==============>...............] - ETA: 1s
 93790208/170498071 [===============>..............] - ETA: 1s
 97198080/170498071 [================>.............] - ETA: 1s
100810752/170498071 [================>.............] - ETA: 1s
104390656/170498071 [=================>............] - ETA: 1s
107782144/170498071 [=================>............] - ETA: 1s
111190016/170498071 [==================>...........] - ETA: 0s
114745344/170498071 [===================>..........] - ETA: 0s
118349824/170498071 [===================>..........] - ETA: 0s
121741312/170498071 [====================>.........] - ETA: 0s
125149184/170498071 [=====================>........] - ETA: 0s
128598016/170498071 [=====================>........] - ETA: 0s
132243456/170498071 [======================>.......] - ETA: 0s
135700480/170498071 [======================>.......] - ETA: 0s
139141120/170498071 [=======================>......] - ETA: 0s
142532608/170498071 [========================>.....] - ETA: 0s
146128896/170498071 [========================>.....] - ETA: 0s
149741568/170498071 [=========================>....] - ETA: 0s
153198592/170498071 [=========================>....] - ETA: 0s
156590080/170498071 [==========================>...] - ETA: 0s
160096256/170498071 [===========================>..] - ETA: 0s
163717120/170498071 [===========================>..] - ETA: 0s
167149568/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 2s
 6848512/94765736 [=>............................] - ETA: 0s
13549568/94765736 [===>..........................] - ETA: 0s
18841600/94765736 [====>.........................] - ETA: 0s
26034176/94765736 [=======>......................] - ETA: 0s
29409280/94765736 [========>.....................] - ETA: 0s
36732928/94765736 [==========>...................] - ETA: 0s
42467328/94765736 [============>.................] - ETA: 0s
47136768/94765736 [=============>................] - ETA: 0s
51445760/94765736 [===============>..............] - ETA: 0s
56598528/94765736 [================>.............] - ETA: 0s
60964864/94765736 [==================>...........] - ETA: 0s
68345856/94765736 [====================>.........] - ETA: 0s
74997760/94765736 [======================>.......] - ETA: 0s
76054528/94765736 [=======================>......] - ETA: 0s
82239488/94765736 [=========================>....] - ETA: 0s
84893696/94765736 [=========================>....] - ETA: 0s
85409792/94765736 [==========================>...] - ETA: 0s
89718784/94765736 [===========================>..] - ETA: 0s
94633984/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 1s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 14.405145645141602
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1609194922.584735s

Real time: 1609194922.5847583
Epoch 1/5

on_train_batch_begin: 1609194923.431255s

on_train_batch_end: 1609194940.586752s

 2048/50000 [>.............................] - ETA: 7:01 - loss: 17.9549 - accuracy: 1.4377e-04
on_train_batch_begin: 1609194940.587462s

1 step training time: 17.156207s

on_train_batch_end: 1609194940.806540s

 4096/50000 [=>............................] - ETA: 3:24 - loss: 13.8212 - accuracy: 3.9363e-04
on_train_batch_begin: 1609194940.806945s

2 step training time: 0.219483s

on_train_batch_end: 1609194941.025679s

 6144/50000 [==>...........................] - ETA: 2:11 - loss: 11.9483 - accuracy: 8.5521e-04
on_train_batch_begin: 1609194941.026062s

3 step training time: 0.219116s

on_train_batch_end: 1609194941.241719s

 8192/50000 [===>..........................] - ETA: 1:35 - loss: 10.9708 - accuracy: 0.0018    
on_train_batch_begin: 1609194941.242101s

4 step training time: 0.216040s

on_train_batch_end: 1609194941.455016s

10240/50000 [=====>........................] - ETA: 1:13 - loss: 10.3496 - accuracy: 0.0042
on_train_batch_begin: 1609194941.455418s

5 step training time: 0.213317s

on_train_batch_end: 1609194941.670854s

12288/50000 [======>.......................] - ETA: 58s - loss: 9.9389 - accuracy: 0.0066  
on_train_batch_begin: 1609194941.671240s

6 step training time: 0.215822s

on_train_batch_end: 1609194941.886889s

14336/50000 [=======>......................] - ETA: 48s - loss: 9.6485 - accuracy: 0.0089
on_train_batch_begin: 1609194941.887278s

7 step training time: 0.216038s

on_train_batch_end: 1609194942.104740s

16384/50000 [========>.....................] - ETA: 40s - loss: 9.3897 - accuracy: 0.0099
on_train_batch_begin: 1609194942.105122s

8 step training time: 0.217845s

on_train_batch_end: 1609194942.319885s

18432/50000 [==========>...................] - ETA: 33s - loss: 9.1737 - accuracy: 0.0116
on_train_batch_begin: 1609194942.320266s

9 step training time: 0.215143s

on_train_batch_end: 1609194942.535860s

20480/50000 [===========>..................] - ETA: 28s - loss: 9.0116 - accuracy: 0.0119
on_train_batch_begin: 1609194942.536247s

10 step training time: 0.215981s

on_train_batch_end: 1609194942.756490s

22528/50000 [============>.................] - ETA: 24s - loss: 8.8865 - accuracy: 0.0131
on_train_batch_begin: 1609194942.756873s

11 step training time: 0.220627s

on_train_batch_end: 1609194942.971346s

24576/50000 [=============>................] - ETA: 21s - loss: 8.7617 - accuracy: 0.0149
on_train_batch_begin: 1609194942.971720s

12 step training time: 0.214846s

on_train_batch_end: 1609194943.188968s

26624/50000 [==============>...............] - ETA: 18s - loss: 8.6541 - accuracy: 0.0176
on_train_batch_begin: 1609194943.189359s

13 step training time: 0.217639s

on_train_batch_end: 1609194943.406182s

28672/50000 [================>.............] - ETA: 15s - loss: 8.5498 - accuracy: 0.0201
on_train_batch_begin: 1609194943.406593s

14 step training time: 0.217234s

on_train_batch_end: 1609194943.620106s

30720/50000 [=================>............] - ETA: 13s - loss: 8.4517 - accuracy: 0.0224
on_train_batch_begin: 1609194943.620482s

15 step training time: 0.213889s

on_train_batch_end: 1609194943.835569s

32768/50000 [==================>...........] - ETA: 11s - loss: 8.3578 - accuracy: 0.0250
on_train_batch_begin: 1609194943.835939s

16 step training time: 0.215457s

on_train_batch_end: 1609194944.050942s

34816/50000 [===================>..........] - ETA: 9s - loss: 8.2749 - accuracy: 0.0277 
on_train_batch_begin: 1609194944.051397s

17 step training time: 0.215457s

on_train_batch_end: 1609194944.265956s

36864/50000 [=====================>........] - ETA: 7s - loss: 8.1873 - accuracy: 0.0298
on_train_batch_begin: 1609194944.266329s

18 step training time: 0.214932s

on_train_batch_end: 1609194944.482941s

38912/50000 [======================>.......] - ETA: 6s - loss: 8.1064 - accuracy: 0.0314
on_train_batch_begin: 1609194944.483315s

19 step training time: 0.216985s

on_train_batch_end: 1609194944.703813s

40960/50000 [=======================>......] - ETA: 4s - loss: 8.0279 - accuracy: 0.0332
on_train_batch_begin: 1609194944.704190s

20 step training time: 0.220876s

on_train_batch_end: 1609194944.924109s

43008/50000 [========================>.....] - ETA: 3s - loss: 7.9560 - accuracy: 0.0348
on_train_batch_begin: 1609194944.924490s

21 step training time: 0.220300s

on_train_batch_end: 1609194945.138489s

45056/50000 [==========================>...] - ETA: 2s - loss: 7.8901 - accuracy: 0.0361
on_train_batch_begin: 1609194945.138879s

22 step training time: 0.214388s

on_train_batch_end: 1609194945.353883s

47104/50000 [===========================>..] - ETA: 1s - loss: 7.8396 - accuracy: 0.0363
on_train_batch_begin: 1609194945.354266s

23 step training time: 0.215388s

on_train_batch_end: 1609194945.564618s

49152/50000 [============================>.] - ETA: 0s - loss: 7.7850 - accuracy: 0.0378
on_train_batch_begin: 1609194945.565008s

24 step training time: 0.210742s

on_train_batch_end: 1609194947.404306s

on_test_batch_begin: 1609194947.619950s

25 step training time: 2.054941s

on_epoch_end: 1609194951.119364s

Validation time: 3.499394s

Real time: 1609194951.119364s

Epoch time: 28.534626960754395s

50000/50000 [==============================] - 29s 571us/sample - loss: 7.7624 - accuracy: 0.0380 - val_loss: 66964.1595 - val_accuracy: 0.1001

on_epoch_begin: 1609194951.119633s

Real time: 1609194951.1196427
Epoch 2/5

on_train_batch_begin: 1609194951.123994s

on_train_batch_end: 1609194951.342042s

 2048/50000 [>.............................] - ETA: 5s - loss: 6.4307 - accuracy: 0.0735
on_train_batch_begin: 1609194951.342484s

1 step training time: 0.218490s

on_train_batch_end: 1609194951.557365s

 4096/50000 [=>............................] - ETA: 4s - loss: 6.3796 - accuracy: 0.0711
on_train_batch_begin: 1609194951.557745s

2 step training time: 0.215261s

on_train_batch_end: 1609194951.775144s

 6144/50000 [==>...........................] - ETA: 4s - loss: 6.3679 - accuracy: 0.0702
on_train_batch_begin: 1609194951.775530s

3 step training time: 0.217784s

on_train_batch_end: 1609194951.992003s

 8192/50000 [===>..........................] - ETA: 4s - loss: 6.3299 - accuracy: 0.0706
on_train_batch_begin: 1609194951.992385s

4 step training time: 0.216856s

on_train_batch_end: 1609194952.210051s

10240/50000 [=====>........................] - ETA: 4s - loss: 6.2899 - accuracy: 0.0701
on_train_batch_begin: 1609194952.210468s

5 step training time: 0.218082s

on_train_batch_end: 1609194952.430898s

12288/50000 [======>.......................] - ETA: 4s - loss: 6.2437 - accuracy: 0.0683
on_train_batch_begin: 1609194952.431281s

6 step training time: 0.220814s

on_train_batch_end: 1609194952.651533s

14336/50000 [=======>......................] - ETA: 3s - loss: 6.2057 - accuracy: 0.0676
on_train_batch_begin: 1609194952.651929s

7 step training time: 0.220648s

on_train_batch_end: 1609194952.871492s

16384/50000 [========>.....................] - ETA: 3s - loss: 6.1675 - accuracy: 0.0668
on_train_batch_begin: 1609194952.871879s

8 step training time: 0.219950s

on_train_batch_end: 1609194953.085978s

18432/50000 [==========>...................] - ETA: 3s - loss: 6.1290 - accuracy: 0.0666
on_train_batch_begin: 1609194953.086397s

9 step training time: 0.214518s

on_train_batch_end: 1609194953.301030s

20480/50000 [===========>..................] - ETA: 3s - loss: 6.1010 - accuracy: 0.0659
on_train_batch_begin: 1609194953.301420s

10 step training time: 0.215023s

on_train_batch_end: 1609194953.516773s

22528/50000 [============>.................] - ETA: 2s - loss: 6.0703 - accuracy: 0.0651
on_train_batch_begin: 1609194953.517150s

11 step training time: 0.215730s

on_train_batch_end: 1609194953.732540s

24576/50000 [=============>................] - ETA: 2s - loss: 6.0445 - accuracy: 0.0638
on_train_batch_begin: 1609194953.732924s

12 step training time: 0.215774s

on_train_batch_end: 1609194953.947128s

26624/50000 [==============>...............] - ETA: 2s - loss: 6.0175 - accuracy: 0.0634
on_train_batch_begin: 1609194953.947507s

13 step training time: 0.214583s

on_train_batch_end: 1609194954.162483s

28672/50000 [================>.............] - ETA: 2s - loss: 5.9995 - accuracy: 0.0638
on_train_batch_begin: 1609194954.162860s

14 step training time: 0.215353s

on_train_batch_end: 1609194954.379265s

30720/50000 [=================>............] - ETA: 2s - loss: 5.9735 - accuracy: 0.0641
on_train_batch_begin: 1609194954.379646s

15 step training time: 0.216787s

on_train_batch_end: 1609194954.597227s

32768/50000 [==================>...........] - ETA: 1s - loss: 5.9543 - accuracy: 0.0644
on_train_batch_begin: 1609194954.597604s

16 step training time: 0.217958s

on_train_batch_end: 1609194954.812403s

34816/50000 [===================>..........] - ETA: 1s - loss: 5.9301 - accuracy: 0.0650
on_train_batch_begin: 1609194954.812787s

17 step training time: 0.215182s

on_train_batch_end: 1609194955.028146s

36864/50000 [=====================>........] - ETA: 1s - loss: 5.9136 - accuracy: 0.0651
on_train_batch_begin: 1609194955.028607s

18 step training time: 0.215821s

on_train_batch_end: 1609194955.245971s

38912/50000 [======================>.......] - ETA: 1s - loss: 5.8867 - accuracy: 0.0651
on_train_batch_begin: 1609194955.246346s

19 step training time: 0.217739s

on_train_batch_end: 1609194955.461283s

40960/50000 [=======================>......] - ETA: 0s - loss: 5.8674 - accuracy: 0.0649
on_train_batch_begin: 1609194955.461656s

20 step training time: 0.215310s

on_train_batch_end: 1609194955.677318s

43008/50000 [========================>.....] - ETA: 0s - loss: 5.8462 - accuracy: 0.0644
on_train_batch_begin: 1609194955.677689s

21 step training time: 0.216033s

on_train_batch_end: 1609194955.892285s

45056/50000 [==========================>...] - ETA: 0s - loss: 5.8176 - accuracy: 0.0647
on_train_batch_begin: 1609194955.892673s

22 step training time: 0.214984s

on_train_batch_end: 1609194956.107958s

47104/50000 [===========================>..] - ETA: 0s - loss: 5.7935 - accuracy: 0.0646
on_train_batch_begin: 1609194956.108337s

23 step training time: 0.215664s

on_train_batch_end: 1609194956.321180s

49152/50000 [============================>.] - ETA: 0s - loss: 5.7778 - accuracy: 0.0641
on_train_batch_begin: 1609194956.321558s

24 step training time: 0.213221s

on_train_batch_end: 1609194956.427964s

on_test_batch_begin: 1609194956.459631s

25 step training time: 0.138072s

on_epoch_end: 1609194956.904537s

Validation time: 0.444890s

Real time: 1609194956.904537s

Epoch time: 5.784915447235107s

50000/50000 [==============================] - 6s 116us/sample - loss: 5.7703 - accuracy: 0.0641 - val_loss: 928.8001 - val_accuracy: 0.1001

on_epoch_begin: 1609194956.904792s

Real time: 1609194956.9048014
Epoch 3/5

on_train_batch_begin: 1609194956.909103s

on_train_batch_end: 1609194957.126554s

 2048/50000 [>.............................] - ETA: 5s - loss: 5.2508 - accuracy: 0.0536
on_train_batch_begin: 1609194957.126940s

1 step training time: 0.217837s

on_train_batch_end: 1609194957.342916s

 4096/50000 [=>............................] - ETA: 4s - loss: 5.2766 - accuracy: 0.0608
on_train_batch_begin: 1609194957.343296s

2 step training time: 0.216356s

on_train_batch_end: 1609194957.558286s

 6144/50000 [==>...........................] - ETA: 4s - loss: 5.2107 - accuracy: 0.0598
on_train_batch_begin: 1609194957.558700s

3 step training time: 0.215404s

on_train_batch_end: 1609194957.775519s

 8192/50000 [===>..........................] - ETA: 4s - loss: 5.1747 - accuracy: 0.0618
on_train_batch_begin: 1609194957.775901s

4 step training time: 0.217201s

on_train_batch_end: 1609194957.991554s

10240/50000 [=====>........................] - ETA: 4s - loss: 5.1710 - accuracy: 0.0631
on_train_batch_begin: 1609194957.991951s

5 step training time: 0.216051s

on_train_batch_end: 1609194958.206022s

12288/50000 [======>.......................] - ETA: 3s - loss: 5.1503 - accuracy: 0.0646
on_train_batch_begin: 1609194958.206430s

6 step training time: 0.214478s

on_train_batch_end: 1609194958.423872s

14336/50000 [=======>......................] - ETA: 3s - loss: 5.1402 - accuracy: 0.0663
on_train_batch_begin: 1609194958.424267s

7 step training time: 0.217837s

on_train_batch_end: 1609194958.639198s

16384/50000 [========>.....................] - ETA: 3s - loss: 5.1120 - accuracy: 0.0672
on_train_batch_begin: 1609194958.639586s

8 step training time: 0.215319s

on_train_batch_end: 1609194958.854484s

18432/50000 [==========>...................] - ETA: 3s - loss: 5.1077 - accuracy: 0.0676
on_train_batch_begin: 1609194958.854881s

9 step training time: 0.215295s

on_train_batch_end: 1609194959.068897s

20480/50000 [===========>..................] - ETA: 3s - loss: 5.1000 - accuracy: 0.0680
on_train_batch_begin: 1609194959.069307s

10 step training time: 0.214426s

on_train_batch_end: 1609194959.283117s

22528/50000 [============>.................] - ETA: 2s - loss: 5.0789 - accuracy: 0.0687
on_train_batch_begin: 1609194959.283502s

11 step training time: 0.214196s

on_train_batch_end: 1609194959.498747s

24576/50000 [=============>................] - ETA: 2s - loss: 5.0753 - accuracy: 0.0692
on_train_batch_begin: 1609194959.499131s

12 step training time: 0.215629s

on_train_batch_end: 1609194959.714491s

26624/50000 [==============>...............] - ETA: 2s - loss: 5.0607 - accuracy: 0.0691
on_train_batch_begin: 1609194959.714986s

13 step training time: 0.215854s

on_train_batch_end: 1609194959.929644s

28672/50000 [================>.............] - ETA: 2s - loss: 5.0383 - accuracy: 0.0691
on_train_batch_begin: 1609194959.930032s

14 step training time: 0.215046s

on_train_batch_end: 1609194960.143572s

30720/50000 [=================>............] - ETA: 2s - loss: 5.0031 - accuracy: 0.0690
on_train_batch_begin: 1609194960.143987s

15 step training time: 0.213955s

on_train_batch_end: 1609194960.361171s

32768/50000 [==================>...........] - ETA: 1s - loss: 4.9740 - accuracy: 0.0692
on_train_batch_begin: 1609194960.361548s

16 step training time: 0.217561s

on_train_batch_end: 1609194960.576715s

34816/50000 [===================>..........] - ETA: 1s - loss: 4.9359 - accuracy: 0.0695
on_train_batch_begin: 1609194960.577096s

17 step training time: 0.215548s

on_train_batch_end: 1609194960.791726s

36864/50000 [=====================>........] - ETA: 1s - loss: 4.9090 - accuracy: 0.0696
on_train_batch_begin: 1609194960.792114s

18 step training time: 0.215018s

on_train_batch_end: 1609194961.007859s

38912/50000 [======================>.......] - ETA: 1s - loss: 4.8645 - accuracy: 0.0703
on_train_batch_begin: 1609194961.008253s

19 step training time: 0.216139s

on_train_batch_end: 1609194961.226866s

40960/50000 [=======================>......] - ETA: 0s - loss: 4.8193 - accuracy: 0.0710
on_train_batch_begin: 1609194961.227266s

20 step training time: 0.219013s

on_train_batch_end: 1609194961.443573s

43008/50000 [========================>.....] - ETA: 0s - loss: 4.7726 - accuracy: 0.0717
on_train_batch_begin: 1609194961.443961s

21 step training time: 0.216696s

on_train_batch_end: 1609194961.663990s

45056/50000 [==========================>...] - ETA: 0s - loss: 4.7278 - accuracy: 0.0725
on_train_batch_begin: 1609194961.664376s

22 step training time: 0.220415s

on_train_batch_end: 1609194961.879542s

47104/50000 [===========================>..] - ETA: 0s - loss: 4.6876 - accuracy: 0.0732
on_train_batch_begin: 1609194961.879925s

23 step training time: 0.215549s

on_train_batch_end: 1609194962.093745s

49152/50000 [============================>.] - ETA: 0s - loss: 4.6401 - accuracy: 0.0739
on_train_batch_begin: 1609194962.094124s

24 step training time: 0.214199s

on_train_batch_end: 1609194962.199762s

on_test_batch_begin: 1609194962.230860s

25 step training time: 0.136736s

on_epoch_end: 1609194962.664210s

Validation time: 0.433335s

Real time: 1609194962.664210s

Epoch time: 5.759429931640625s

50000/50000 [==============================] - 6s 115us/sample - loss: 4.6205 - accuracy: 0.0740 - val_loss: 7.1885 - val_accuracy: 0.0000e+00

on_epoch_begin: 1609194962.664472s

Real time: 1609194962.6644816
Epoch 4/5

on_train_batch_begin: 1609194962.668807s

on_train_batch_end: 1609194962.886179s

 2048/50000 [>.............................] - ETA: 5s - loss: 3.2376 - accuracy: 0.0908
on_train_batch_begin: 1609194962.886588s

1 step training time: 0.217780s

on_train_batch_end: 1609194963.101105s

 4096/50000 [=>............................] - ETA: 4s - loss: 3.2779 - accuracy: 0.0921
on_train_batch_begin: 1609194963.101483s

2 step training time: 0.214895s

on_train_batch_end: 1609194963.317240s

 6144/50000 [==>...........................] - ETA: 4s - loss: 3.2988 - accuracy: 0.0916
on_train_batch_begin: 1609194963.317622s

3 step training time: 0.216140s

on_train_batch_end: 1609194963.535245s

 8192/50000 [===>..........................] - ETA: 4s - loss: 3.2685 - accuracy: 0.0915
on_train_batch_begin: 1609194963.535633s

4 step training time: 0.218011s

on_train_batch_end: 1609194963.755258s

10240/50000 [=====>........................] - ETA: 4s - loss: 3.2683 - accuracy: 0.0916
on_train_batch_begin: 1609194963.755753s

5 step training time: 0.220120s

on_train_batch_end: 1609194963.973301s

12288/50000 [======>.......................] - ETA: 4s - loss: 3.2394 - accuracy: 0.0919
on_train_batch_begin: 1609194963.973691s

6 step training time: 0.217938s

on_train_batch_end: 1609194964.189479s

14336/50000 [=======>......................] - ETA: 3s - loss: 3.2197 - accuracy: 0.0919
on_train_batch_begin: 1609194964.189874s

7 step training time: 0.216182s

on_train_batch_end: 1609194964.404823s

16384/50000 [========>.....................] - ETA: 3s - loss: 3.1634 - accuracy: 0.0922
on_train_batch_begin: 1609194964.405211s

8 step training time: 0.215337s

on_train_batch_end: 1609194964.620407s

18432/50000 [==========>...................] - ETA: 3s - loss: 3.1026 - accuracy: 0.0926
on_train_batch_begin: 1609194964.620795s

9 step training time: 0.215584s

on_train_batch_end: 1609194964.837536s

20480/50000 [===========>..................] - ETA: 3s - loss: 3.0550 - accuracy: 0.0933
on_train_batch_begin: 1609194964.837911s

10 step training time: 0.217117s

on_train_batch_end: 1609194965.053704s

22528/50000 [============>.................] - ETA: 2s - loss: 3.0030 - accuracy: 0.0938
on_train_batch_begin: 1609194965.054081s

11 step training time: 0.216170s

on_train_batch_end: 1609194965.270090s

24576/50000 [=============>................] - ETA: 2s - loss: 2.9558 - accuracy: 0.0941
on_train_batch_begin: 1609194965.270501s

12 step training time: 0.216420s

on_train_batch_end: 1609194965.491095s

26624/50000 [==============>...............] - ETA: 2s - loss: 2.9192 - accuracy: 0.0943
on_train_batch_begin: 1609194965.491491s

13 step training time: 0.220990s

on_train_batch_end: 1609194965.707218s

28672/50000 [================>.............] - ETA: 2s - loss: 2.8856 - accuracy: 0.0947
on_train_batch_begin: 1609194965.707618s

14 step training time: 0.216127s

on_train_batch_end: 1609194965.920821s

30720/50000 [=================>............] - ETA: 2s - loss: 2.8504 - accuracy: 0.0949
on_train_batch_begin: 1609194965.921199s

15 step training time: 0.213582s

on_train_batch_end: 1609194966.135463s

32768/50000 [==================>...........] - ETA: 1s - loss: 2.8159 - accuracy: 0.0952
on_train_batch_begin: 1609194966.135840s

16 step training time: 0.214641s

on_train_batch_end: 1609194966.354429s

34816/50000 [===================>..........] - ETA: 1s - loss: 2.7815 - accuracy: 0.0954
on_train_batch_begin: 1609194966.354810s

17 step training time: 0.218970s

on_train_batch_end: 1609194966.572303s

36864/50000 [=====================>........] - ETA: 1s - loss: 2.7515 - accuracy: 0.0956
on_train_batch_begin: 1609194966.572696s

18 step training time: 0.217886s

on_train_batch_end: 1609194966.788915s

38912/50000 [======================>.......] - ETA: 1s - loss: 2.7125 - accuracy: 0.0958
on_train_batch_begin: 1609194966.789334s

19 step training time: 0.216638s

on_train_batch_end: 1609194967.008280s

40960/50000 [=======================>......] - ETA: 0s - loss: 2.6884 - accuracy: 0.0960
on_train_batch_begin: 1609194967.008665s

20 step training time: 0.219332s

on_train_batch_end: 1609194967.226066s

43008/50000 [========================>.....] - ETA: 0s - loss: 2.6541 - accuracy: 0.0962
on_train_batch_begin: 1609194967.226489s

21 step training time: 0.217824s

on_train_batch_end: 1609194967.441112s

45056/50000 [==========================>...] - ETA: 0s - loss: 2.6269 - accuracy: 0.0964
on_train_batch_begin: 1609194967.441517s

22 step training time: 0.215029s

on_train_batch_end: 1609194967.656277s

47104/50000 [===========================>..] - ETA: 0s - loss: 2.5982 - accuracy: 0.0966
on_train_batch_begin: 1609194967.656666s

23 step training time: 0.215148s

on_train_batch_end: 1609194967.864789s

49152/50000 [============================>.] - ETA: 0s - loss: 2.5719 - accuracy: 0.0967
on_train_batch_begin: 1609194967.865175s

24 step training time: 0.208509s

on_train_batch_end: 1609194967.975117s

on_test_batch_begin: 1609194968.005528s

25 step training time: 0.140353s

on_epoch_end: 1609194968.434414s

Validation time: 0.428872s

Real time: 1609194968.434414s

Epoch time: 5.769952774047852s

50000/50000 [==============================] - 6s 115us/sample - loss: 2.5600 - accuracy: 0.0967 - val_loss: 6.9670 - val_accuracy: 0.1001

on_epoch_begin: 1609194968.434659s

Real time: 1609194968.434668
Epoch 5/5

on_train_batch_begin: 1609194968.438936s

on_train_batch_end: 1609194968.656565s

 2048/50000 [>.............................] - ETA: 5s - loss: 2.0522 - accuracy: 0.0989
on_train_batch_begin: 1609194968.656940s

1 step training time: 0.218004s

on_train_batch_end: 1609194968.872160s

 4096/50000 [=>............................] - ETA: 4s - loss: 1.9725 - accuracy: 0.0992
on_train_batch_begin: 1609194968.872650s

2 step training time: 0.215710s

on_train_batch_end: 1609194969.087399s

 6144/50000 [==>...........................] - ETA: 4s - loss: 1.8981 - accuracy: 0.0994
on_train_batch_begin: 1609194969.087786s

3 step training time: 0.215136s

on_train_batch_end: 1609194969.303903s

 8192/50000 [===>..........................] - ETA: 4s - loss: 1.8513 - accuracy: 0.0997
on_train_batch_begin: 1609194969.304279s

4 step training time: 0.216492s

on_train_batch_end: 1609194969.522571s

10240/50000 [=====>........................] - ETA: 4s - loss: 1.8316 - accuracy: 0.0997
on_train_batch_begin: 1609194969.522947s

5 step training time: 0.218668s

on_train_batch_end: 1609194969.736886s

12288/50000 [======>.......................] - ETA: 3s - loss: 1.8024 - accuracy: 0.0998
on_train_batch_begin: 1609194969.737257s

6 step training time: 0.214310s

on_train_batch_end: 1609194969.952743s

14336/50000 [=======>......................] - ETA: 3s - loss: 1.7922 - accuracy: 0.0998
on_train_batch_begin: 1609194969.953117s

7 step training time: 0.215860s

on_train_batch_end: 1609194970.167949s

16384/50000 [========>.....................] - ETA: 3s - loss: 1.7835 - accuracy: 0.0999
on_train_batch_begin: 1609194970.168318s

8 step training time: 0.215201s

on_train_batch_end: 1609194970.382299s

18432/50000 [==========>...................] - ETA: 3s - loss: 1.7699 - accuracy: 0.0999
on_train_batch_begin: 1609194970.382723s

9 step training time: 0.214405s

on_train_batch_end: 1609194970.598617s

20480/50000 [===========>..................] - ETA: 3s - loss: 1.7595 - accuracy: 0.0999
on_train_batch_begin: 1609194970.599001s

10 step training time: 0.216278s

on_train_batch_end: 1609194970.814538s

22528/50000 [============>.................] - ETA: 2s - loss: 1.7412 - accuracy: 0.0999
on_train_batch_begin: 1609194970.814912s

11 step training time: 0.215911s

on_train_batch_end: 1609194971.030304s

24576/50000 [=============>................] - ETA: 2s - loss: 1.7262 - accuracy: 0.0999
on_train_batch_begin: 1609194971.030709s

12 step training time: 0.215797s

on_train_batch_end: 1609194971.247875s

26624/50000 [==============>...............] - ETA: 2s - loss: 1.7155 - accuracy: 0.0999
on_train_batch_begin: 1609194971.248251s

13 step training time: 0.217543s

on_train_batch_end: 1609194971.466652s

28672/50000 [================>.............] - ETA: 2s - loss: 1.7019 - accuracy: 0.0999
on_train_batch_begin: 1609194971.467050s

14 step training time: 0.218799s

on_train_batch_end: 1609194971.681927s

30720/50000 [=================>............] - ETA: 2s - loss: 1.6912 - accuracy: 0.0999
on_train_batch_begin: 1609194971.682316s

15 step training time: 0.215266s

on_train_batch_end: 1609194971.897082s

32768/50000 [==================>...........] - ETA: 1s - loss: 1.6794 - accuracy: 0.0999
on_train_batch_begin: 1609194971.897465s

16 step training time: 0.215149s

on_train_batch_end: 1609194972.111100s

34816/50000 [===================>..........] - ETA: 1s - loss: 1.6692 - accuracy: 0.0999
on_train_batch_begin: 1609194972.111474s

17 step training time: 0.214009s

on_train_batch_end: 1609194972.327753s

36864/50000 [=====================>........] - ETA: 1s - loss: 1.6671 - accuracy: 0.1000
on_train_batch_begin: 1609194972.328136s

18 step training time: 0.216662s

on_train_batch_end: 1609194972.543179s

38912/50000 [======================>.......] - ETA: 1s - loss: 1.6627 - accuracy: 0.1000
on_train_batch_begin: 1609194972.543555s

19 step training time: 0.215419s

on_train_batch_end: 1609194972.757091s

40960/50000 [=======================>......] - ETA: 0s - loss: 1.6568 - accuracy: 0.1000
on_train_batch_begin: 1609194972.757478s

20 step training time: 0.213923s

on_train_batch_end: 1609194972.980322s

43008/50000 [========================>.....] - ETA: 0s - loss: 1.6475 - accuracy: 0.1000
on_train_batch_begin: 1609194972.980710s

21 step training time: 0.223232s

on_train_batch_end: 1609194973.196698s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.6425 - accuracy: 0.1000
on_train_batch_begin: 1609194973.197104s

22 step training time: 0.216394s

on_train_batch_end: 1609194973.416451s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.6307 - accuracy: 0.1000
on_train_batch_begin: 1609194973.416839s

23 step training time: 0.219735s

on_train_batch_end: 1609194973.632100s

49152/50000 [============================>.] - ETA: 0s - loss: 1.6263 - accuracy: 0.1000
on_train_batch_begin: 1609194973.632484s

24 step training time: 0.215646s

on_train_batch_end: 1609194973.737245s

on_test_batch_begin: 1609194973.767829s

25 step training time: 0.135344s

on_epoch_end: 1609194974.195411s

Validation time: 0.427567s

Real time: 1609194974.195411s

Epoch time: 5.760765075683594s

50000/50000 [==============================] - 6s 115us/sample - loss: 1.6215 - accuracy: 0.1000 - val_loss: 7.3521 - val_accuracy: 0.0490
Tempo do fit: 55.44906735420227