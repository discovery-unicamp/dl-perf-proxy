wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:37
   204800/170498071 [..............................] - ETA: 1:17
  1171456/170498071 [..............................] - ETA: 20s 
  3850240/170498071 [..............................] - ETA: 8s 
  7086080/170498071 [>.............................] - ETA: 5s
 10428416/170498071 [>.............................] - ETA: 4s
 13754368/170498071 [=>............................] - ETA: 3s
 17113088/170498071 [==>...........................] - ETA: 3s
 20455424/170498071 [==>...........................] - ETA: 3s
 23601152/170498071 [===>..........................] - ETA: 3s
 26763264/170498071 [===>..........................] - ETA: 2s
 30023680/170498071 [====>.........................] - ETA: 2s
 33357824/170498071 [====>.........................] - ETA: 2s
 36626432/170498071 [=====>........................] - ETA: 2s
 39919616/170498071 [======>.......................] - ETA: 2s
 43229184/170498071 [======>.......................] - ETA: 2s
 46596096/170498071 [=======>......................] - ETA: 2s
 49864704/170498071 [=======>......................] - ETA: 2s
 53190656/170498071 [========>.....................] - ETA: 2s
 56434688/170498071 [========>.....................] - ETA: 2s
 59662336/170498071 [=========>....................] - ETA: 1s
 62889984/170498071 [==========>...................] - ETA: 1s
 66199552/170498071 [==========>...................] - ETA: 1s
 68083712/170498071 [==========>...................] - ETA: 1s
 70803456/170498071 [===========>..................] - ETA: 1s
 74145792/170498071 [============>.................] - ETA: 1s
 77512704/170498071 [============>.................] - ETA: 1s
 80814080/170498071 [=============>................] - ETA: 1s
 84123648/170498071 [=============>................] - ETA: 1s
 87318528/170498071 [==============>...............] - ETA: 1s
 90677248/170498071 [==============>...............] - ETA: 1s
 93954048/170498071 [===============>..............] - ETA: 1s
 97116160/170498071 [================>.............] - ETA: 1s
100261888/170498071 [================>.............] - ETA: 1s
103571456/170498071 [=================>............] - ETA: 1s
106807296/170498071 [=================>............] - ETA: 1s
110125056/170498071 [==================>...........] - ETA: 1s
113451008/170498071 [==================>...........] - ETA: 0s
116629504/170498071 [===================>..........] - ETA: 0s
119955456/170498071 [====================>.........] - ETA: 0s
123265024/170498071 [====================>.........] - ETA: 0s
126590976/170498071 [=====================>........] - ETA: 0s
129916928/170498071 [=====================>........] - ETA: 0s
133177344/170498071 [======================>.......] - ETA: 0s
136388608/170498071 [======================>.......] - ETA: 0s
139591680/170498071 [=======================>......] - ETA: 0s
142860288/170498071 [========================>.....] - ETA: 0s
146169856/170498071 [========================>.....] - ETA: 0s
149504000/170498071 [=========================>....] - ETA: 0s
152690688/170498071 [=========================>....] - ETA: 0s
156000256/170498071 [==========================>...] - ETA: 0s
159326208/170498071 [===========================>..] - ETA: 0s
162652160/170498071 [===========================>..] - ETA: 0s
165912576/170498071 [============================>.] - ETA: 0s
169140224/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 2s
 7356416/94765736 [=>............................] - ETA: 0s
 9388032/94765736 [=>............................] - ETA: 1s
14114816/94765736 [===>..........................] - ETA: 0s
18841600/94765736 [====>.........................] - ETA: 1s
24748032/94765736 [======>.......................] - ETA: 1s
28286976/94765736 [=======>......................] - ETA: 1s
28688384/94765736 [========>.....................] - ETA: 1s
32235520/94765736 [=========>....................] - ETA: 1s
34127872/94765736 [=========>....................] - ETA: 1s
37683200/94765736 [==========>...................] - ETA: 1s
44089344/94765736 [============>.................] - ETA: 1s
47136768/94765736 [=============>................] - ETA: 1s
53051392/94765736 [===============>..............] - ETA: 1s
56598528/94765736 [================>.............] - ETA: 0s
63414272/94765736 [===================>..........] - ETA: 0s
65986560/94765736 [===================>..........] - ETA: 0s
71942144/94765736 [=====================>........] - ETA: 0s
75440128/94765736 [======================>.......] - ETA: 0s
84541440/94765736 [=========================>....] - ETA: 0s
84893696/94765736 [=========================>....] - ETA: 0s
92487680/94765736 [============================>.] - ETA: 0s
94281728/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 2s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 19.353618144989014
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1598511877.997986s

Real time: 1598511877.9980037
Epoch 1/5

on_train_batch_begin: 1598511878.788323s

on_train_batch_end: 1598511928.355312s

 2048/50000 [>.............................] - ETA: 19:39 - loss: 17.5169 - accuracy: 2.1935e-04
on_train_batch_begin: 1598511928.355896s

1 step training time: 49.567574s

on_train_batch_end: 1598511928.576171s

 4096/50000 [=>............................] - ETA: 9:26 - loss: 14.5196 - accuracy: 3.5858e-04 
on_train_batch_begin: 1598511928.576496s

2 step training time: 0.220600s

on_train_batch_end: 1598511928.794634s

 6144/50000 [==>...........................] - ETA: 6:02 - loss: 12.6169 - accuracy: 8.5862e-04
on_train_batch_begin: 1598511928.794952s

3 step training time: 0.218456s

on_train_batch_end: 1598511929.013532s

 8192/50000 [===>..........................] - ETA: 4:20 - loss: 11.4934 - accuracy: 0.0019    
on_train_batch_begin: 1598511929.013834s

4 step training time: 0.218882s

on_train_batch_end: 1598511929.235117s

10240/50000 [=====>........................] - ETA: 3:18 - loss: 10.7867 - accuracy: 0.0048
on_train_batch_begin: 1598511929.235408s

5 step training time: 0.221575s

on_train_batch_end: 1598511929.452276s

12288/50000 [======>.......................] - ETA: 2:37 - loss: 10.3131 - accuracy: 0.0090
on_train_batch_begin: 1598511929.452564s

6 step training time: 0.217155s

on_train_batch_end: 1598511929.668616s

14336/50000 [=======>......................] - ETA: 2:08 - loss: 9.9541 - accuracy: 0.0125 
on_train_batch_begin: 1598511929.668905s

7 step training time: 0.216341s

on_train_batch_end: 1598511929.886974s

16384/50000 [========>.....................] - ETA: 1:46 - loss: 9.6613 - accuracy: 0.0165
on_train_batch_begin: 1598511929.887260s

8 step training time: 0.218355s

on_train_batch_end: 1598511930.106435s

18432/50000 [==========>...................] - ETA: 1:29 - loss: 9.4274 - accuracy: 0.0200
on_train_batch_begin: 1598511930.106723s

9 step training time: 0.219463s

on_train_batch_end: 1598511930.323556s

20480/50000 [===========>..................] - ETA: 1:15 - loss: 9.2272 - accuracy: 0.0234
on_train_batch_begin: 1598511930.323841s

10 step training time: 0.217118s

on_train_batch_end: 1598511930.541022s

22528/50000 [============>.................] - ETA: 1:04 - loss: 9.0584 - accuracy: 0.0273
on_train_batch_begin: 1598511930.541322s

11 step training time: 0.217480s

on_train_batch_end: 1598511930.761650s

24576/50000 [=============>................] - ETA: 54s - loss: 8.9084 - accuracy: 0.0304 
on_train_batch_begin: 1598511930.761937s

12 step training time: 0.220616s

on_train_batch_end: 1598511930.977673s

26624/50000 [==============>...............] - ETA: 46s - loss: 8.7723 - accuracy: 0.0329
on_train_batch_begin: 1598511930.977977s

13 step training time: 0.216040s

on_train_batch_end: 1598511931.194244s

28672/50000 [================>.............] - ETA: 39s - loss: 8.6572 - accuracy: 0.0347
on_train_batch_begin: 1598511931.194533s

14 step training time: 0.216556s

on_train_batch_end: 1598511931.414171s

30720/50000 [=================>............] - ETA: 33s - loss: 8.5485 - accuracy: 0.0367
on_train_batch_begin: 1598511931.414456s

15 step training time: 0.219923s

on_train_batch_end: 1598511931.634363s

32768/50000 [==================>...........] - ETA: 28s - loss: 8.4466 - accuracy: 0.0368
on_train_batch_begin: 1598511931.634664s

16 step training time: 0.220208s

on_train_batch_end: 1598511931.851116s

34816/50000 [===================>..........] - ETA: 23s - loss: 8.3580 - accuracy: 0.0370
on_train_batch_begin: 1598511931.851411s

17 step training time: 0.216746s

on_train_batch_end: 1598511932.068360s

36864/50000 [=====================>........] - ETA: 19s - loss: 8.2801 - accuracy: 0.0364
on_train_batch_begin: 1598511932.068645s

18 step training time: 0.217234s

on_train_batch_end: 1598511932.285661s

38912/50000 [======================>.......] - ETA: 15s - loss: 8.1990 - accuracy: 0.0376
on_train_batch_begin: 1598511932.285946s

19 step training time: 0.217301s

on_train_batch_end: 1598511932.502017s

40960/50000 [=======================>......] - ETA: 12s - loss: 8.1172 - accuracy: 0.0392
on_train_batch_begin: 1598511932.502305s

20 step training time: 0.216359s

on_train_batch_end: 1598511932.722837s

43008/50000 [========================>.....] - ETA: 8s - loss: 8.0408 - accuracy: 0.0407 
on_train_batch_begin: 1598511932.723136s

21 step training time: 0.220832s

on_train_batch_end: 1598511932.940273s

45056/50000 [==========================>...] - ETA: 6s - loss: 7.9721 - accuracy: 0.0420
on_train_batch_begin: 1598511932.940585s

22 step training time: 0.217449s

on_train_batch_end: 1598511933.160471s

47104/50000 [===========================>..] - ETA: 3s - loss: 7.9041 - accuracy: 0.0433
on_train_batch_begin: 1598511933.160741s

23 step training time: 0.220156s

on_train_batch_end: 1598511933.375537s

49152/50000 [============================>.] - ETA: 0s - loss: 7.8375 - accuracy: 0.0441
on_train_batch_begin: 1598511933.375823s

24 step training time: 0.215082s

on_train_batch_end: 1598511936.173318s

on_test_batch_begin: 1598511936.407845s

25 step training time: 3.032022s

on_epoch_end: 1598511941.897872s

Validation time: 5.490012s

Real time: 1598511941.897872s

Epoch time: 63.8998863697052s

50000/50000 [==============================] - 64s 1ms/sample - loss: 7.8118 - accuracy: 0.0443 - val_loss: 183250.4010 - val_accuracy: 0.0000e+00

on_epoch_begin: 1598511941.898065s

Real time: 1598511941.8980863
Epoch 2/5

on_train_batch_begin: 1598511941.902622s

on_train_batch_end: 1598511942.124434s

 2048/50000 [>.............................] - ETA: 5s - loss: 6.2498 - accuracy: 0.0591
on_train_batch_begin: 1598511942.124724s

1 step training time: 0.222102s

on_train_batch_end: 1598511942.346515s

 4096/50000 [=>............................] - ETA: 5s - loss: 6.1566 - accuracy: 0.0584
on_train_batch_begin: 1598511942.346803s

2 step training time: 0.222079s

on_train_batch_end: 1598511942.568585s

 6144/50000 [==>...........................] - ETA: 4s - loss: 6.0872 - accuracy: 0.0589
on_train_batch_begin: 1598511942.568869s

3 step training time: 0.222065s

on_train_batch_end: 1598511942.787397s

 8192/50000 [===>..........................] - ETA: 4s - loss: 6.0726 - accuracy: 0.0586
on_train_batch_begin: 1598511942.787681s

4 step training time: 0.218812s

on_train_batch_end: 1598511943.010390s

10240/50000 [=====>........................] - ETA: 4s - loss: 6.0384 - accuracy: 0.0597
on_train_batch_begin: 1598511943.010661s

5 step training time: 0.222980s

on_train_batch_end: 1598511943.230809s

12288/50000 [======>.......................] - ETA: 4s - loss: 5.9958 - accuracy: 0.0604
on_train_batch_begin: 1598511943.231118s

6 step training time: 0.220456s

on_train_batch_end: 1598511943.449954s

14336/50000 [=======>......................] - ETA: 3s - loss: 5.9496 - accuracy: 0.0617
on_train_batch_begin: 1598511943.450243s

7 step training time: 0.219126s

on_train_batch_end: 1598511943.668422s

16384/50000 [========>.....................] - ETA: 3s - loss: 5.9137 - accuracy: 0.0622
on_train_batch_begin: 1598511943.668708s

8 step training time: 0.218464s

on_train_batch_end: 1598511943.886593s

18432/50000 [==========>...................] - ETA: 3s - loss: 5.8740 - accuracy: 0.0629
on_train_batch_begin: 1598511943.886879s

9 step training time: 0.218172s

on_train_batch_end: 1598511944.107205s

20480/50000 [===========>..................] - ETA: 3s - loss: 5.8447 - accuracy: 0.0627
on_train_batch_begin: 1598511944.107494s

10 step training time: 0.220615s

on_train_batch_end: 1598511944.325841s

22528/50000 [============>.................] - ETA: 2s - loss: 5.8088 - accuracy: 0.0624
on_train_batch_begin: 1598511944.326130s

11 step training time: 0.218636s

on_train_batch_end: 1598511944.544468s

24576/50000 [=============>................] - ETA: 2s - loss: 5.7770 - accuracy: 0.0624
on_train_batch_begin: 1598511944.544758s

12 step training time: 0.218628s

on_train_batch_end: 1598511944.769230s

26624/50000 [==============>...............] - ETA: 2s - loss: 5.7462 - accuracy: 0.0623
on_train_batch_begin: 1598511944.769527s

13 step training time: 0.224769s

on_train_batch_end: 1598511944.989182s

28672/50000 [================>.............] - ETA: 2s - loss: 5.7079 - accuracy: 0.0624
on_train_batch_begin: 1598511944.989475s

14 step training time: 0.219948s

on_train_batch_end: 1598511945.209228s

30720/50000 [=================>............] - ETA: 2s - loss: 5.6731 - accuracy: 0.0622
on_train_batch_begin: 1598511945.209519s

15 step training time: 0.220044s

on_train_batch_end: 1598511945.431790s

32768/50000 [==================>...........] - ETA: 1s - loss: 5.6338 - accuracy: 0.0619
on_train_batch_begin: 1598511945.432080s

16 step training time: 0.222561s

on_train_batch_end: 1598511945.651166s

34816/50000 [===================>..........] - ETA: 1s - loss: 5.5976 - accuracy: 0.0617
on_train_batch_begin: 1598511945.651467s

17 step training time: 0.219387s

on_train_batch_end: 1598511945.871036s

36864/50000 [=====================>........] - ETA: 1s - loss: 5.5584 - accuracy: 0.0615
on_train_batch_begin: 1598511945.871326s

18 step training time: 0.219859s

on_train_batch_end: 1598511946.095350s

38912/50000 [======================>.......] - ETA: 1s - loss: 5.5179 - accuracy: 0.0613
on_train_batch_begin: 1598511946.095640s

19 step training time: 0.224315s

on_train_batch_end: 1598511946.315351s

40960/50000 [=======================>......] - ETA: 0s - loss: 5.4810 - accuracy: 0.0612
on_train_batch_begin: 1598511946.315638s

20 step training time: 0.219997s

on_train_batch_end: 1598511946.536168s

43008/50000 [========================>.....] - ETA: 0s - loss: 5.4459 - accuracy: 0.0610
on_train_batch_begin: 1598511946.536453s

21 step training time: 0.220815s

on_train_batch_end: 1598511946.755653s

45056/50000 [==========================>...] - ETA: 0s - loss: 5.4074 - accuracy: 0.0610
on_train_batch_begin: 1598511946.755941s

22 step training time: 0.219488s

on_train_batch_end: 1598511946.973841s

47104/50000 [===========================>..] - ETA: 0s - loss: 5.3679 - accuracy: 0.0609
on_train_batch_begin: 1598511946.974128s

23 step training time: 0.218188s

on_train_batch_end: 1598511947.193467s

49152/50000 [============================>.] - ETA: 0s - loss: 5.3318 - accuracy: 0.0610
on_train_batch_begin: 1598511947.193755s

24 step training time: 0.219626s

on_train_batch_end: 1598511947.323938s

on_test_batch_begin: 1598511947.416234s

25 step training time: 0.222480s

on_epoch_end: 1598511947.746284s

Validation time: 0.330034s

Real time: 1598511947.746284s

Epoch time: 5.8482139110565186s

50000/50000 [==============================] - 6s 117us/sample - loss: 5.3133 - accuracy: 0.0611 - val_loss: 8.4352 - val_accuracy: 0.0000e+00

on_epoch_begin: 1598511947.746466s

Real time: 1598511947.7464707
Epoch 3/5

on_train_batch_begin: 1598511947.750709s

on_train_batch_end: 1598511947.972718s

 2048/50000 [>.............................] - ETA: 5s - loss: 4.1873 - accuracy: 0.0654
on_train_batch_begin: 1598511947.973028s

1 step training time: 0.222320s

on_train_batch_end: 1598511948.193160s

 4096/50000 [=>............................] - ETA: 5s - loss: 4.1359 - accuracy: 0.0670
on_train_batch_begin: 1598511948.193444s

2 step training time: 0.220416s

on_train_batch_end: 1598511948.412075s

 6144/50000 [==>...........................] - ETA: 4s - loss: 4.0789 - accuracy: 0.0679
on_train_batch_begin: 1598511948.412364s

3 step training time: 0.218920s

on_train_batch_end: 1598511948.635565s

 8192/50000 [===>..........................] - ETA: 4s - loss: 4.0289 - accuracy: 0.0687
on_train_batch_begin: 1598511948.635851s

4 step training time: 0.223487s

on_train_batch_end: 1598511948.855435s

10240/50000 [=====>........................] - ETA: 4s - loss: 4.0042 - accuracy: 0.0696
on_train_batch_begin: 1598511948.855723s

5 step training time: 0.219871s

on_train_batch_end: 1598511949.077350s

12288/50000 [======>.......................] - ETA: 4s - loss: 3.9607 - accuracy: 0.0699
on_train_batch_begin: 1598511949.077639s

6 step training time: 0.221916s

on_train_batch_end: 1598511949.300149s

14336/50000 [=======>......................] - ETA: 3s - loss: 3.9094 - accuracy: 0.0709
on_train_batch_begin: 1598511949.300453s

7 step training time: 0.222814s

on_train_batch_end: 1598511949.527250s

16384/50000 [========>.....................] - ETA: 3s - loss: 3.8496 - accuracy: 0.0722
on_train_batch_begin: 1598511949.527535s

8 step training time: 0.227083s

on_train_batch_end: 1598511949.747943s

18432/50000 [==========>...................] - ETA: 3s - loss: 3.8173 - accuracy: 0.0728
on_train_batch_begin: 1598511949.748225s

9 step training time: 0.220690s

on_train_batch_end: 1598511949.971652s

20480/50000 [===========>..................] - ETA: 3s - loss: 3.7941 - accuracy: 0.0734
on_train_batch_begin: 1598511949.971954s

10 step training time: 0.223728s

on_train_batch_end: 1598511950.197114s

22528/50000 [============>.................] - ETA: 2s - loss: 3.7586 - accuracy: 0.0742
on_train_batch_begin: 1598511950.197417s

11 step training time: 0.225464s

on_train_batch_end: 1598511950.416659s

24576/50000 [=============>................] - ETA: 2s - loss: 3.7055 - accuracy: 0.0752
on_train_batch_begin: 1598511950.416980s

12 step training time: 0.219562s

on_train_batch_end: 1598511950.642792s

26624/50000 [==============>...............] - ETA: 2s - loss: 3.6500 - accuracy: 0.0761
on_train_batch_begin: 1598511950.643085s

13 step training time: 0.226105s

on_train_batch_end: 1598511950.863279s

28672/50000 [================>.............] - ETA: 2s - loss: 3.6087 - accuracy: 0.0770
on_train_batch_begin: 1598511950.863588s

14 step training time: 0.220503s

on_train_batch_end: 1598511951.086247s

30720/50000 [=================>............] - ETA: 2s - loss: 3.5560 - accuracy: 0.0779
on_train_batch_begin: 1598511951.086554s

15 step training time: 0.222966s

on_train_batch_end: 1598511951.308213s

32768/50000 [==================>...........] - ETA: 1s - loss: 3.5122 - accuracy: 0.0786
on_train_batch_begin: 1598511951.308520s

16 step training time: 0.221966s

on_train_batch_end: 1598511951.532780s

34816/50000 [===================>..........] - ETA: 1s - loss: 3.4671 - accuracy: 0.0795
on_train_batch_begin: 1598511951.533107s

17 step training time: 0.224587s

on_train_batch_end: 1598511951.755890s

36864/50000 [=====================>........] - ETA: 1s - loss: 3.4390 - accuracy: 0.0803
on_train_batch_begin: 1598511951.756177s

18 step training time: 0.223069s

on_train_batch_end: 1598511951.974255s

38912/50000 [======================>.......] - ETA: 1s - loss: 3.4022 - accuracy: 0.0811
on_train_batch_begin: 1598511951.974545s

19 step training time: 0.218368s

on_train_batch_end: 1598511952.197708s

40960/50000 [=======================>......] - ETA: 0s - loss: 3.3611 - accuracy: 0.0819
on_train_batch_begin: 1598511952.197995s

20 step training time: 0.223450s

on_train_batch_end: 1598511952.419551s

43008/50000 [========================>.....] - ETA: 0s - loss: 3.3322 - accuracy: 0.0826
on_train_batch_begin: 1598511952.419845s

21 step training time: 0.221850s

on_train_batch_end: 1598511952.641514s

45056/50000 [==========================>...] - ETA: 0s - loss: 3.2970 - accuracy: 0.0832
on_train_batch_begin: 1598511952.641814s

22 step training time: 0.221969s

on_train_batch_end: 1598511952.862309s

47104/50000 [===========================>..] - ETA: 0s - loss: 3.2698 - accuracy: 0.0839
on_train_batch_begin: 1598511952.862596s

23 step training time: 0.220782s

on_train_batch_end: 1598511953.081484s

49152/50000 [============================>.] - ETA: 0s - loss: 3.2340 - accuracy: 0.0845
on_train_batch_begin: 1598511953.081784s

24 step training time: 0.219188s

on_train_batch_end: 1598511953.211776s

on_test_batch_begin: 1598511953.304384s

25 step training time: 0.222600s

on_epoch_end: 1598511953.625480s

Validation time: 0.321081s

Real time: 1598511953.625480s

Epoch time: 5.87902569770813s

50000/50000 [==============================] - 6s 118us/sample - loss: 3.2195 - accuracy: 0.0846 - val_loss: 8.1892 - val_accuracy: 0.0000e+00

on_epoch_begin: 1598511953.625661s

Real time: 1598511953.625666
Epoch 4/5

on_train_batch_begin: 1598511953.630038s

on_train_batch_end: 1598511953.850494s

 2048/50000 [>.............................] - ETA: 5s - loss: 2.4567 - accuracy: 0.0992
on_train_batch_begin: 1598511953.850765s

1 step training time: 0.220727s

on_train_batch_end: 1598511954.072712s

 4096/50000 [=>............................] - ETA: 5s - loss: 2.3844 - accuracy: 0.0996
on_train_batch_begin: 1598511954.073036s

2 step training time: 0.222270s

on_train_batch_end: 1598511954.298660s

 6144/50000 [==>...........................] - ETA: 4s - loss: 2.3085 - accuracy: 0.0997
on_train_batch_begin: 1598511954.298946s

3 step training time: 0.225910s

on_train_batch_end: 1598511954.519797s

 8192/50000 [===>..........................] - ETA: 4s - loss: 2.2675 - accuracy: 0.0998
on_train_batch_begin: 1598511954.520090s

4 step training time: 0.221144s

on_train_batch_end: 1598511954.741657s

10240/50000 [=====>........................] - ETA: 4s - loss: 2.2304 - accuracy: 0.0999
on_train_batch_begin: 1598511954.741944s

5 step training time: 0.221854s

on_train_batch_end: 1598511954.964407s

12288/50000 [======>.......................] - ETA: 4s - loss: 2.1938 - accuracy: 0.0998
on_train_batch_begin: 1598511954.964687s

6 step training time: 0.222742s

on_train_batch_end: 1598511955.190775s

14336/50000 [=======>......................] - ETA: 3s - loss: 2.1542 - accuracy: 0.0999
on_train_batch_begin: 1598511955.191072s

7 step training time: 0.226386s

on_train_batch_end: 1598511955.417678s

16384/50000 [========>.....................] - ETA: 3s - loss: 2.1265 - accuracy: 0.0998
on_train_batch_begin: 1598511955.417965s

8 step training time: 0.226893s

on_train_batch_end: 1598511955.641005s

18432/50000 [==========>...................] - ETA: 3s - loss: 2.0970 - accuracy: 0.1000
on_train_batch_begin: 1598511955.641279s

9 step training time: 0.223313s

on_train_batch_end: 1598511955.862514s

20480/50000 [===========>..................] - ETA: 3s - loss: 2.0690 - accuracy: 0.1001
on_train_batch_begin: 1598511955.862797s

10 step training time: 0.221519s

on_train_batch_end: 1598511956.085080s

22528/50000 [============>.................] - ETA: 2s - loss: 2.0425 - accuracy: 0.1001
on_train_batch_begin: 1598511956.085368s

11 step training time: 0.222571s

on_train_batch_end: 1598511956.307092s

24576/50000 [=============>................] - ETA: 2s - loss: 2.0215 - accuracy: 0.1001
on_train_batch_begin: 1598511956.307380s

12 step training time: 0.222012s

on_train_batch_end: 1598511956.532158s

26624/50000 [==============>...............] - ETA: 2s - loss: 2.0037 - accuracy: 0.1001
on_train_batch_begin: 1598511956.532447s

13 step training time: 0.225066s

on_train_batch_end: 1598511956.759694s

28672/50000 [================>.............] - ETA: 2s - loss: 1.9879 - accuracy: 0.1001
on_train_batch_begin: 1598511956.759987s

14 step training time: 0.227540s

on_train_batch_end: 1598511956.982034s

30720/50000 [=================>............] - ETA: 2s - loss: 1.9707 - accuracy: 0.1001
on_train_batch_begin: 1598511956.982324s

15 step training time: 0.222337s

on_train_batch_end: 1598511957.206214s

32768/50000 [==================>...........] - ETA: 1s - loss: 1.9606 - accuracy: 0.1002
on_train_batch_begin: 1598511957.206490s

16 step training time: 0.224166s

on_train_batch_end: 1598511957.431795s

34816/50000 [===================>..........] - ETA: 1s - loss: 1.9433 - accuracy: 0.1002
on_train_batch_begin: 1598511957.432078s

17 step training time: 0.225588s

on_train_batch_end: 1598511957.653465s

36864/50000 [=====================>........] - ETA: 1s - loss: 1.9247 - accuracy: 0.1002
on_train_batch_begin: 1598511957.653760s

18 step training time: 0.221682s

on_train_batch_end: 1598511957.878782s

38912/50000 [======================>.......] - ETA: 1s - loss: 1.9091 - accuracy: 0.1002
on_train_batch_begin: 1598511957.879090s

19 step training time: 0.225330s

on_train_batch_end: 1598511958.099750s

40960/50000 [=======================>......] - ETA: 0s - loss: 1.8943 - accuracy: 0.1002
on_train_batch_begin: 1598511958.100031s

20 step training time: 0.220941s

on_train_batch_end: 1598511958.322099s

43008/50000 [========================>.....] - ETA: 0s - loss: 1.8753 - accuracy: 0.1002
on_train_batch_begin: 1598511958.322383s

21 step training time: 0.222352s

on_train_batch_end: 1598511958.545967s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.8556 - accuracy: 0.1002
on_train_batch_begin: 1598511958.546254s

22 step training time: 0.223871s

on_train_batch_end: 1598511958.767233s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.8394 - accuracy: 0.1002
on_train_batch_begin: 1598511958.767516s

23 step training time: 0.221262s

on_train_batch_end: 1598511958.988744s

49152/50000 [============================>.] - ETA: 0s - loss: 1.8234 - accuracy: 0.1003
on_train_batch_begin: 1598511958.989057s

24 step training time: 0.221540s

on_train_batch_end: 1598511959.117815s

on_test_batch_begin: 1598511959.208513s

25 step training time: 0.219457s

on_epoch_end: 1598511959.547911s

Validation time: 0.339384s

Real time: 1598511959.547911s

Epoch time: 5.922261476516724s

50000/50000 [==============================] - 6s 118us/sample - loss: 1.8190 - accuracy: 0.1003 - val_loss: 7.5727 - val_accuracy: 0.0000e+00

on_epoch_begin: 1598511959.548086s

Real time: 1598511959.5480907
Epoch 5/5

on_train_batch_begin: 1598511959.552537s

on_train_batch_end: 1598511959.774455s

 2048/50000 [>.............................] - ETA: 5s - loss: 1.2717 - accuracy: 0.1003
on_train_batch_begin: 1598511959.774736s

1 step training time: 0.222199s

on_train_batch_end: 1598511960.000578s

 4096/50000 [=>............................] - ETA: 5s - loss: 1.3047 - accuracy: 0.1004
on_train_batch_begin: 1598511960.000860s

2 step training time: 0.226124s

on_train_batch_end: 1598511960.223833s

 6144/50000 [==>...........................] - ETA: 4s - loss: 1.3539 - accuracy: 0.1003
on_train_batch_begin: 1598511960.224115s

3 step training time: 0.223255s

on_train_batch_end: 1598511960.444818s

 8192/50000 [===>..........................] - ETA: 4s - loss: 1.3341 - accuracy: 0.1003
on_train_batch_begin: 1598511960.445122s

4 step training time: 0.221007s

on_train_batch_end: 1598511960.672400s

10240/50000 [=====>........................] - ETA: 4s - loss: 1.3237 - accuracy: 0.1003
on_train_batch_begin: 1598511960.672682s

5 step training time: 0.227560s

on_train_batch_end: 1598511960.896634s

12288/50000 [======>.......................] - ETA: 4s - loss: 1.3197 - accuracy: 0.1004
on_train_batch_begin: 1598511960.896919s

6 step training time: 0.224237s

on_train_batch_end: 1598511961.119456s

14336/50000 [=======>......................] - ETA: 3s - loss: 1.3163 - accuracy: 0.1004
on_train_batch_begin: 1598511961.119749s

7 step training time: 0.222830s

on_train_batch_end: 1598511961.344335s

16384/50000 [========>.....................] - ETA: 3s - loss: 1.3029 - accuracy: 0.1005
on_train_batch_begin: 1598511961.344618s

8 step training time: 0.224869s

on_train_batch_end: 1598511961.566619s

18432/50000 [==========>...................] - ETA: 3s - loss: 1.2983 - accuracy: 0.1005
on_train_batch_begin: 1598511961.566914s

9 step training time: 0.222296s

on_train_batch_end: 1598511961.788154s

20480/50000 [===========>..................] - ETA: 3s - loss: 1.2930 - accuracy: 0.1005
on_train_batch_begin: 1598511961.788443s

10 step training time: 0.221529s

on_train_batch_end: 1598511962.012724s

22528/50000 [============>.................] - ETA: 3s - loss: 1.2836 - accuracy: 0.1005
on_train_batch_begin: 1598511962.013035s

11 step training time: 0.224592s

on_train_batch_end: 1598511962.239336s

24576/50000 [=============>................] - ETA: 2s - loss: 1.2723 - accuracy: 0.1005
on_train_batch_begin: 1598511962.239623s

12 step training time: 0.226589s

on_train_batch_end: 1598511962.458797s

26624/50000 [==============>...............] - ETA: 2s - loss: 1.2756 - accuracy: 0.1005
on_train_batch_begin: 1598511962.459102s

13 step training time: 0.219479s

on_train_batch_end: 1598511962.683531s

28672/50000 [================>.............] - ETA: 2s - loss: 1.2712 - accuracy: 0.1005
on_train_batch_begin: 1598511962.683811s

14 step training time: 0.224709s

on_train_batch_end: 1598511962.909510s

30720/50000 [=================>............] - ETA: 2s - loss: 1.2683 - accuracy: 0.1005
on_train_batch_begin: 1598511962.909801s

15 step training time: 0.225990s

on_train_batch_end: 1598511963.130737s

32768/50000 [==================>...........] - ETA: 1s - loss: 1.2701 - accuracy: 0.1005
on_train_batch_begin: 1598511963.131038s

16 step training time: 0.221236s

on_train_batch_end: 1598511963.357020s

34816/50000 [===================>..........] - ETA: 1s - loss: 1.2627 - accuracy: 0.1005
on_train_batch_begin: 1598511963.357306s

17 step training time: 0.226268s

on_train_batch_end: 1598511963.579588s

36864/50000 [=====================>........] - ETA: 1s - loss: 1.2636 - accuracy: 0.1005
on_train_batch_begin: 1598511963.579881s

18 step training time: 0.222576s

on_train_batch_end: 1598511963.803075s

38912/50000 [======================>.......] - ETA: 1s - loss: 1.2568 - accuracy: 0.1006
on_train_batch_begin: 1598511963.803360s

19 step training time: 0.223479s

on_train_batch_end: 1598511964.025002s

40960/50000 [=======================>......] - ETA: 0s - loss: 1.2482 - accuracy: 0.1006
on_train_batch_begin: 1598511964.025288s

20 step training time: 0.221928s

on_train_batch_end: 1598511964.248824s

43008/50000 [========================>.....] - ETA: 0s - loss: 1.2417 - accuracy: 0.1006
on_train_batch_begin: 1598511964.249175s

21 step training time: 0.223887s

on_train_batch_end: 1598511964.474773s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.2372 - accuracy: 0.1006
on_train_batch_begin: 1598511964.475061s

22 step training time: 0.225887s

on_train_batch_end: 1598511964.699443s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.2353 - accuracy: 0.1006
on_train_batch_begin: 1598511964.699733s

23 step training time: 0.224671s

on_train_batch_end: 1598511964.921701s

49152/50000 [============================>.] - ETA: 0s - loss: 1.2325 - accuracy: 0.1006
on_train_batch_begin: 1598511964.921988s

24 step training time: 0.222255s

on_train_batch_end: 1598511965.050800s

on_test_batch_begin: 1598511965.141772s

25 step training time: 0.219785s

on_epoch_end: 1598511965.473053s

Validation time: 0.331267s

Real time: 1598511965.473053s

Epoch time: 5.924978256225586s

50000/50000 [==============================] - 6s 118us/sample - loss: 1.2309 - accuracy: 0.1006 - val_loss: 7.2425 - val_accuracy: 8.3807e-06
Tempo do fit: 90.96067762374878