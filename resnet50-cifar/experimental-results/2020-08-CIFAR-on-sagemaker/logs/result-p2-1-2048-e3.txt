wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:44
   204800/170498071 [..............................] - ETA: 1:00
   811008/170498071 [..............................] - ETA: 29s 
  3375104/170498071 [..............................] - ETA: 9s 
  6643712/170498071 [>.............................] - ETA: 5s
  9822208/170498071 [>.............................] - ETA: 4s
 13115392/170498071 [=>............................] - ETA: 4s
 16375808/170498071 [=>............................] - ETA: 3s
 19685376/170498071 [==>...........................] - ETA: 3s
 22921216/170498071 [===>..........................] - ETA: 3s
 26157056/170498071 [===>..........................] - ETA: 2s
 29188096/170498071 [====>.........................] - ETA: 2s
 32415744/170498071 [====>.........................] - ETA: 2s
 35545088/170498071 [=====>........................] - ETA: 2s
 38854656/170498071 [=====>........................] - ETA: 2s
 41984000/170498071 [======>.......................] - ETA: 2s
 45162496/170498071 [======>.......................] - ETA: 2s
 48160768/170498071 [=======>......................] - ETA: 2s
 51396608/170498071 [========>.....................] - ETA: 2s
 54689792/170498071 [========>.....................] - ETA: 2s
 57991168/170498071 [=========>....................] - ETA: 2s
 61161472/170498071 [=========>....................] - ETA: 1s
 64454656/170498071 [==========>...................] - ETA: 1s
 67723264/170498071 [==========>...................] - ETA: 1s
 70983680/170498071 [===========>..................] - ETA: 1s
 74129408/170498071 [============>.................] - ETA: 1s
 77324288/170498071 [============>.................] - ETA: 1s
 80502784/170498071 [=============>................] - ETA: 1s
 83632128/170498071 [=============>................] - ETA: 1s
 86876160/170498071 [==============>...............] - ETA: 1s
 90169344/170498071 [==============>...............] - ETA: 1s
 93446144/170498071 [===============>..............] - ETA: 1s
 96747520/170498071 [================>.............] - ETA: 1s
 99950592/170498071 [================>.............] - ETA: 1s
103145472/170498071 [=================>............] - ETA: 1s
106405888/170498071 [=================>............] - ETA: 1s
109584384/170498071 [==================>...........] - ETA: 1s
112779264/170498071 [==================>...........] - ETA: 0s
116031488/170498071 [===================>..........] - ETA: 0s
119152640/170498071 [===================>..........] - ETA: 0s
122347520/170498071 [====================>.........] - ETA: 0s
125616128/170498071 [=====================>........] - ETA: 0s
128851968/170498071 [=====================>........] - ETA: 0s
132046848/170498071 [======================>.......] - ETA: 0s
135192576/170498071 [======================>.......] - ETA: 0s
138354688/170498071 [=======================>......] - ETA: 0s
141549568/170498071 [=======================>......] - ETA: 0s
144793600/170498071 [========================>.....] - ETA: 0s
148119552/170498071 [=========================>....] - ETA: 0s
151347200/170498071 [=========================>....] - ETA: 0s
154492928/170498071 [==========================>...] - ETA: 0s
157687808/170498071 [==========================>...] - ETA: 0s
160956416/170498071 [===========================>..] - ETA: 0s
164257792/170498071 [===========================>..] - ETA: 0s
167501824/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 4s
 4161536/94765736 [>.............................] - ETA: 1s
 9388032/94765736 [=>............................] - ETA: 1s
12926976/94765736 [===>..........................] - ETA: 1s
15040512/94765736 [===>..........................] - ETA: 1s
17104896/94765736 [====>.........................] - ETA: 1s
18841600/94765736 [====>.........................] - ETA: 1s
25927680/94765736 [=======>......................] - ETA: 1s
28286976/94765736 [=======>......................] - ETA: 1s
35201024/94765736 [==========>...................] - ETA: 1s
37912576/94765736 [===========>..................] - ETA: 1s
44105728/94765736 [============>.................] - ETA: 0s
48324608/94765736 [==============>...............] - ETA: 0s
54075392/94765736 [================>.............] - ETA: 0s
59252736/94765736 [=================>............] - ETA: 0s
65986560/94765736 [===================>..........] - ETA: 0s
69771264/94765736 [=====================>........] - ETA: 0s
75440128/94765736 [======================>.......] - ETA: 0s
80846848/94765736 [========================>.....] - ETA: 0s
84893696/94765736 [=========================>....] - ETA: 0s
91004928/94765736 [===========================>..] - ETA: 0s
93691904/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 2s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 15.899151086807251
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1607705213.489201s

Real time: 1607705213.4892244
Epoch 1/5

on_train_batch_begin: 1607705214.350668s

on_train_batch_end: 1607705242.144603s

 2048/50000 [>.............................] - ETA: 11:10 - loss: 17.9068 - accuracy: 4.3654e-04
on_train_batch_begin: 1607705242.145314s

1 step training time: 27.794645s

on_train_batch_end: 1607705243.486232s

 4096/50000 [=>............................] - ETA: 5:36 - loss: 14.7407 - accuracy: 3.8254e-04 
on_train_batch_begin: 1607705243.486639s

2 step training time: 1.341326s

on_train_batch_end: 1607705244.828226s

 6144/50000 [==>...........................] - ETA: 3:43 - loss: 12.7722 - accuracy: 7.6636e-04
on_train_batch_begin: 1607705244.828622s

3 step training time: 1.341982s

on_train_batch_end: 1607705246.157294s

 8192/50000 [===>..........................] - ETA: 2:46 - loss: 11.6176 - accuracy: 0.0020    
on_train_batch_begin: 1607705246.157725s

4 step training time: 1.329103s

on_train_batch_end: 1607705247.491429s

10240/50000 [=====>........................] - ETA: 2:12 - loss: 10.8763 - accuracy: 0.0053
on_train_batch_begin: 1607705247.491853s

5 step training time: 1.334128s

on_train_batch_end: 1607705248.837053s

12288/50000 [======>.......................] - ETA: 1:48 - loss: 10.3533 - accuracy: 0.0092
on_train_batch_begin: 1607705248.837510s

6 step training time: 1.345658s

on_train_batch_end: 1607705250.181656s

14336/50000 [=======>......................] - ETA: 1:31 - loss: 9.9528 - accuracy: 0.0137 
on_train_batch_begin: 1607705250.182062s

7 step training time: 1.344551s

on_train_batch_end: 1607705251.515781s

16384/50000 [========>.....................] - ETA: 1:18 - loss: 9.6473 - accuracy: 0.0178
on_train_batch_begin: 1607705251.516165s

8 step training time: 1.334104s

on_train_batch_end: 1607705252.863575s

18432/50000 [==========>...................] - ETA: 1:07 - loss: 9.3899 - accuracy: 0.0218
on_train_batch_begin: 1607705252.863963s

9 step training time: 1.347797s

on_train_batch_end: 1607705254.195277s

20480/50000 [===========>..................] - ETA: 58s - loss: 9.1786 - accuracy: 0.0254 
on_train_batch_begin: 1607705254.195667s

10 step training time: 1.331704s

on_train_batch_end: 1607705255.538779s

22528/50000 [============>.................] - ETA: 51s - loss: 8.9905 - accuracy: 0.0287
on_train_batch_begin: 1607705255.539165s

11 step training time: 1.343498s

on_train_batch_end: 1607705256.874065s

24576/50000 [=============>................] - ETA: 44s - loss: 8.8293 - accuracy: 0.0316
on_train_batch_begin: 1607705256.874503s

12 step training time: 1.335338s

on_train_batch_end: 1607705258.217906s

26624/50000 [==============>...............] - ETA: 39s - loss: 8.6749 - accuracy: 0.0345
on_train_batch_begin: 1607705258.218296s

13 step training time: 1.343792s

on_train_batch_end: 1607705259.554247s

28672/50000 [================>.............] - ETA: 34s - loss: 8.5367 - accuracy: 0.0370
on_train_batch_begin: 1607705259.554656s

14 step training time: 1.336360s

on_train_batch_end: 1607705260.888649s

30720/50000 [=================>............] - ETA: 29s - loss: 8.4211 - accuracy: 0.0394
on_train_batch_begin: 1607705260.889041s

15 step training time: 1.334385s

on_train_batch_end: 1607705262.235552s

32768/50000 [==================>...........] - ETA: 25s - loss: 8.3136 - accuracy: 0.0414
on_train_batch_begin: 1607705262.235998s

16 step training time: 1.346957s

on_train_batch_end: 1607705263.577003s

34816/50000 [===================>..........] - ETA: 21s - loss: 8.2111 - accuracy: 0.0438
on_train_batch_begin: 1607705263.577443s

17 step training time: 1.341445s

on_train_batch_end: 1607705264.925071s

36864/50000 [=====================>........] - ETA: 18s - loss: 8.1126 - accuracy: 0.0457
on_train_batch_begin: 1607705264.925504s

18 step training time: 1.348061s

on_train_batch_end: 1607705266.268558s

38912/50000 [======================>.......] - ETA: 15s - loss: 8.0201 - accuracy: 0.0471
on_train_batch_begin: 1607705266.268962s

19 step training time: 1.343458s

on_train_batch_end: 1607705267.615772s

40960/50000 [=======================>......] - ETA: 11s - loss: 7.9314 - accuracy: 0.0487
on_train_batch_begin: 1607705267.616176s

20 step training time: 1.347214s

on_train_batch_end: 1607705268.961104s

43008/50000 [========================>.....] - ETA: 9s - loss: 7.8455 - accuracy: 0.0502 
on_train_batch_begin: 1607705268.961550s

21 step training time: 1.345373s

on_train_batch_end: 1607705270.295557s

45056/50000 [==========================>...] - ETA: 6s - loss: 7.7609 - accuracy: 0.0516
on_train_batch_begin: 1607705270.296097s

22 step training time: 1.334547s

on_train_batch_end: 1607705271.641362s

47104/50000 [===========================>..] - ETA: 3s - loss: 7.6796 - accuracy: 0.0529
on_train_batch_begin: 1607705271.641803s

23 step training time: 1.345706s

on_train_batch_end: 1607705272.976801s

49152/50000 [============================>.] - ETA: 1s - loss: 7.6060 - accuracy: 0.0540
on_train_batch_begin: 1607705272.977198s

24 step training time: 1.335395s

on_train_batch_end: 1607705280.558380s

on_test_batch_begin: 1607705280.776965s

25 step training time: 7.799767s

on_epoch_end: 1607705287.432617s

Validation time: 6.655630s

Real time: 1607705287.432617s

Epoch time: 73.9434118270874s

50000/50000 [==============================] - 74s 1ms/sample - loss: 7.5728 - accuracy: 0.0541 - val_loss: 383121.6822 - val_accuracy: 0.0000e+00

on_epoch_begin: 1607705287.432897s

Real time: 1607705287.4329093
Epoch 2/5

on_train_batch_begin: 1607705287.437293s

on_train_batch_end: 1607705288.780886s

 2048/50000 [>.............................] - ETA: 31s - loss: 5.5121 - accuracy: 0.0745
on_train_batch_begin: 1607705288.781288s

1 step training time: 1.343996s

on_train_batch_end: 1607705290.117933s

 4096/50000 [=>............................] - ETA: 30s - loss: 5.4737 - accuracy: 0.0772
on_train_batch_begin: 1607705290.118316s

2 step training time: 1.337027s

on_train_batch_end: 1607705291.453317s

 6144/50000 [==>...........................] - ETA: 28s - loss: 5.4895 - accuracy: 0.0790
on_train_batch_begin: 1607705291.453737s

3 step training time: 1.335421s

on_train_batch_end: 1607705292.792513s

 8192/50000 [===>..........................] - ETA: 27s - loss: 5.4261 - accuracy: 0.0799
on_train_batch_begin: 1607705292.792903s

4 step training time: 1.339166s

on_train_batch_end: 1607705294.139026s

10240/50000 [=====>........................] - ETA: 26s - loss: 5.3598 - accuracy: 0.0799
on_train_batch_begin: 1607705294.139405s

5 step training time: 1.346503s

on_train_batch_end: 1607705295.488165s

12288/50000 [======>.......................] - ETA: 24s - loss: 5.3266 - accuracy: 0.0789
on_train_batch_begin: 1607705295.488553s

6 step training time: 1.349148s

on_train_batch_end: 1607705296.839958s

14336/50000 [=======>......................] - ETA: 23s - loss: 5.3062 - accuracy: 0.0781
on_train_batch_begin: 1607705296.840348s

7 step training time: 1.351795s

on_train_batch_end: 1607705298.183689s

16384/50000 [========>.....................] - ETA: 22s - loss: 5.2482 - accuracy: 0.0779
on_train_batch_begin: 1607705298.184069s

8 step training time: 1.343721s

on_train_batch_end: 1607705299.525034s

18432/50000 [==========>...................] - ETA: 20s - loss: 5.2337 - accuracy: 0.0768
on_train_batch_begin: 1607705299.525499s

9 step training time: 1.341430s

on_train_batch_end: 1607705300.863355s

20480/50000 [===========>..................] - ETA: 19s - loss: 5.2040 - accuracy: 0.0764
on_train_batch_begin: 1607705300.863764s

10 step training time: 1.338265s

on_train_batch_end: 1607705302.214546s

22528/50000 [============>.................] - ETA: 18s - loss: 5.1614 - accuracy: 0.0763
on_train_batch_begin: 1607705302.214935s

11 step training time: 1.351171s

on_train_batch_end: 1607705303.555575s

24576/50000 [=============>................] - ETA: 16s - loss: 5.1117 - accuracy: 0.0763
on_train_batch_begin: 1607705303.555983s

12 step training time: 1.341048s

on_train_batch_end: 1607705304.897297s

26624/50000 [==============>...............] - ETA: 15s - loss: 5.0784 - accuracy: 0.0762
on_train_batch_begin: 1607705304.897737s

13 step training time: 1.341754s

on_train_batch_end: 1607705306.247256s

28672/50000 [================>.............] - ETA: 13s - loss: 5.0371 - accuracy: 0.0764
on_train_batch_begin: 1607705306.247652s

14 step training time: 1.349915s

on_train_batch_end: 1607705307.589620s

30720/50000 [=================>............] - ETA: 12s - loss: 5.0045 - accuracy: 0.0768
on_train_batch_begin: 1607705307.590019s

15 step training time: 1.342366s

on_train_batch_end: 1607705308.942291s

32768/50000 [==================>...........] - ETA: 11s - loss: 4.9710 - accuracy: 0.0771
on_train_batch_begin: 1607705308.942712s

16 step training time: 1.352694s

on_train_batch_end: 1607705310.282472s

34816/50000 [===================>..........] - ETA: 9s - loss: 4.9323 - accuracy: 0.0773 
on_train_batch_begin: 1607705310.282872s

17 step training time: 1.340160s

on_train_batch_end: 1607705311.621032s

36864/50000 [=====================>........] - ETA: 8s - loss: 4.8934 - accuracy: 0.0776
on_train_batch_begin: 1607705311.621483s

18 step training time: 1.338611s

on_train_batch_end: 1607705312.963961s

38912/50000 [======================>.......] - ETA: 7s - loss: 4.8648 - accuracy: 0.0779
on_train_batch_begin: 1607705312.964382s

19 step training time: 1.342899s

on_train_batch_end: 1607705314.302646s

40960/50000 [=======================>......] - ETA: 5s - loss: 4.8455 - accuracy: 0.0782
on_train_batch_begin: 1607705314.303044s

20 step training time: 1.338662s

on_train_batch_end: 1607705315.654691s

43008/50000 [========================>.....] - ETA: 4s - loss: 4.8131 - accuracy: 0.0785
on_train_batch_begin: 1607705315.655078s

21 step training time: 1.352034s

on_train_batch_end: 1607705316.990353s

45056/50000 [==========================>...] - ETA: 3s - loss: 4.7854 - accuracy: 0.0786
on_train_batch_begin: 1607705316.990745s

22 step training time: 1.335667s

on_train_batch_end: 1607705318.336720s

47104/50000 [===========================>..] - ETA: 1s - loss: 4.7650 - accuracy: 0.0785
on_train_batch_begin: 1607705318.337132s

23 step training time: 1.346388s

on_train_batch_end: 1607705319.677761s

49152/50000 [============================>.] - ETA: 0s - loss: 4.7393 - accuracy: 0.0788
on_train_batch_begin: 1607705319.678169s

24 step training time: 1.341037s

on_train_batch_end: 1607705320.277920s

on_test_batch_begin: 1607705320.303816s

25 step training time: 0.625646s

on_epoch_end: 1607705322.124698s

Validation time: 1.820860s

Real time: 1607705322.124698s

Epoch time: 34.69180941581726s

50000/50000 [==============================] - 35s 694us/sample - loss: 4.7300 - accuracy: 0.0788 - val_loss: 550.9545 - val_accuracy: 0.0000e+00

on_epoch_begin: 1607705322.124967s

Real time: 1607705322.1249795
Epoch 3/5

on_train_batch_begin: 1607705322.129519s

on_train_batch_end: 1607705323.489022s

 2048/50000 [>.............................] - ETA: 31s - loss: 3.9271 - accuracy: 0.0826
on_train_batch_begin: 1607705323.489428s

1 step training time: 1.359909s

on_train_batch_end: 1607705324.827214s

 4096/50000 [=>............................] - ETA: 30s - loss: 3.8218 - accuracy: 0.0851
on_train_batch_begin: 1607705324.827627s

2 step training time: 1.338199s

on_train_batch_end: 1607705326.181903s

 6144/50000 [==>...........................] - ETA: 28s - loss: 3.8217 - accuracy: 0.0832
on_train_batch_begin: 1607705326.182334s

3 step training time: 1.354707s

on_train_batch_end: 1607705327.525002s

 8192/50000 [===>..........................] - ETA: 27s - loss: 3.8217 - accuracy: 0.0839
on_train_batch_begin: 1607705327.525417s

4 step training time: 1.343083s

on_train_batch_end: 1607705328.865746s

10240/50000 [=====>........................] - ETA: 26s - loss: 3.8425 - accuracy: 0.0835
on_train_batch_begin: 1607705328.866167s

5 step training time: 1.340750s

on_train_batch_end: 1607705330.206846s

12288/50000 [======>.......................] - ETA: 24s - loss: 3.8347 - accuracy: 0.0824
on_train_batch_begin: 1607705330.207226s

6 step training time: 1.341059s

on_train_batch_end: 1607705331.545229s

14336/50000 [=======>......................] - ETA: 23s - loss: 3.8411 - accuracy: 0.0826
on_train_batch_begin: 1607705331.545662s

7 step training time: 1.338436s

on_train_batch_end: 1607705332.903141s

16384/50000 [========>.....................] - ETA: 22s - loss: 3.8421 - accuracy: 0.0838
on_train_batch_begin: 1607705332.903539s

8 step training time: 1.357877s

on_train_batch_end: 1607705334.246559s

18432/50000 [==========>...................] - ETA: 20s - loss: 3.8371 - accuracy: 0.0852
on_train_batch_begin: 1607705334.246960s

9 step training time: 1.343421s

on_train_batch_end: 1607705335.585315s

20480/50000 [===========>..................] - ETA: 19s - loss: 3.8289 - accuracy: 0.0863
on_train_batch_begin: 1607705335.585762s

10 step training time: 1.338802s

on_train_batch_end: 1607705336.929023s

22528/50000 [============>.................] - ETA: 18s - loss: 3.8265 - accuracy: 0.0863
on_train_batch_begin: 1607705336.929446s

11 step training time: 1.343684s

on_train_batch_end: 1607705338.271266s

24576/50000 [=============>................] - ETA: 16s - loss: 3.8230 - accuracy: 0.0870
on_train_batch_begin: 1607705338.271657s

12 step training time: 1.342211s

on_train_batch_end: 1607705339.615696s

26624/50000 [==============>...............] - ETA: 15s - loss: 3.8092 - accuracy: 0.0872
on_train_batch_begin: 1607705339.616093s

13 step training time: 1.344436s

on_train_batch_end: 1607705340.953280s

28672/50000 [================>.............] - ETA: 14s - loss: 3.7851 - accuracy: 0.0874
on_train_batch_begin: 1607705340.953708s

14 step training time: 1.337615s

on_train_batch_end: 1607705342.303596s

30720/50000 [=================>............] - ETA: 12s - loss: 3.7690 - accuracy: 0.0873
on_train_batch_begin: 1607705342.304005s

15 step training time: 1.350297s

on_train_batch_end: 1607705343.661871s

32768/50000 [==================>...........] - ETA: 11s - loss: 3.7777 - accuracy: 0.0869
on_train_batch_begin: 1607705343.662261s

16 step training time: 1.358256s

on_train_batch_end: 1607705344.999970s

34816/50000 [===================>..........] - ETA: 9s - loss: 3.7670 - accuracy: 0.0869 
on_train_batch_begin: 1607705345.000366s

17 step training time: 1.338105s

on_train_batch_end: 1607705346.348820s

36864/50000 [=====================>........] - ETA: 8s - loss: 3.7636 - accuracy: 0.0866
on_train_batch_begin: 1607705346.349238s

18 step training time: 1.348872s

on_train_batch_end: 1607705347.693178s

38912/50000 [======================>.......] - ETA: 7s - loss: 3.7580 - accuracy: 0.0864
on_train_batch_begin: 1607705347.693610s

19 step training time: 1.344372s

on_train_batch_end: 1607705349.044999s

40960/50000 [=======================>......] - ETA: 5s - loss: 3.7473 - accuracy: 0.0864
on_train_batch_begin: 1607705349.045443s

20 step training time: 1.351832s

on_train_batch_end: 1607705350.392946s

43008/50000 [========================>.....] - ETA: 4s - loss: 3.7405 - accuracy: 0.0862
on_train_batch_begin: 1607705350.393344s

21 step training time: 1.347902s

on_train_batch_end: 1607705351.737603s

45056/50000 [==========================>...] - ETA: 3s - loss: 3.7353 - accuracy: 0.0862
on_train_batch_begin: 1607705351.738018s

22 step training time: 1.344674s

on_train_batch_end: 1607705353.084949s

47104/50000 [===========================>..] - ETA: 1s - loss: 3.7367 - accuracy: 0.0859
on_train_batch_begin: 1607705353.085338s

23 step training time: 1.347319s

on_train_batch_end: 1607705354.419373s

49152/50000 [============================>.] - ETA: 0s - loss: 3.7310 - accuracy: 0.0858
on_train_batch_begin: 1607705354.419821s

24 step training time: 1.334483s

on_train_batch_end: 1607705355.029796s

on_test_batch_begin: 1607705355.055139s

25 step training time: 0.635318s

on_epoch_end: 1607705356.862049s

Validation time: 1.806888s

Real time: 1607705356.862049s

Epoch time: 34.73708915710449s

50000/50000 [==============================] - 35s 695us/sample - loss: 3.7327 - accuracy: 0.0857 - val_loss: 7.2567 - val_accuracy: 0.0998

on_epoch_begin: 1607705356.862293s

Real time: 1607705356.862303
Epoch 4/5

on_train_batch_begin: 1607705356.866541s

on_train_batch_end: 1607705358.212235s

 2048/50000 [>.............................] - ETA: 31s - loss: 3.5313 - accuracy: 0.0825
on_train_batch_begin: 1607705358.212617s

1 step training time: 1.346076s

on_train_batch_end: 1607705359.559993s

 4096/50000 [=>............................] - ETA: 30s - loss: 3.5883 - accuracy: 0.0822
on_train_batch_begin: 1607705359.560373s

2 step training time: 1.347755s

on_train_batch_end: 1607705360.904531s

 6144/50000 [==>...........................] - ETA: 28s - loss: 3.5544 - accuracy: 0.0829
on_train_batch_begin: 1607705360.904916s

3 step training time: 1.344543s

on_train_batch_end: 1607705362.250839s

 8192/50000 [===>..........................] - ETA: 27s - loss: 3.5357 - accuracy: 0.0844
on_train_batch_begin: 1607705362.251220s

4 step training time: 1.346305s

on_train_batch_end: 1607705363.605014s

10240/50000 [=====>........................] - ETA: 26s - loss: 3.5044 - accuracy: 0.0851
on_train_batch_begin: 1607705363.605417s

5 step training time: 1.354197s

on_train_batch_end: 1607705364.946978s

12288/50000 [======>.......................] - ETA: 24s - loss: 3.4753 - accuracy: 0.0851
on_train_batch_begin: 1607705364.947392s

6 step training time: 1.341974s

on_train_batch_end: 1607705366.291418s

14336/50000 [=======>......................] - ETA: 23s - loss: 3.4607 - accuracy: 0.0855
on_train_batch_begin: 1607705366.291800s

7 step training time: 1.344408s

on_train_batch_end: 1607705367.634898s

16384/50000 [========>.....................] - ETA: 22s - loss: 3.4429 - accuracy: 0.0858
on_train_batch_begin: 1607705367.635307s

8 step training time: 1.343507s

on_train_batch_end: 1607705368.983830s

18432/50000 [==========>...................] - ETA: 20s - loss: 3.4388 - accuracy: 0.0857
on_train_batch_begin: 1607705368.984224s

9 step training time: 1.348917s

on_train_batch_end: 1607705370.331223s

20480/50000 [===========>..................] - ETA: 19s - loss: 3.4357 - accuracy: 0.0852
on_train_batch_begin: 1607705370.331616s

10 step training time: 1.347392s

on_train_batch_end: 1607705371.691898s

22528/50000 [============>.................] - ETA: 18s - loss: 3.4260 - accuracy: 0.0852
on_train_batch_begin: 1607705371.692292s

11 step training time: 1.360676s

on_train_batch_end: 1607705373.028360s

24576/50000 [=============>................] - ETA: 16s - loss: 3.4217 - accuracy: 0.0849
on_train_batch_begin: 1607705373.028754s

12 step training time: 1.336462s

on_train_batch_end: 1607705374.374369s

26624/50000 [==============>...............] - ETA: 15s - loss: 3.4152 - accuracy: 0.0845
on_train_batch_begin: 1607705374.374753s

13 step training time: 1.346000s

on_train_batch_end: 1607705375.716828s

28672/50000 [================>.............] - ETA: 14s - loss: 3.4103 - accuracy: 0.0844
on_train_batch_begin: 1607705375.717217s

14 step training time: 1.342464s

on_train_batch_end: 1607705377.061188s

30720/50000 [=================>............] - ETA: 12s - loss: 3.4104 - accuracy: 0.0841
on_train_batch_begin: 1607705377.061651s

15 step training time: 1.344434s

on_train_batch_end: 1607705378.406474s

32768/50000 [==================>...........] - ETA: 11s - loss: 3.4110 - accuracy: 0.0840
on_train_batch_begin: 1607705378.406898s

16 step training time: 1.345247s

on_train_batch_end: 1607705379.754970s

34816/50000 [===================>..........] - ETA: 9s - loss: 3.4046 - accuracy: 0.0838 
on_train_batch_begin: 1607705379.755378s

17 step training time: 1.348480s

on_train_batch_end: 1607705381.110888s

36864/50000 [=====================>........] - ETA: 8s - loss: 3.4043 - accuracy: 0.0838
on_train_batch_begin: 1607705381.111274s

18 step training time: 1.355895s

on_train_batch_end: 1607705382.456525s

38912/50000 [======================>.......] - ETA: 7s - loss: 3.4020 - accuracy: 0.0840
on_train_batch_begin: 1607705382.456916s

19 step training time: 1.345642s

on_train_batch_end: 1607705383.798792s

40960/50000 [=======================>......] - ETA: 5s - loss: 3.3951 - accuracy: 0.0842
on_train_batch_begin: 1607705383.799197s

20 step training time: 1.342281s

on_train_batch_end: 1607705385.157666s

43008/50000 [========================>.....] - ETA: 4s - loss: 3.3922 - accuracy: 0.0843
on_train_batch_begin: 1607705385.158072s

21 step training time: 1.358875s

on_train_batch_end: 1607705386.509395s

45056/50000 [==========================>...] - ETA: 3s - loss: 3.3888 - accuracy: 0.0844
on_train_batch_begin: 1607705386.509816s

22 step training time: 1.351743s

on_train_batch_end: 1607705387.864585s

47104/50000 [===========================>..] - ETA: 1s - loss: 3.3836 - accuracy: 0.0846
on_train_batch_begin: 1607705387.864983s

23 step training time: 1.355167s

on_train_batch_end: 1607705389.207870s

49152/50000 [============================>.] - ETA: 0s - loss: 3.3826 - accuracy: 0.0845
on_train_batch_begin: 1607705389.208297s

24 step training time: 1.343314s

on_train_batch_end: 1607705389.800566s

on_test_batch_begin: 1607705389.825891s

25 step training time: 0.617595s

on_epoch_end: 1607705391.639783s

Validation time: 1.813870s

Real time: 1607705391.639783s

Epoch time: 34.777501583099365s

50000/50000 [==============================] - 35s 696us/sample - loss: 3.3857 - accuracy: 0.0845 - val_loss: 7.0348 - val_accuracy: 0.1001

on_epoch_begin: 1607705391.640037s

Real time: 1607705391.640047
Epoch 5/5

on_train_batch_begin: 1607705391.644502s

on_train_batch_end: 1607705392.987658s

 2048/50000 [>.............................] - ETA: 31s - loss: 3.2403 - accuracy: 0.0946
on_train_batch_begin: 1607705392.988058s

1 step training time: 1.343556s

on_train_batch_end: 1607705394.329749s

 4096/50000 [=>............................] - ETA: 30s - loss: 3.3106 - accuracy: 0.0956
on_train_batch_begin: 1607705394.330176s

2 step training time: 1.342118s

on_train_batch_end: 1607705395.675460s

 6144/50000 [==>...........................] - ETA: 28s - loss: 3.3592 - accuracy: 0.0968
on_train_batch_begin: 1607705395.675855s

3 step training time: 1.345678s

on_train_batch_end: 1607705397.030937s

 8192/50000 [===>..........................] - ETA: 27s - loss: 3.4122 - accuracy: 0.0972
on_train_batch_begin: 1607705397.031314s

4 step training time: 1.355460s

on_train_batch_end: 1607705398.385341s

10240/50000 [=====>........................] - ETA: 26s - loss: 3.4365 - accuracy: 0.0972
on_train_batch_begin: 1607705398.385795s

5 step training time: 1.354481s

on_train_batch_end: 1607705399.729328s

12288/50000 [======>.......................] - ETA: 24s - loss: 3.5016 - accuracy: 0.0964
on_train_batch_begin: 1607705399.729749s

6 step training time: 1.343954s

on_train_batch_end: 1607705401.071288s

14336/50000 [=======>......................] - ETA: 23s - loss: 3.5110 - accuracy: 0.0959
on_train_batch_begin: 1607705401.071687s

7 step training time: 1.341938s

on_train_batch_end: 1607705402.425097s

16384/50000 [========>.....................] - ETA: 22s - loss: 3.5044 - accuracy: 0.0958
on_train_batch_begin: 1607705402.425542s

8 step training time: 1.353855s

on_train_batch_end: 1607705403.775386s

18432/50000 [==========>...................] - ETA: 20s - loss: 3.5071 - accuracy: 0.0953
on_train_batch_begin: 1607705403.775797s

9 step training time: 1.350255s

on_train_batch_end: 1607705405.122898s

20480/50000 [===========>..................] - ETA: 19s - loss: 3.5052 - accuracy: 0.0950
on_train_batch_begin: 1607705405.123508s

10 step training time: 1.347712s

on_train_batch_end: 1607705406.477348s

22528/50000 [============>.................] - ETA: 18s - loss: 3.5160 - accuracy: 0.0943
on_train_batch_begin: 1607705406.477764s

11 step training time: 1.354255s

on_train_batch_end: 1607705407.825149s

24576/50000 [=============>................] - ETA: 16s - loss: 3.5035 - accuracy: 0.0939
on_train_batch_begin: 1607705407.825583s

12 step training time: 1.347819s

on_train_batch_end: 1607705409.173184s

26624/50000 [==============>...............] - ETA: 15s - loss: 3.5014 - accuracy: 0.0936
on_train_batch_begin: 1607705409.173598s

13 step training time: 1.348015s

on_train_batch_end: 1607705410.521040s

28672/50000 [================>.............] - ETA: 14s - loss: 3.4896 - accuracy: 0.0934
on_train_batch_begin: 1607705410.521457s

14 step training time: 1.347859s

on_train_batch_end: 1607705411.865014s

30720/50000 [=================>............] - ETA: 12s - loss: 3.4716 - accuracy: 0.0932
on_train_batch_begin: 1607705411.865454s

15 step training time: 1.343997s

on_train_batch_end: 1607705413.219644s

32768/50000 [==================>...........] - ETA: 11s - loss: 3.4737 - accuracy: 0.0929
on_train_batch_begin: 1607705413.220047s

16 step training time: 1.354593s

on_train_batch_end: 1607705414.560639s

34816/50000 [===================>..........] - ETA: 9s - loss: 3.4675 - accuracy: 0.0927 
on_train_batch_begin: 1607705414.561046s

17 step training time: 1.341000s

on_train_batch_end: 1607705415.904339s

36864/50000 [=====================>........] - ETA: 8s - loss: 3.4619 - accuracy: 0.0924
on_train_batch_begin: 1607705415.904740s

18 step training time: 1.343694s

on_train_batch_end: 1607705417.253885s

38912/50000 [======================>.......] - ETA: 7s - loss: 3.4542 - accuracy: 0.0921
on_train_batch_begin: 1607705417.254287s

19 step training time: 1.349547s

on_train_batch_end: 1607705418.603303s

40960/50000 [=======================>......] - ETA: 5s - loss: 3.4511 - accuracy: 0.0918
on_train_batch_begin: 1607705418.603684s

20 step training time: 1.349397s

on_train_batch_end: 1607705419.947924s

43008/50000 [========================>.....] - ETA: 4s - loss: 3.4421 - accuracy: 0.0916
on_train_batch_begin: 1607705419.948302s

21 step training time: 1.344618s

on_train_batch_end: 1607705421.300586s

45056/50000 [==========================>...] - ETA: 3s - loss: 3.4396 - accuracy: 0.0914
on_train_batch_begin: 1607705421.300993s

22 step training time: 1.352691s

on_train_batch_end: 1607705422.648552s

47104/50000 [===========================>..] - ETA: 1s - loss: 3.4404 - accuracy: 0.0911
on_train_batch_begin: 1607705422.648962s

23 step training time: 1.347968s

on_train_batch_end: 1607705423.990471s

49152/50000 [============================>.] - ETA: 0s - loss: 3.4353 - accuracy: 0.0909
on_train_batch_begin: 1607705423.990875s

24 step training time: 1.341914s

on_train_batch_end: 1607705424.588892s

on_test_batch_begin: 1607705424.614675s

25 step training time: 0.623800s

on_epoch_end: 1607705426.424660s

Validation time: 1.809966s

Real time: 1607705426.424660s

Epoch time: 34.7846360206604s

50000/50000 [==============================] - 35s 696us/sample - loss: 3.4303 - accuracy: 0.0909 - val_loss: 21719.1668 - val_accuracy: 0.0000e+00
Tempo do fit: 216.8111970424652