wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:25
   188416/170498071 [..............................] - ETA: 1:19
  1236992/170498071 [..............................] - ETA: 21s 
  4513792/170498071 [..............................] - ETA: 7s 
  8183808/170498071 [>.............................] - ETA: 5s
 11771904/170498071 [=>............................] - ETA: 4s
 15032320/170498071 [=>............................] - ETA: 3s
 18653184/170498071 [==>...........................] - ETA: 3s
 22315008/170498071 [==>...........................] - ETA: 3s
 25829376/170498071 [===>..........................] - ETA: 2s
 29073408/170498071 [====>.........................] - ETA: 2s
 32595968/170498071 [====>.........................] - ETA: 2s
 36216832/170498071 [=====>........................] - ETA: 2s
 39821312/170498071 [======>.......................] - ETA: 2s
 43065344/170498071 [======>.......................] - ETA: 2s
 46432256/170498071 [=======>......................] - ETA: 2s
 50028544/170498071 [=======>......................] - ETA: 2s
 53420032/170498071 [========>.....................] - ETA: 2s
 56958976/170498071 [=========>....................] - ETA: 1s
 60317696/170498071 [=========>....................] - ETA: 1s
 63799296/170498071 [==========>...................] - ETA: 1s
 67297280/170498071 [==========>...................] - ETA: 1s
 70852608/170498071 [===========>..................] - ETA: 1s
 74309632/170498071 [============>.................] - ETA: 1s
 77684736/170498071 [============>.................] - ETA: 1s
 81207296/170498071 [=============>................] - ETA: 1s
 84713472/170498071 [=============>................] - ETA: 1s
 88268800/170498071 [==============>...............] - ETA: 1s
 91570176/170498071 [===============>..............] - ETA: 1s
 95002624/170498071 [===============>..............] - ETA: 1s
 98525184/170498071 [================>.............] - ETA: 1s
102080512/170498071 [================>.............] - ETA: 1s
105553920/170498071 [=================>............] - ETA: 1s
108863488/170498071 [==================>...........] - ETA: 0s
112369664/170498071 [==================>...........] - ETA: 0s
115892224/170498071 [===================>..........] - ETA: 0s
119185408/170498071 [===================>..........] - ETA: 0s
122822656/170498071 [====================>.........] - ETA: 0s
126230528/170498071 [=====================>........] - ETA: 0s
129687552/170498071 [=====================>........] - ETA: 0s
133111808/170498071 [======================>.......] - ETA: 0s
136667136/170498071 [=======================>......] - ETA: 0s
140156928/170498071 [=======================>......] - ETA: 0s
143679488/170498071 [========================>.....] - ETA: 0s
147054592/170498071 [========================>.....] - ETA: 0s
150577152/170498071 [=========================>....] - ETA: 0s
154148864/170498071 [==========================>...] - ETA: 0s
157630464/170498071 [==========================>...] - ETA: 0s
160997376/170498071 [===========================>..] - ETA: 0s
164421632/170498071 [===========================>..] - ETA: 0s
167985152/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 1s
 5644288/94765736 [>.............................] - ETA: 0s
 9388032/94765736 [=>............................] - ETA: 1s
10846208/94765736 [==>...........................] - ETA: 1s
15851520/94765736 [====>.........................] - ETA: 1s
18841600/94765736 [====>.........................] - ETA: 1s
27508736/94765736 [=======>......................] - ETA: 0s
28286976/94765736 [=======>......................] - ETA: 1s
30007296/94765736 [========>.....................] - ETA: 1s
37683200/94765736 [==========>...................] - ETA: 0s
46104576/94765736 [=============>................] - ETA: 0s
50438144/94765736 [==============>...............] - ETA: 0s
56598528/94765736 [================>.............] - ETA: 0s
62078976/94765736 [==================>...........] - ETA: 0s
65986560/94765736 [===================>..........] - ETA: 0s
70582272/94765736 [=====================>........] - ETA: 0s
76619776/94765736 [=======================>......] - ETA: 0s
84893696/94765736 [=========================>....] - ETA: 0s
94281728/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 1s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 17.19009804725647
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1615691776.214038s

Real time: 1615691776.2140546
Epoch 1/5

on_train_batch_begin: 1615691777.010583s

on_train_batch_end: 1615691824.700481s

 2048/50000 [>.............................] - ETA: 18:55 - loss: 17.8119 - accuracy: 3.0041e-04
on_train_batch_begin: 1615691824.701071s

1 step training time: 47.690488s

on_train_batch_end: 1615691824.911160s

 4096/50000 [=>............................] - ETA: 9:05 - loss: 14.4610 - accuracy: 4.0960e-04 
on_train_batch_begin: 1615691824.911499s

2 step training time: 0.210428s

on_train_batch_end: 1615691825.116989s

 6144/50000 [==>...........................] - ETA: 5:49 - loss: 12.4935 - accuracy: 0.0010    
on_train_batch_begin: 1615691825.117287s

3 step training time: 0.205788s

on_train_batch_end: 1615691825.327094s

 8192/50000 [===>..........................] - ETA: 4:10 - loss: 11.3971 - accuracy: 0.0020
on_train_batch_begin: 1615691825.327394s

4 step training time: 0.210107s

on_train_batch_end: 1615691825.532325s

10240/50000 [=====>........................] - ETA: 3:11 - loss: 10.6871 - accuracy: 0.0040
on_train_batch_begin: 1615691825.532635s

5 step training time: 0.205242s

on_train_batch_end: 1615691825.741527s

12288/50000 [======>.......................] - ETA: 2:32 - loss: 10.1988 - accuracy: 0.0067
on_train_batch_begin: 1615691825.741828s

6 step training time: 0.209193s

on_train_batch_end: 1615691825.947170s

14336/50000 [=======>......................] - ETA: 2:03 - loss: 9.8493 - accuracy: 0.0097 
on_train_batch_begin: 1615691825.947464s

7 step training time: 0.205636s

on_train_batch_end: 1615691826.153986s

16384/50000 [========>.....................] - ETA: 1:42 - loss: 9.5723 - accuracy: 0.0124
on_train_batch_begin: 1615691826.154277s

8 step training time: 0.206813s

on_train_batch_end: 1615691826.361334s

18432/50000 [==========>...................] - ETA: 1:25 - loss: 9.3314 - accuracy: 0.0158
on_train_batch_begin: 1615691826.361657s

9 step training time: 0.207379s

on_train_batch_end: 1615691826.567528s

20480/50000 [===========>..................] - ETA: 1:12 - loss: 9.1488 - accuracy: 0.0184
on_train_batch_begin: 1615691826.567834s

10 step training time: 0.206177s

on_train_batch_end: 1615691826.773590s

22528/50000 [============>.................] - ETA: 1:01 - loss: 8.9876 - accuracy: 0.0214
on_train_batch_begin: 1615691826.773883s

11 step training time: 0.206048s

on_train_batch_end: 1615691826.979376s

24576/50000 [=============>................] - ETA: 52s - loss: 8.8325 - accuracy: 0.0248 
on_train_batch_begin: 1615691826.979668s

12 step training time: 0.205786s

on_train_batch_end: 1615691827.186545s

26624/50000 [==============>...............] - ETA: 44s - loss: 8.6879 - accuracy: 0.0282
on_train_batch_begin: 1615691827.186840s

13 step training time: 0.207172s

on_train_batch_end: 1615691827.392023s

28672/50000 [================>.............] - ETA: 38s - loss: 8.5633 - accuracy: 0.0313
on_train_batch_begin: 1615691827.392318s

14 step training time: 0.205478s

on_train_batch_end: 1615691827.597111s

30720/50000 [=================>............] - ETA: 32s - loss: 8.4463 - accuracy: 0.0337
on_train_batch_begin: 1615691827.597402s

15 step training time: 0.205084s

on_train_batch_end: 1615691827.801522s

32768/50000 [==================>...........] - ETA: 27s - loss: 8.3446 - accuracy: 0.0360
on_train_batch_begin: 1615691827.801838s

16 step training time: 0.204437s

on_train_batch_end: 1615691828.006124s

34816/50000 [===================>..........] - ETA: 22s - loss: 8.2498 - accuracy: 0.0383
on_train_batch_begin: 1615691828.006412s

17 step training time: 0.204573s

on_train_batch_end: 1615691828.211741s

36864/50000 [=====================>........] - ETA: 18s - loss: 8.1523 - accuracy: 0.0402
on_train_batch_begin: 1615691828.212033s

18 step training time: 0.205621s

on_train_batch_end: 1615691828.420433s

38912/50000 [======================>.......] - ETA: 14s - loss: 8.0636 - accuracy: 0.0411
on_train_batch_begin: 1615691828.420721s

19 step training time: 0.208688s

on_train_batch_end: 1615691828.627002s

40960/50000 [=======================>......] - ETA: 11s - loss: 7.9839 - accuracy: 0.0414
on_train_batch_begin: 1615691828.627292s

20 step training time: 0.206571s

on_train_batch_end: 1615691828.832480s

43008/50000 [========================>.....] - ETA: 8s - loss: 7.9095 - accuracy: 0.0419 
on_train_batch_begin: 1615691828.832814s

21 step training time: 0.205522s

on_train_batch_end: 1615691829.036990s

45056/50000 [==========================>...] - ETA: 5s - loss: 7.8373 - accuracy: 0.0425
on_train_batch_begin: 1615691829.037276s

22 step training time: 0.204462s

on_train_batch_end: 1615691829.243198s

47104/50000 [===========================>..] - ETA: 3s - loss: 7.7704 - accuracy: 0.0435
on_train_batch_begin: 1615691829.243494s

23 step training time: 0.206218s

on_train_batch_end: 1615691829.447088s

49152/50000 [============================>.] - ETA: 0s - loss: 7.7071 - accuracy: 0.0447
on_train_batch_begin: 1615691829.447376s

24 step training time: 0.203883s

on_train_batch_end: 1615691831.973427s

on_test_batch_begin: 1615691832.211205s

25 step training time: 2.763828s

on_epoch_end: 1615691837.617323s

Validation time: 5.406104s

Real time: 1615691837.617323s

Epoch time: 61.40328550338745s

50000/50000 [==============================] - 61s 1ms/sample - loss: 7.6843 - accuracy: 0.0448 - val_loss: 11323.6851 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615691837.617510s

Real time: 1615691837.6175308
Epoch 2/5

on_train_batch_begin: 1615691837.622049s

on_train_batch_end: 1615691837.830825s

 2048/50000 [>.............................] - ETA: 4s - loss: 6.1353 - accuracy: 0.0706
on_train_batch_begin: 1615691837.831120s

1 step training time: 0.209072s

on_train_batch_end: 1615691838.037485s

 4096/50000 [=>............................] - ETA: 4s - loss: 6.0898 - accuracy: 0.0720
on_train_batch_begin: 1615691838.037773s

2 step training time: 0.206652s

on_train_batch_end: 1615691838.249006s

 6144/50000 [==>...........................] - ETA: 4s - loss: 6.0813 - accuracy: 0.0691
on_train_batch_begin: 1615691838.249292s

3 step training time: 0.211519s

on_train_batch_end: 1615691838.454494s

 8192/50000 [===>..........................] - ETA: 4s - loss: 6.0573 - accuracy: 0.0694
on_train_batch_begin: 1615691838.454791s

4 step training time: 0.205500s

on_train_batch_end: 1615691838.660351s

10240/50000 [=====>........................] - ETA: 4s - loss: 6.0464 - accuracy: 0.0699
on_train_batch_begin: 1615691838.660634s

5 step training time: 0.205843s

on_train_batch_end: 1615691838.865332s

12288/50000 [======>.......................] - ETA: 3s - loss: 6.0003 - accuracy: 0.0702
on_train_batch_begin: 1615691838.865618s

6 step training time: 0.204984s

on_train_batch_end: 1615691839.072196s

14336/50000 [=======>......................] - ETA: 3s - loss: 5.9605 - accuracy: 0.0704
on_train_batch_begin: 1615691839.072498s

7 step training time: 0.206880s

on_train_batch_end: 1615691839.280330s

16384/50000 [========>.....................] - ETA: 3s - loss: 5.9332 - accuracy: 0.0695
on_train_batch_begin: 1615691839.280614s

8 step training time: 0.208115s

on_train_batch_end: 1615691839.488095s

18432/50000 [==========>...................] - ETA: 3s - loss: 5.9109 - accuracy: 0.0687
on_train_batch_begin: 1615691839.488382s

9 step training time: 0.207768s

on_train_batch_end: 1615691839.693568s

20480/50000 [===========>..................] - ETA: 2s - loss: 5.8898 - accuracy: 0.0686
on_train_batch_begin: 1615691839.693856s

10 step training time: 0.205474s

on_train_batch_end: 1615691839.900371s

22528/50000 [============>.................] - ETA: 2s - loss: 5.8630 - accuracy: 0.0701
on_train_batch_begin: 1615691839.900671s

11 step training time: 0.206815s

on_train_batch_end: 1615691840.107886s

24576/50000 [=============>................] - ETA: 2s - loss: 5.8496 - accuracy: 0.0703
on_train_batch_begin: 1615691840.108173s

12 step training time: 0.207502s

on_train_batch_end: 1615691840.316430s

26624/50000 [==============>...............] - ETA: 2s - loss: 5.8377 - accuracy: 0.0703
on_train_batch_begin: 1615691840.316716s

13 step training time: 0.208543s

on_train_batch_end: 1615691840.525002s

28672/50000 [================>.............] - ETA: 2s - loss: 5.8151 - accuracy: 0.0712
on_train_batch_begin: 1615691840.525279s

14 step training time: 0.208563s

on_train_batch_end: 1615691840.732164s

30720/50000 [=================>............] - ETA: 1s - loss: 5.7983 - accuracy: 0.0715
on_train_batch_begin: 1615691840.732494s

15 step training time: 0.207215s

on_train_batch_end: 1615691840.937753s

32768/50000 [==================>...........] - ETA: 1s - loss: 5.7892 - accuracy: 0.0719
on_train_batch_begin: 1615691840.938061s

16 step training time: 0.205568s

on_train_batch_end: 1615691841.143379s

34816/50000 [===================>..........] - ETA: 1s - loss: 5.7607 - accuracy: 0.0722
on_train_batch_begin: 1615691841.143669s

17 step training time: 0.205607s

on_train_batch_end: 1615691841.350629s

36864/50000 [=====================>........] - ETA: 1s - loss: 5.7424 - accuracy: 0.0727
on_train_batch_begin: 1615691841.350914s

18 step training time: 0.207245s

on_train_batch_end: 1615691841.556716s

38912/50000 [======================>.......] - ETA: 1s - loss: 5.7240 - accuracy: 0.0727
on_train_batch_begin: 1615691841.557005s

19 step training time: 0.206091s

on_train_batch_end: 1615691841.763363s

40960/50000 [=======================>......] - ETA: 0s - loss: 5.7115 - accuracy: 0.0727
on_train_batch_begin: 1615691841.763654s

20 step training time: 0.206648s

on_train_batch_end: 1615691841.969889s

43008/50000 [========================>.....] - ETA: 0s - loss: 5.6979 - accuracy: 0.0723
on_train_batch_begin: 1615691841.970203s

21 step training time: 0.206550s

on_train_batch_end: 1615691842.177028s

45056/50000 [==========================>...] - ETA: 0s - loss: 5.6838 - accuracy: 0.0715
on_train_batch_begin: 1615691842.177315s

22 step training time: 0.207112s

on_train_batch_end: 1615691842.384533s

47104/50000 [===========================>..] - ETA: 0s - loss: 5.6626 - accuracy: 0.0709
on_train_batch_begin: 1615691842.384821s

23 step training time: 0.207506s

on_train_batch_end: 1615691842.589661s

49152/50000 [============================>.] - ETA: 0s - loss: 5.6507 - accuracy: 0.0700
on_train_batch_begin: 1615691842.589973s

24 step training time: 0.205153s

on_train_batch_end: 1615691842.713185s

on_test_batch_begin: 1615691842.804748s

25 step training time: 0.214774s

on_epoch_end: 1615691843.089788s

Validation time: 0.285025s

Real time: 1615691843.089788s

Epoch time: 5.472272157669067s

50000/50000 [==============================] - 5s 109us/sample - loss: 5.6454 - accuracy: 0.0699 - val_loss: 7.7474 - val_accuracy: 0.0417

on_epoch_begin: 1615691843.089978s

Real time: 1615691843.0899832
Epoch 3/5

on_train_batch_begin: 1615691843.094550s

on_train_batch_end: 1615691843.301002s

 2048/50000 [>.............................] - ETA: 4s - loss: 5.0446 - accuracy: 0.0506
on_train_batch_begin: 1615691843.301325s

1 step training time: 0.206774s

on_train_batch_end: 1615691843.506040s

 4096/50000 [=>............................] - ETA: 4s - loss: 5.0263 - accuracy: 0.0507
on_train_batch_begin: 1615691843.506331s

2 step training time: 0.205006s

on_train_batch_end: 1615691843.714957s

 6144/50000 [==>...........................] - ETA: 4s - loss: 4.9825 - accuracy: 0.0524
on_train_batch_begin: 1615691843.715249s

3 step training time: 0.208918s

on_train_batch_end: 1615691843.922724s

 8192/50000 [===>..........................] - ETA: 4s - loss: 4.9362 - accuracy: 0.0533
on_train_batch_begin: 1615691843.923009s

4 step training time: 0.207761s

on_train_batch_end: 1615691844.129114s

10240/50000 [=====>........................] - ETA: 4s - loss: 4.8883 - accuracy: 0.0540
on_train_batch_begin: 1615691844.129401s

5 step training time: 0.206391s

on_train_batch_end: 1615691844.336875s

12288/50000 [======>.......................] - ETA: 3s - loss: 4.8510 - accuracy: 0.0550
on_train_batch_begin: 1615691844.337181s

6 step training time: 0.207780s

on_train_batch_end: 1615691844.547334s

14336/50000 [=======>......................] - ETA: 3s - loss: 4.7901 - accuracy: 0.0570
on_train_batch_begin: 1615691844.547621s

7 step training time: 0.210440s

on_train_batch_end: 1615691844.754051s

16384/50000 [========>.....................] - ETA: 3s - loss: 4.7298 - accuracy: 0.0591
on_train_batch_begin: 1615691844.754361s

8 step training time: 0.206740s

on_train_batch_end: 1615691844.963071s

18432/50000 [==========>...................] - ETA: 3s - loss: 4.6734 - accuracy: 0.0608
on_train_batch_begin: 1615691844.963371s

9 step training time: 0.209010s

on_train_batch_end: 1615691845.168976s

20480/50000 [===========>..................] - ETA: 2s - loss: 4.6323 - accuracy: 0.0624
on_train_batch_begin: 1615691845.169262s

10 step training time: 0.205891s

on_train_batch_end: 1615691845.377542s

22528/50000 [============>.................] - ETA: 2s - loss: 4.5610 - accuracy: 0.0646
on_train_batch_begin: 1615691845.377829s

11 step training time: 0.208567s

on_train_batch_end: 1615691845.584016s

24576/50000 [=============>................] - ETA: 2s - loss: 4.4889 - accuracy: 0.0665
on_train_batch_begin: 1615691845.584307s

12 step training time: 0.206478s

on_train_batch_end: 1615691845.790688s

26624/50000 [==============>...............] - ETA: 2s - loss: 4.4246 - accuracy: 0.0682
on_train_batch_begin: 1615691845.790977s

13 step training time: 0.206671s

on_train_batch_end: 1615691845.996990s

28672/50000 [================>.............] - ETA: 2s - loss: 4.3608 - accuracy: 0.0698
on_train_batch_begin: 1615691845.997277s

14 step training time: 0.206299s

on_train_batch_end: 1615691846.204365s

30720/50000 [=================>............] - ETA: 1s - loss: 4.2987 - accuracy: 0.0711
on_train_batch_begin: 1615691846.204663s

15 step training time: 0.207386s

on_train_batch_end: 1615691846.411409s

32768/50000 [==================>...........] - ETA: 1s - loss: 4.2375 - accuracy: 0.0725
on_train_batch_begin: 1615691846.411698s

16 step training time: 0.207035s

on_train_batch_end: 1615691846.618783s

34816/50000 [===================>..........] - ETA: 1s - loss: 4.1755 - accuracy: 0.0737
on_train_batch_begin: 1615691846.619074s

17 step training time: 0.207376s

on_train_batch_end: 1615691846.830702s

36864/50000 [=====================>........] - ETA: 1s - loss: 4.1193 - accuracy: 0.0749
on_train_batch_begin: 1615691846.830994s

18 step training time: 0.211920s

on_train_batch_end: 1615691847.039754s

38912/50000 [======================>.......] - ETA: 1s - loss: 4.0728 - accuracy: 0.0758
on_train_batch_begin: 1615691847.040044s

19 step training time: 0.209050s

on_train_batch_end: 1615691847.248593s

40960/50000 [=======================>......] - ETA: 0s - loss: 4.0116 - accuracy: 0.0770
on_train_batch_begin: 1615691847.248879s

20 step training time: 0.208836s

on_train_batch_end: 1615691847.456547s

43008/50000 [========================>.....] - ETA: 0s - loss: 3.9585 - accuracy: 0.0780
on_train_batch_begin: 1615691847.456839s

21 step training time: 0.207959s

on_train_batch_end: 1615691847.664803s

45056/50000 [==========================>...] - ETA: 0s - loss: 3.9083 - accuracy: 0.0790
on_train_batch_begin: 1615691847.665094s

22 step training time: 0.208256s

on_train_batch_end: 1615691847.872393s

47104/50000 [===========================>..] - ETA: 0s - loss: 3.8663 - accuracy: 0.0798
on_train_batch_begin: 1615691847.872677s

23 step training time: 0.207583s

on_train_batch_end: 1615691848.077677s

49152/50000 [============================>.] - ETA: 0s - loss: 3.8194 - accuracy: 0.0807
on_train_batch_begin: 1615691848.077981s

24 step training time: 0.205304s

on_train_batch_end: 1615691848.200103s

on_test_batch_begin: 1615691848.290334s

25 step training time: 0.212353s

on_epoch_end: 1615691848.614401s

Validation time: 0.324052s

Real time: 1615691848.614401s

Epoch time: 5.524432182312012s

50000/50000 [==============================] - 6s 110us/sample - loss: 3.8017 - accuracy: 0.0808 - val_loss: 7.4521 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615691848.614579s

Real time: 1615691848.6145842
Epoch 4/5

on_train_batch_begin: 1615691848.618981s

on_train_batch_end: 1615691848.830806s

 2048/50000 [>.............................] - ETA: 5s - loss: 2.5192 - accuracy: 0.1015
on_train_batch_begin: 1615691848.831122s

1 step training time: 0.212141s

on_train_batch_end: 1615691849.039309s

 4096/50000 [=>............................] - ETA: 4s - loss: 2.4974 - accuracy: 0.1005
on_train_batch_begin: 1615691849.039595s

2 step training time: 0.208472s

on_train_batch_end: 1615691849.246252s

 6144/50000 [==>...........................] - ETA: 4s - loss: 2.5348 - accuracy: 0.0997
on_train_batch_begin: 1615691849.246545s

3 step training time: 0.206950s

on_train_batch_end: 1615691849.455494s

 8192/50000 [===>..........................] - ETA: 4s - loss: 2.5465 - accuracy: 0.0996
on_train_batch_begin: 1615691849.455781s

4 step training time: 0.209237s

on_train_batch_end: 1615691849.663058s

10240/50000 [=====>........................] - ETA: 4s - loss: 2.5074 - accuracy: 0.0997
on_train_batch_begin: 1615691849.663342s

5 step training time: 0.207560s

on_train_batch_end: 1615691849.871199s

12288/50000 [======>.......................] - ETA: 3s - loss: 2.4704 - accuracy: 0.0999
on_train_batch_begin: 1615691849.871490s

6 step training time: 0.208148s

on_train_batch_end: 1615691850.078661s

14336/50000 [=======>......................] - ETA: 3s - loss: 2.4563 - accuracy: 0.0998
on_train_batch_begin: 1615691850.078950s

7 step training time: 0.207461s

on_train_batch_end: 1615691850.287598s

16384/50000 [========>.....................] - ETA: 3s - loss: 2.4084 - accuracy: 0.0998
on_train_batch_begin: 1615691850.287887s

8 step training time: 0.208937s

on_train_batch_end: 1615691850.499542s

18432/50000 [==========>...................] - ETA: 3s - loss: 2.3890 - accuracy: 0.0998
on_train_batch_begin: 1615691850.499827s

9 step training time: 0.211940s

on_train_batch_end: 1615691850.708919s

20480/50000 [===========>..................] - ETA: 3s - loss: 2.3565 - accuracy: 0.0999
on_train_batch_begin: 1615691850.709221s

10 step training time: 0.209394s

on_train_batch_end: 1615691850.916563s

22528/50000 [============>.................] - ETA: 2s - loss: 2.3370 - accuracy: 0.0999
on_train_batch_begin: 1615691850.916854s

11 step training time: 0.207633s

on_train_batch_end: 1615691851.125352s

24576/50000 [=============>................] - ETA: 2s - loss: 2.3138 - accuracy: 0.0999
on_train_batch_begin: 1615691851.125640s

12 step training time: 0.208786s

on_train_batch_end: 1615691851.332183s

26624/50000 [==============>...............] - ETA: 2s - loss: 2.2845 - accuracy: 0.0999
on_train_batch_begin: 1615691851.332485s

13 step training time: 0.206845s

on_train_batch_end: 1615691851.539566s

28672/50000 [================>.............] - ETA: 2s - loss: 2.2639 - accuracy: 0.0999
on_train_batch_begin: 1615691851.539855s

14 step training time: 0.207370s

on_train_batch_end: 1615691851.748545s

30720/50000 [=================>............] - ETA: 1s - loss: 2.2347 - accuracy: 0.0999
on_train_batch_begin: 1615691851.748833s

15 step training time: 0.208978s

on_train_batch_end: 1615691851.957309s

32768/50000 [==================>...........] - ETA: 1s - loss: 2.2126 - accuracy: 0.0999
on_train_batch_begin: 1615691851.957597s

16 step training time: 0.208764s

on_train_batch_end: 1615691852.164751s

34816/50000 [===================>..........] - ETA: 1s - loss: 2.1903 - accuracy: 0.1000
on_train_batch_begin: 1615691852.165066s

17 step training time: 0.207469s

on_train_batch_end: 1615691852.372874s

36864/50000 [=====================>........] - ETA: 1s - loss: 2.1650 - accuracy: 0.1000
on_train_batch_begin: 1615691852.373156s

18 step training time: 0.208090s

on_train_batch_end: 1615691852.579641s

38912/50000 [======================>.......] - ETA: 1s - loss: 2.1448 - accuracy: 0.1000
on_train_batch_begin: 1615691852.579931s

19 step training time: 0.206776s

on_train_batch_end: 1615691852.789458s

40960/50000 [=======================>......] - ETA: 0s - loss: 2.1270 - accuracy: 0.1000
on_train_batch_begin: 1615691852.789745s

20 step training time: 0.209814s

on_train_batch_end: 1615691852.996675s

43008/50000 [========================>.....] - ETA: 0s - loss: 2.1117 - accuracy: 0.1000
on_train_batch_begin: 1615691852.996964s

21 step training time: 0.207219s

on_train_batch_end: 1615691853.204546s

45056/50000 [==========================>...] - ETA: 0s - loss: 2.1022 - accuracy: 0.1000
on_train_batch_begin: 1615691853.204839s

22 step training time: 0.207875s

on_train_batch_end: 1615691853.412270s

47104/50000 [===========================>..] - ETA: 0s - loss: 2.0875 - accuracy: 0.1001
on_train_batch_begin: 1615691853.412561s

23 step training time: 0.207721s

on_train_batch_end: 1615691853.617816s

49152/50000 [============================>.] - ETA: 0s - loss: 2.0652 - accuracy: 0.1001
on_train_batch_begin: 1615691853.618135s

24 step training time: 0.205574s

on_train_batch_end: 1615691853.739645s

on_test_batch_begin: 1615691853.829616s

25 step training time: 0.211481s

on_epoch_end: 1615691854.125175s

Validation time: 0.295545s

Real time: 1615691854.125175s

Epoch time: 5.510606050491333s

50000/50000 [==============================] - 6s 110us/sample - loss: 2.0593 - accuracy: 0.1001 - val_loss: 7.5400 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615691854.125355s

Real time: 1615691854.1253598
Epoch 5/5

on_train_batch_begin: 1615691854.129824s

on_train_batch_end: 1615691854.338229s

 2048/50000 [>.............................] - ETA: 4s - loss: 1.4739 - accuracy: 0.1008
on_train_batch_begin: 1615691854.338516s

1 step training time: 0.208693s

on_train_batch_end: 1615691854.548201s

 4096/50000 [=>............................] - ETA: 4s - loss: 1.4843 - accuracy: 0.1003
on_train_batch_begin: 1615691854.548492s

2 step training time: 0.209976s

on_train_batch_end: 1615691854.755772s

 6144/50000 [==>...........................] - ETA: 4s - loss: 1.4586 - accuracy: 0.1005
on_train_batch_begin: 1615691854.756058s

3 step training time: 0.207566s

on_train_batch_end: 1615691854.963854s

 8192/50000 [===>..........................] - ETA: 4s - loss: 1.4693 - accuracy: 0.1005
on_train_batch_begin: 1615691854.964141s

4 step training time: 0.208083s

on_train_batch_end: 1615691855.172409s

10240/50000 [=====>........................] - ETA: 4s - loss: 1.4411 - accuracy: 0.1006
on_train_batch_begin: 1615691855.172708s

5 step training time: 0.208567s

on_train_batch_end: 1615691855.381421s

12288/50000 [======>.......................] - ETA: 3s - loss: 1.4339 - accuracy: 0.1006
on_train_batch_begin: 1615691855.381712s

6 step training time: 0.209004s

on_train_batch_end: 1615691855.590208s

14336/50000 [=======>......................] - ETA: 3s - loss: 1.4096 - accuracy: 0.1007
on_train_batch_begin: 1615691855.590499s

7 step training time: 0.208787s

on_train_batch_end: 1615691855.798789s

16384/50000 [========>.....................] - ETA: 3s - loss: 1.4094 - accuracy: 0.1007
on_train_batch_begin: 1615691855.799081s

8 step training time: 0.208582s

on_train_batch_end: 1615691856.007019s

18432/50000 [==========>...................] - ETA: 3s - loss: 1.3988 - accuracy: 0.1007
on_train_batch_begin: 1615691856.007304s

9 step training time: 0.208224s

on_train_batch_end: 1615691856.219630s

20480/50000 [===========>..................] - ETA: 3s - loss: 1.4048 - accuracy: 0.1007
on_train_batch_begin: 1615691856.219920s

10 step training time: 0.212616s

on_train_batch_end: 1615691856.428025s

22528/50000 [============>.................] - ETA: 2s - loss: 1.4054 - accuracy: 0.1007
on_train_batch_begin: 1615691856.428326s

11 step training time: 0.208405s

on_train_batch_end: 1615691856.639034s

24576/50000 [=============>................] - ETA: 2s - loss: 1.4026 - accuracy: 0.1007
on_train_batch_begin: 1615691856.639321s

12 step training time: 0.210995s

on_train_batch_end: 1615691856.849049s

26624/50000 [==============>...............] - ETA: 2s - loss: 1.3988 - accuracy: 0.1007
on_train_batch_begin: 1615691856.849337s

13 step training time: 0.210017s

on_train_batch_end: 1615691857.058384s

28672/50000 [================>.............] - ETA: 2s - loss: 1.3975 - accuracy: 0.1007
on_train_batch_begin: 1615691857.058686s

14 step training time: 0.209349s

on_train_batch_end: 1615691857.269461s

30720/50000 [=================>............] - ETA: 1s - loss: 1.3910 - accuracy: 0.1007
on_train_batch_begin: 1615691857.269744s

15 step training time: 0.211058s

on_train_batch_end: 1615691857.479465s

32768/50000 [==================>...........] - ETA: 1s - loss: 1.3854 - accuracy: 0.1007
on_train_batch_begin: 1615691857.479754s

16 step training time: 0.210010s

on_train_batch_end: 1615691857.687322s

34816/50000 [===================>..........] - ETA: 1s - loss: 1.3847 - accuracy: 0.1007
on_train_batch_begin: 1615691857.687610s

17 step training time: 0.207856s

on_train_batch_end: 1615691857.897234s

36864/50000 [=====================>........] - ETA: 1s - loss: 1.3770 - accuracy: 0.1007
on_train_batch_begin: 1615691857.897523s

18 step training time: 0.209913s

on_train_batch_end: 1615691858.106213s

38912/50000 [======================>.......] - ETA: 1s - loss: 1.3666 - accuracy: 0.1007
on_train_batch_begin: 1615691858.106499s

19 step training time: 0.208975s

on_train_batch_end: 1615691858.314876s

40960/50000 [=======================>......] - ETA: 0s - loss: 1.3642 - accuracy: 0.1007
on_train_batch_begin: 1615691858.315169s

20 step training time: 0.208670s

on_train_batch_end: 1615691858.522508s

43008/50000 [========================>.....] - ETA: 0s - loss: 1.3572 - accuracy: 0.1007
on_train_batch_begin: 1615691858.522808s

21 step training time: 0.207639s

on_train_batch_end: 1615691858.730371s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.3507 - accuracy: 0.1007
on_train_batch_begin: 1615691858.730671s

22 step training time: 0.207863s

on_train_batch_end: 1615691858.940332s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.3426 - accuracy: 0.1007
on_train_batch_begin: 1615691858.940623s

23 step training time: 0.209952s

on_train_batch_end: 1615691859.147032s

49152/50000 [============================>.] - ETA: 0s - loss: 1.3335 - accuracy: 0.1007
on_train_batch_begin: 1615691859.147329s

24 step training time: 0.206706s

on_train_batch_end: 1615691859.270994s

on_test_batch_begin: 1615691859.364224s

25 step training time: 0.216896s

on_epoch_end: 1615691859.660816s

Validation time: 0.296578s

Real time: 1615691859.660816s

Epoch time: 5.535471200942993s

50000/50000 [==============================] - 6s 111us/sample - loss: 1.3340 - accuracy: 0.1007 - val_loss: 7.2368 - val_accuracy: 0.1001
Tempo do fit: 86.9307324886322