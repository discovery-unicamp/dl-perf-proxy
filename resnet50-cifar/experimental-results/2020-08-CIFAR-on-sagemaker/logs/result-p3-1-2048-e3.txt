wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:46
   172032/170498071 [..............................] - ETA: 1:11
   811008/170498071 [..............................] - ETA: 29s 
  3350528/170498071 [..............................] - ETA: 9s 
  6594560/170498071 [>.............................] - ETA: 6s
  9838592/170498071 [>.............................] - ETA: 4s
 13099008/170498071 [=>............................] - ETA: 4s
 15867904/170498071 [=>............................] - ETA: 3s
 19636224/170498071 [==>...........................] - ETA: 3s
 22831104/170498071 [===>..........................] - ETA: 3s
 26058752/170498071 [===>..........................] - ETA: 3s
 29073408/170498071 [====>.........................] - ETA: 2s
 32333824/170498071 [====>.........................] - ETA: 2s
 35528704/170498071 [=====>........................] - ETA: 2s
 38772736/170498071 [=====>........................] - ETA: 2s
 42000384/170498071 [======>.......................] - ETA: 2s
 45228032/170498071 [======>.......................] - ETA: 2s
 48422912/170498071 [=======>......................] - ETA: 2s
 51617792/170498071 [========>.....................] - ETA: 2s
 54779904/170498071 [========>.....................] - ETA: 2s
 57974784/170498071 [=========>....................] - ETA: 2s
 61153280/170498071 [=========>....................] - ETA: 1s
 64364544/170498071 [==========>...................] - ETA: 1s
 67575808/170498071 [==========>...................] - ETA: 1s
 70819840/170498071 [===========>..................] - ETA: 1s
 74047488/170498071 [============>.................] - ETA: 1s
 77291520/170498071 [============>.................] - ETA: 1s
 80535552/170498071 [=============>................] - ETA: 1s
 83746816/170498071 [=============>................] - ETA: 1s
 86958080/170498071 [==============>...............] - ETA: 1s
 90202112/170498071 [==============>...............] - ETA: 1s
 93429760/170498071 [===============>..............] - ETA: 1s
 96624640/170498071 [================>.............] - ETA: 1s
 99852288/170498071 [================>.............] - ETA: 1s
103112704/170498071 [=================>............] - ETA: 1s
106340352/170498071 [=================>............] - ETA: 1s
109502464/170498071 [==================>...........] - ETA: 1s
112697344/170498071 [==================>...........] - ETA: 0s
115875840/170498071 [===================>..........] - ETA: 0s
119070720/170498071 [===================>..........] - ETA: 0s
122265600/170498071 [====================>.........] - ETA: 0s
125509632/170498071 [=====================>........] - ETA: 0s
128753664/170498071 [=====================>........] - ETA: 0s
132014080/170498071 [======================>.......] - ETA: 0s
135208960/170498071 [======================>.......] - ETA: 0s
138436608/170498071 [=======================>......] - ETA: 0s
141664256/170498071 [=======================>......] - ETA: 0s
144908288/170498071 [========================>.....] - ETA: 0s
148152320/170498071 [=========================>....] - ETA: 0s
151363584/170498071 [=========================>....] - ETA: 0s
154566656/170498071 [==========================>...] - ETA: 0s
157753344/170498071 [==========================>...] - ETA: 0s
160972800/170498071 [===========================>..] - ETA: 0s
164192256/170498071 [===========================>..] - ETA: 0s
167354368/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 3s
 6725632/94765736 [=>............................] - ETA: 0s
 9388032/94765736 [=>............................] - ETA: 1s
16547840/94765736 [====>.........................] - ETA: 0s
20725760/94765736 [=====>........................] - ETA: 0s
25673728/94765736 [=======>......................] - ETA: 0s
28286976/94765736 [=======>......................] - ETA: 0s
29409280/94765736 [========>.....................] - ETA: 0s
36732928/94765736 [==========>...................] - ETA: 0s
38862848/94765736 [===========>..................] - ETA: 0s
46088192/94765736 [=============>................] - ETA: 0s
51716096/94765736 [===============>..............] - ETA: 0s
56598528/94765736 [================>.............] - ETA: 0s
57958400/94765736 [=================>............] - ETA: 0s
65560576/94765736 [===================>..........] - ETA: 0s
67551232/94765736 [====================>.........] - ETA: 0s
74178560/94765736 [======================>.......] - ETA: 0s
77389824/94765736 [=======================>......] - ETA: 0s
84869120/94765736 [=========================>....] - ETA: 0s
84893696/94765736 [=========================>....] - ETA: 0s
89145344/94765736 [===========================>..] - ETA: 0s
94773248/94765736 [==============================] - 1s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 15.390419244766235
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1607803973.875240s

Real time: 1607803973.8752673
Epoch 1/5

on_train_batch_begin: 1607803974.743450s

on_train_batch_end: 1607803992.535918s

 2048/50000 [>.............................] - ETA: 7:16 - loss: 17.8764 - accuracy: 2.4223e-04
on_train_batch_begin: 1607803992.536763s

1 step training time: 17.793314s

on_train_batch_end: 1607803992.753979s

 4096/50000 [=>............................] - ETA: 3:31 - loss: 15.0792 - accuracy: 3.6478e-04
on_train_batch_begin: 1607803992.754505s

2 step training time: 0.217741s

on_train_batch_end: 1607803992.974595s

 6144/50000 [==>...........................] - ETA: 2:16 - loss: 13.0411 - accuracy: 5.4280e-04
on_train_batch_begin: 1607803992.975075s

3 step training time: 0.220570s

on_train_batch_end: 1607803993.190176s

 8192/50000 [===>..........................] - ETA: 1:38 - loss: 11.8357 - accuracy: 0.0012    
on_train_batch_begin: 1607803993.190653s

4 step training time: 0.215579s

on_train_batch_end: 1607803993.408833s

10240/50000 [=====>........................] - ETA: 1:15 - loss: 11.0381 - accuracy: 0.0034
on_train_batch_begin: 1607803993.409300s

5 step training time: 0.218647s

on_train_batch_end: 1607803993.627857s

12288/50000 [======>.......................] - ETA: 1:00 - loss: 10.4866 - accuracy: 0.0070
on_train_batch_begin: 1607803993.628326s

6 step training time: 0.219026s

on_train_batch_end: 1607803993.849135s

14336/50000 [=======>......................] - ETA: 49s - loss: 10.0609 - accuracy: 0.0111 
on_train_batch_begin: 1607803993.849606s

7 step training time: 0.221279s

on_train_batch_end: 1607803994.065578s

16384/50000 [========>.....................] - ETA: 41s - loss: 9.7260 - accuracy: 0.0157 
on_train_batch_begin: 1607803994.066045s

8 step training time: 0.216439s

on_train_batch_end: 1607803994.281513s

18432/50000 [==========>...................] - ETA: 34s - loss: 9.4361 - accuracy: 0.0195
on_train_batch_begin: 1607803994.281983s

9 step training time: 0.215938s

on_train_batch_end: 1607803994.501704s

20480/50000 [===========>..................] - ETA: 29s - loss: 9.1982 - accuracy: 0.0226
on_train_batch_begin: 1607803994.502192s

10 step training time: 0.220209s

on_train_batch_end: 1607803994.718009s

22528/50000 [============>.................] - ETA: 25s - loss: 8.9946 - accuracy: 0.0258
on_train_batch_begin: 1607803994.718519s

11 step training time: 0.216327s

on_train_batch_end: 1607803994.934679s

24576/50000 [=============>................] - ETA: 21s - loss: 8.8171 - accuracy: 0.0288
on_train_batch_begin: 1607803994.935185s

12 step training time: 0.216666s

on_train_batch_end: 1607803995.156013s

26624/50000 [==============>...............] - ETA: 18s - loss: 8.6537 - accuracy: 0.0303
on_train_batch_begin: 1607803995.156482s

13 step training time: 0.221296s

on_train_batch_end: 1607803995.373148s

28672/50000 [================>.............] - ETA: 15s - loss: 8.5166 - accuracy: 0.0335
on_train_batch_begin: 1607803995.373620s

14 step training time: 0.217138s

on_train_batch_end: 1607803995.587908s

30720/50000 [=================>............] - ETA: 13s - loss: 8.3902 - accuracy: 0.0364
on_train_batch_begin: 1607803995.588372s

15 step training time: 0.214752s

on_train_batch_end: 1607803995.807394s

32768/50000 [==================>...........] - ETA: 11s - loss: 8.2755 - accuracy: 0.0387
on_train_batch_begin: 1607803995.807891s

16 step training time: 0.219519s

on_train_batch_end: 1607803996.023062s

34816/50000 [===================>..........] - ETA: 9s - loss: 8.1765 - accuracy: 0.0410 
on_train_batch_begin: 1607803996.023682s

17 step training time: 0.215791s

on_train_batch_end: 1607803996.240455s

36864/50000 [=====================>........] - ETA: 7s - loss: 8.0829 - accuracy: 0.0432
on_train_batch_begin: 1607803996.240948s

18 step training time: 0.217265s

on_train_batch_end: 1607803996.455955s

38912/50000 [======================>.......] - ETA: 6s - loss: 8.0008 - accuracy: 0.0450
on_train_batch_begin: 1607803996.456450s

19 step training time: 0.215502s

on_train_batch_end: 1607803996.672906s

40960/50000 [=======================>......] - ETA: 5s - loss: 7.9150 - accuracy: 0.0465
on_train_batch_begin: 1607803996.673388s

20 step training time: 0.216938s

on_train_batch_end: 1607803996.893489s

43008/50000 [========================>.....] - ETA: 3s - loss: 7.8357 - accuracy: 0.0478
on_train_batch_begin: 1607803996.893958s

21 step training time: 0.220570s

on_train_batch_end: 1607803997.113794s

45056/50000 [==========================>...] - ETA: 2s - loss: 7.7666 - accuracy: 0.0491
on_train_batch_begin: 1607803997.114284s

22 step training time: 0.220326s

on_train_batch_end: 1607803997.330115s

47104/50000 [===========================>..] - ETA: 1s - loss: 7.7041 - accuracy: 0.0502
on_train_batch_begin: 1607803997.330601s

23 step training time: 0.216318s

on_train_batch_end: 1607803997.541830s

49152/50000 [============================>.] - ETA: 0s - loss: 7.6380 - accuracy: 0.0512
on_train_batch_begin: 1607803997.542339s

24 step training time: 0.211738s

on_train_batch_end: 1607803999.385581s

on_test_batch_begin: 1607803999.610539s

25 step training time: 2.068200s

on_epoch_end: 1607804003.205125s

Validation time: 3.594560s

Real time: 1607804003.205125s

Epoch time: 29.329888105392456s

50000/50000 [==============================] - 29s 587us/sample - loss: 7.6114 - accuracy: 0.0514 - val_loss: 1113360.0272 - val_accuracy: 0.0000e+00

on_epoch_begin: 1607804003.205433s

Real time: 1607804003.2054431
Epoch 2/5

on_train_batch_begin: 1607804003.209835s

on_train_batch_end: 1607804003.430453s

 2048/50000 [>.............................] - ETA: 5s - loss: 5.8482 - accuracy: 0.0752
on_train_batch_begin: 1607804003.430964s

1 step training time: 0.221129s

on_train_batch_end: 1607804003.647169s

 4096/50000 [=>............................] - ETA: 4s - loss: 5.8854 - accuracy: 0.0750
on_train_batch_begin: 1607804003.647629s

2 step training time: 0.216665s

on_train_batch_end: 1607804003.862979s

 6144/50000 [==>...........................] - ETA: 4s - loss: 5.8671 - accuracy: 0.0729
on_train_batch_begin: 1607804003.863441s

3 step training time: 0.215812s

on_train_batch_end: 1607804004.079727s

 8192/50000 [===>..........................] - ETA: 4s - loss: 5.8478 - accuracy: 0.0718
on_train_batch_begin: 1607804004.080192s

4 step training time: 0.216751s

on_train_batch_end: 1607804004.298053s

10240/50000 [=====>........................] - ETA: 4s - loss: 5.8012 - accuracy: 0.0724
on_train_batch_begin: 1607804004.298571s

5 step training time: 0.218379s

on_train_batch_end: 1607804004.516824s

12288/50000 [======>.......................] - ETA: 4s - loss: 5.7495 - accuracy: 0.0721
on_train_batch_begin: 1607804004.517314s

6 step training time: 0.218743s

on_train_batch_end: 1607804004.734161s

14336/50000 [=======>......................] - ETA: 3s - loss: 5.7169 - accuracy: 0.0724
on_train_batch_begin: 1607804004.734621s

7 step training time: 0.217307s

on_train_batch_end: 1607804004.952449s

16384/50000 [========>.....................] - ETA: 3s - loss: 5.6707 - accuracy: 0.0735
on_train_batch_begin: 1607804004.952937s

8 step training time: 0.218316s

on_train_batch_end: 1607804005.174304s

18432/50000 [==========>...................] - ETA: 3s - loss: 5.6406 - accuracy: 0.0749
on_train_batch_begin: 1607804005.174770s

9 step training time: 0.221833s

on_train_batch_end: 1607804005.390365s

20480/50000 [===========>..................] - ETA: 3s - loss: 5.6012 - accuracy: 0.0749
on_train_batch_begin: 1607804005.390861s

10 step training time: 0.216091s

on_train_batch_end: 1607804005.606169s

22528/50000 [============>.................] - ETA: 2s - loss: 5.5618 - accuracy: 0.0758
on_train_batch_begin: 1607804005.606640s

11 step training time: 0.215780s

on_train_batch_end: 1607804005.826854s

24576/50000 [=============>................] - ETA: 2s - loss: 5.5530 - accuracy: 0.0757
on_train_batch_begin: 1607804005.827361s

12 step training time: 0.220721s

on_train_batch_end: 1607804006.046943s

26624/50000 [==============>...............] - ETA: 2s - loss: 5.5126 - accuracy: 0.0767
on_train_batch_begin: 1607804006.047472s

13 step training time: 0.220111s

on_train_batch_end: 1607804006.266832s

28672/50000 [================>.............] - ETA: 2s - loss: 5.4839 - accuracy: 0.0775
on_train_batch_begin: 1607804006.267377s

14 step training time: 0.219905s

on_train_batch_end: 1607804006.484978s

30720/50000 [=================>............] - ETA: 2s - loss: 5.4506 - accuracy: 0.0780
on_train_batch_begin: 1607804006.485444s

15 step training time: 0.218067s

on_train_batch_end: 1607804006.701647s

32768/50000 [==================>...........] - ETA: 1s - loss: 5.4232 - accuracy: 0.0778
on_train_batch_begin: 1607804006.702107s

16 step training time: 0.216663s

on_train_batch_end: 1607804006.919705s

34816/50000 [===================>..........] - ETA: 1s - loss: 5.4004 - accuracy: 0.0774
on_train_batch_begin: 1607804006.920193s

17 step training time: 0.218086s

on_train_batch_end: 1607804007.139841s

36864/50000 [=====================>........] - ETA: 1s - loss: 5.3737 - accuracy: 0.0769
on_train_batch_begin: 1607804007.140323s

18 step training time: 0.220131s

on_train_batch_end: 1607804007.361112s

38912/50000 [======================>.......] - ETA: 1s - loss: 5.3505 - accuracy: 0.0763
on_train_batch_begin: 1607804007.361597s

19 step training time: 0.221273s

on_train_batch_end: 1607804007.581141s

40960/50000 [=======================>......] - ETA: 0s - loss: 5.3216 - accuracy: 0.0759
on_train_batch_begin: 1607804007.581609s

20 step training time: 0.220012s

on_train_batch_end: 1607804007.799455s

43008/50000 [========================>.....] - ETA: 0s - loss: 5.2977 - accuracy: 0.0756
on_train_batch_begin: 1607804007.799913s

21 step training time: 0.218304s

on_train_batch_end: 1607804008.019902s

45056/50000 [==========================>...] - ETA: 0s - loss: 5.2702 - accuracy: 0.0755
on_train_batch_begin: 1607804008.020367s

22 step training time: 0.220454s

on_train_batch_end: 1607804008.236011s

47104/50000 [===========================>..] - ETA: 0s - loss: 5.2481 - accuracy: 0.0755
on_train_batch_begin: 1607804008.236483s

23 step training time: 0.216116s

on_train_batch_end: 1607804008.451290s

49152/50000 [============================>.] - ETA: 0s - loss: 5.2284 - accuracy: 0.0753
on_train_batch_begin: 1607804008.451759s

24 step training time: 0.215276s

on_train_batch_end: 1607804008.562359s

on_test_batch_begin: 1607804008.594999s

25 step training time: 0.143239s

on_epoch_end: 1607804009.032518s

Validation time: 0.437503s

Real time: 1607804009.032518s

Epoch time: 5.827106475830078s

50000/50000 [==============================] - 6s 117us/sample - loss: 5.2192 - accuracy: 0.0753 - val_loss: 34528.1393 - val_accuracy: 0.0000e+00

on_epoch_begin: 1607804009.032831s

Real time: 1607804009.0328422
Epoch 3/5

on_train_batch_begin: 1607804009.037513s

on_train_batch_end: 1607804009.254089s

 2048/50000 [>.............................] - ETA: 5s - loss: 4.5809 - accuracy: 0.0720
on_train_batch_begin: 1607804009.254587s

1 step training time: 0.217075s

on_train_batch_end: 1607804009.472310s

 4096/50000 [=>............................] - ETA: 4s - loss: 4.6047 - accuracy: 0.0722
on_train_batch_begin: 1607804009.472798s

2 step training time: 0.218210s

on_train_batch_end: 1607804009.692138s

 6144/50000 [==>...........................] - ETA: 4s - loss: 4.5592 - accuracy: 0.0724
on_train_batch_begin: 1607804009.692600s

3 step training time: 0.219802s

on_train_batch_end: 1607804009.912750s

 8192/50000 [===>..........................] - ETA: 4s - loss: 4.5142 - accuracy: 0.0720
on_train_batch_begin: 1607804009.913232s

4 step training time: 0.220632s

on_train_batch_end: 1607804010.128521s

10240/50000 [=====>........................] - ETA: 4s - loss: 4.4859 - accuracy: 0.0722
on_train_batch_begin: 1607804010.129002s

5 step training time: 0.215770s

on_train_batch_end: 1607804010.346171s

12288/50000 [======>.......................] - ETA: 4s - loss: 4.4717 - accuracy: 0.0724
on_train_batch_begin: 1607804010.346666s

6 step training time: 0.217664s

on_train_batch_end: 1607804010.562047s

14336/50000 [=======>......................] - ETA: 3s - loss: 4.4374 - accuracy: 0.0723
on_train_batch_begin: 1607804010.562557s

7 step training time: 0.215891s

on_train_batch_end: 1607804010.784512s

16384/50000 [========>.....................] - ETA: 3s - loss: 4.4406 - accuracy: 0.0723
on_train_batch_begin: 1607804010.784992s

8 step training time: 0.222435s

on_train_batch_end: 1607804011.000735s

18432/50000 [==========>...................] - ETA: 3s - loss: 4.4254 - accuracy: 0.0724
on_train_batch_begin: 1607804011.001245s

9 step training time: 0.216252s

on_train_batch_end: 1607804011.223785s

20480/50000 [===========>..................] - ETA: 3s - loss: 4.4156 - accuracy: 0.0723
on_train_batch_begin: 1607804011.224484s

10 step training time: 0.223239s

on_train_batch_end: 1607804011.440210s

22528/50000 [============>.................] - ETA: 2s - loss: 4.4088 - accuracy: 0.0723
on_train_batch_begin: 1607804011.440690s

11 step training time: 0.216206s

on_train_batch_end: 1607804011.660392s

24576/50000 [=============>................] - ETA: 2s - loss: 4.3975 - accuracy: 0.0723
on_train_batch_begin: 1607804011.660861s

12 step training time: 0.220172s

on_train_batch_end: 1607804011.875781s

26624/50000 [==============>...............] - ETA: 2s - loss: 4.3821 - accuracy: 0.0723
on_train_batch_begin: 1607804011.876240s

13 step training time: 0.215379s

on_train_batch_end: 1607804012.095865s

28672/50000 [================>.............] - ETA: 2s - loss: 4.3671 - accuracy: 0.0724
on_train_batch_begin: 1607804012.096332s

14 step training time: 0.220091s

on_train_batch_end: 1607804012.310712s

30720/50000 [=================>............] - ETA: 2s - loss: 4.3586 - accuracy: 0.0725
on_train_batch_begin: 1607804012.311191s

15 step training time: 0.214859s

on_train_batch_end: 1607804012.530683s

32768/50000 [==================>...........] - ETA: 1s - loss: 4.3358 - accuracy: 0.0727
on_train_batch_begin: 1607804012.531165s

16 step training time: 0.219974s

on_train_batch_end: 1607804012.746863s

34816/50000 [===================>..........] - ETA: 1s - loss: 4.3219 - accuracy: 0.0728
on_train_batch_begin: 1607804012.747339s

17 step training time: 0.216174s

on_train_batch_end: 1607804012.962734s

36864/50000 [=====================>........] - ETA: 1s - loss: 4.3042 - accuracy: 0.0729
on_train_batch_begin: 1607804012.963206s

18 step training time: 0.215867s

on_train_batch_end: 1607804013.180267s

38912/50000 [======================>.......] - ETA: 1s - loss: 4.2849 - accuracy: 0.0731
on_train_batch_begin: 1607804013.180758s

19 step training time: 0.217552s

on_train_batch_end: 1607804013.395768s

40960/50000 [=======================>......] - ETA: 0s - loss: 4.2671 - accuracy: 0.0731
on_train_batch_begin: 1607804013.396235s

20 step training time: 0.215477s

on_train_batch_end: 1607804013.611923s

43008/50000 [========================>.....] - ETA: 0s - loss: 4.2542 - accuracy: 0.0731
on_train_batch_begin: 1607804013.612390s

21 step training time: 0.216155s

on_train_batch_end: 1607804013.827470s

45056/50000 [==========================>...] - ETA: 0s - loss: 4.2420 - accuracy: 0.0731
on_train_batch_begin: 1607804013.827958s

22 step training time: 0.215568s

on_train_batch_end: 1607804014.043871s

47104/50000 [===========================>..] - ETA: 0s - loss: 4.2212 - accuracy: 0.0733
on_train_batch_begin: 1607804014.044374s

23 step training time: 0.216416s

on_train_batch_end: 1607804014.259485s

49152/50000 [============================>.] - ETA: 0s - loss: 4.2022 - accuracy: 0.0733
on_train_batch_begin: 1607804014.259953s

24 step training time: 0.215579s

on_train_batch_end: 1607804014.372122s

on_test_batch_begin: 1607804014.404106s

25 step training time: 0.144153s

on_epoch_end: 1607804014.847318s

Validation time: 0.443196s

Real time: 1607804014.847318s

Epoch time: 5.814507007598877s

50000/50000 [==============================] - 6s 116us/sample - loss: 4.1993 - accuracy: 0.0733 - val_loss: 8.5052 - val_accuracy: 0.0130

on_epoch_begin: 1607804014.847619s

Real time: 1607804014.8476303
Epoch 4/5

on_train_batch_begin: 1607804014.852236s

on_train_batch_end: 1607804015.072595s

 2048/50000 [>.............................] - ETA: 5s - loss: 3.6400 - accuracy: 0.0794
on_train_batch_begin: 1607804015.073136s

1 step training time: 0.220900s

on_train_batch_end: 1607804015.294704s

 4096/50000 [=>............................] - ETA: 5s - loss: 3.6445 - accuracy: 0.0788
on_train_batch_begin: 1607804015.295166s

2 step training time: 0.222030s

on_train_batch_end: 1607804015.513540s

 6144/50000 [==>...........................] - ETA: 4s - loss: 3.5404 - accuracy: 0.0802
on_train_batch_begin: 1607804015.514022s

3 step training time: 0.218856s

on_train_batch_end: 1607804015.729339s

 8192/50000 [===>..........................] - ETA: 4s - loss: 3.4969 - accuracy: 0.0826
on_train_batch_begin: 1607804015.729801s

4 step training time: 0.215779s

on_train_batch_end: 1607804015.945544s

10240/50000 [=====>........................] - ETA: 4s - loss: 3.4758 - accuracy: 0.0836
on_train_batch_begin: 1607804015.946027s

5 step training time: 0.216226s

on_train_batch_end: 1607804016.163932s

12288/50000 [======>.......................] - ETA: 4s - loss: 3.4489 - accuracy: 0.0847
on_train_batch_begin: 1607804016.164398s

6 step training time: 0.218371s

on_train_batch_end: 1607804016.379104s

14336/50000 [=======>......................] - ETA: 3s - loss: 3.4264 - accuracy: 0.0853
on_train_batch_begin: 1607804016.379569s

7 step training time: 0.215171s

on_train_batch_end: 1607804016.596781s

16384/50000 [========>.....................] - ETA: 3s - loss: 3.4216 - accuracy: 0.0856
on_train_batch_begin: 1607804016.597292s

8 step training time: 0.217723s

on_train_batch_end: 1607804016.812387s

18432/50000 [==========>...................] - ETA: 3s - loss: 3.3969 - accuracy: 0.0862
on_train_batch_begin: 1607804016.812847s

9 step training time: 0.215555s

on_train_batch_end: 1607804017.034180s

20480/50000 [===========>..................] - ETA: 3s - loss: 3.3806 - accuracy: 0.0865
on_train_batch_begin: 1607804017.034667s

10 step training time: 0.221819s

on_train_batch_end: 1607804017.252318s

22528/50000 [============>.................] - ETA: 2s - loss: 3.3636 - accuracy: 0.0868
on_train_batch_begin: 1607804017.252782s

11 step training time: 0.218115s

on_train_batch_end: 1607804017.472154s

24576/50000 [=============>................] - ETA: 2s - loss: 3.3383 - accuracy: 0.0871
on_train_batch_begin: 1607804017.472641s

12 step training time: 0.219860s

on_train_batch_end: 1607804017.692152s

26624/50000 [==============>...............] - ETA: 2s - loss: 3.3181 - accuracy: 0.0875
on_train_batch_begin: 1607804017.692613s

13 step training time: 0.219971s

on_train_batch_end: 1607804017.912472s

28672/50000 [================>.............] - ETA: 2s - loss: 3.2951 - accuracy: 0.0879
on_train_batch_begin: 1607804017.912937s

14 step training time: 0.220325s

on_train_batch_end: 1607804018.126557s

30720/50000 [=================>............] - ETA: 2s - loss: 3.2849 - accuracy: 0.0881
on_train_batch_begin: 1607804018.127023s

15 step training time: 0.214086s

on_train_batch_end: 1607804018.348348s

32768/50000 [==================>...........] - ETA: 1s - loss: 3.2588 - accuracy: 0.0883
on_train_batch_begin: 1607804018.348834s

16 step training time: 0.221811s

on_train_batch_end: 1607804018.566420s

34816/50000 [===================>..........] - ETA: 1s - loss: 3.2401 - accuracy: 0.0886
on_train_batch_begin: 1607804018.566927s

17 step training time: 0.218093s

on_train_batch_end: 1607804018.782114s

36864/50000 [=====================>........] - ETA: 1s - loss: 3.2143 - accuracy: 0.0890
on_train_batch_begin: 1607804018.782623s

18 step training time: 0.215696s

on_train_batch_end: 1607804019.000548s

38912/50000 [======================>.......] - ETA: 1s - loss: 3.1852 - accuracy: 0.0894
on_train_batch_begin: 1607804019.001036s

19 step training time: 0.218414s

on_train_batch_end: 1607804019.221779s

40960/50000 [=======================>......] - ETA: 0s - loss: 3.1615 - accuracy: 0.0898
on_train_batch_begin: 1607804019.222319s

20 step training time: 0.221283s

on_train_batch_end: 1607804019.440919s

43008/50000 [========================>.....] - ETA: 0s - loss: 3.1326 - accuracy: 0.0902
on_train_batch_begin: 1607804019.441406s

21 step training time: 0.219087s

on_train_batch_end: 1607804019.658255s

45056/50000 [==========================>...] - ETA: 0s - loss: 3.0934 - accuracy: 0.0906
on_train_batch_begin: 1607804019.658743s

22 step training time: 0.217337s

on_train_batch_end: 1607804019.875304s

47104/50000 [===========================>..] - ETA: 0s - loss: 3.0600 - accuracy: 0.0910
on_train_batch_begin: 1607804019.875786s

23 step training time: 0.217043s

on_train_batch_end: 1607804020.087829s

49152/50000 [============================>.] - ETA: 0s - loss: 3.0315 - accuracy: 0.0913
on_train_batch_begin: 1607804020.088318s

24 step training time: 0.212532s

on_train_batch_end: 1607804020.198051s

on_test_batch_begin: 1607804020.231304s

25 step training time: 0.142986s

on_epoch_end: 1607804020.672746s

Validation time: 0.441424s

Real time: 1607804020.672746s

Epoch time: 5.825157642364502s

50000/50000 [==============================] - 6s 117us/sample - loss: 3.0222 - accuracy: 0.0913 - val_loss: 22.8493 - val_accuracy: 0.0000e+00

on_epoch_begin: 1607804020.673073s

Real time: 1607804020.6730835
Epoch 5/5

on_train_batch_begin: 1607804020.677751s

on_train_batch_end: 1607804020.893934s

 2048/50000 [>.............................] - ETA: 5s - loss: 2.0772 - accuracy: 0.0994
on_train_batch_begin: 1607804020.894483s

1 step training time: 0.216732s

on_train_batch_end: 1607804021.109660s

 4096/50000 [=>............................] - ETA: 4s - loss: 2.1355 - accuracy: 0.0989
on_train_batch_begin: 1607804021.110168s

2 step training time: 0.215684s

on_train_batch_end: 1607804021.328149s

 6144/50000 [==>...........................] - ETA: 4s - loss: 2.1043 - accuracy: 0.0991
on_train_batch_begin: 1607804021.328629s

3 step training time: 0.218461s

on_train_batch_end: 1607804021.542459s

 8192/50000 [===>..........................] - ETA: 4s - loss: 2.0939 - accuracy: 0.0993
on_train_batch_begin: 1607804021.542937s

4 step training time: 0.214308s

on_train_batch_end: 1607804021.758350s

10240/50000 [=====>........................] - ETA: 4s - loss: 2.1032 - accuracy: 0.0994
on_train_batch_begin: 1607804021.758825s

5 step training time: 0.215888s

on_train_batch_end: 1607804021.974150s

12288/50000 [======>.......................] - ETA: 3s - loss: 2.0948 - accuracy: 0.0994
on_train_batch_begin: 1607804021.974630s

6 step training time: 0.215806s

on_train_batch_end: 1607804022.195672s

14336/50000 [=======>......................] - ETA: 3s - loss: 2.0898 - accuracy: 0.0995
on_train_batch_begin: 1607804022.196136s

7 step training time: 0.221505s

on_train_batch_end: 1607804022.413054s

16384/50000 [========>.....................] - ETA: 3s - loss: 2.0796 - accuracy: 0.0996
on_train_batch_begin: 1607804022.413516s

8 step training time: 0.217381s

on_train_batch_end: 1607804022.628918s

18432/50000 [==========>...................] - ETA: 3s - loss: 2.0809 - accuracy: 0.0996
on_train_batch_begin: 1607804022.629428s

9 step training time: 0.215912s

on_train_batch_end: 1607804022.844631s

20480/50000 [===========>..................] - ETA: 3s - loss: 2.0622 - accuracy: 0.0996
on_train_batch_begin: 1607804022.845100s

10 step training time: 0.215672s

on_train_batch_end: 1607804023.061381s

22528/50000 [============>.................] - ETA: 2s - loss: 2.0646 - accuracy: 0.0996
on_train_batch_begin: 1607804023.061848s

11 step training time: 0.216748s

on_train_batch_end: 1607804023.279459s

24576/50000 [=============>................] - ETA: 2s - loss: 2.0674 - accuracy: 0.0995
on_train_batch_begin: 1607804023.279957s

12 step training time: 0.218108s

on_train_batch_end: 1607804023.499434s

26624/50000 [==============>...............] - ETA: 2s - loss: 2.0614 - accuracy: 0.0995
on_train_batch_begin: 1607804023.499919s

13 step training time: 0.219962s

on_train_batch_end: 1607804023.719688s

28672/50000 [================>.............] - ETA: 2s - loss: 2.0513 - accuracy: 0.0996
on_train_batch_begin: 1607804023.720148s

14 step training time: 0.220230s

on_train_batch_end: 1607804023.936023s

30720/50000 [=================>............] - ETA: 2s - loss: 2.0392 - accuracy: 0.0996
on_train_batch_begin: 1607804023.936507s

15 step training time: 0.216359s

on_train_batch_end: 1607804024.153222s

32768/50000 [==================>...........] - ETA: 1s - loss: 2.0283 - accuracy: 0.0996
on_train_batch_begin: 1607804024.153708s

16 step training time: 0.217201s

on_train_batch_end: 1607804024.374567s

34816/50000 [===================>..........] - ETA: 1s - loss: 2.0199 - accuracy: 0.0997
on_train_batch_begin: 1607804024.375048s

17 step training time: 0.221340s

on_train_batch_end: 1607804024.591077s

36864/50000 [=====================>........] - ETA: 1s - loss: 2.0114 - accuracy: 0.0997
on_train_batch_begin: 1607804024.591572s

18 step training time: 0.216524s

on_train_batch_end: 1607804024.812834s

38912/50000 [======================>.......] - ETA: 1s - loss: 2.0042 - accuracy: 0.0997
on_train_batch_begin: 1607804024.813299s

19 step training time: 0.221727s

on_train_batch_end: 1607804025.031204s

40960/50000 [=======================>......] - ETA: 0s - loss: 2.0055 - accuracy: 0.0997
on_train_batch_begin: 1607804025.031681s

20 step training time: 0.218382s

on_train_batch_end: 1607804025.249195s

43008/50000 [========================>.....] - ETA: 0s - loss: 1.9986 - accuracy: 0.0997
on_train_batch_begin: 1607804025.249662s

21 step training time: 0.217981s

on_train_batch_end: 1607804025.465873s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.9905 - accuracy: 0.0997
on_train_batch_begin: 1607804025.466382s

22 step training time: 0.216721s

on_train_batch_end: 1607804025.683241s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.9839 - accuracy: 0.0997
on_train_batch_begin: 1607804025.683721s

23 step training time: 0.217339s

on_train_batch_end: 1607804025.894724s

49152/50000 [============================>.] - ETA: 0s - loss: 1.9735 - accuracy: 0.0997
on_train_batch_begin: 1607804025.895208s

24 step training time: 0.211486s

on_train_batch_end: 1607804026.004846s

on_test_batch_begin: 1607804026.038183s

25 step training time: 0.142976s

on_epoch_end: 1607804026.469739s

Validation time: 0.431539s

Real time: 1607804026.469739s

Epoch time: 5.796684741973877s

50000/50000 [==============================] - 6s 116us/sample - loss: 1.9727 - accuracy: 0.0997 - val_loss: 7.2366 - val_accuracy: 0.0998
Tempo do fit: 56.52603316307068