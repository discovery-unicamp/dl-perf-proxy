wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 8:03
   188416/170498071 [..............................] - ETA: 1:06
   811008/170498071 [..............................] - ETA: 25s 
  2334720/170498071 [..............................] - ETA: 12s
  5545984/170498071 [..............................] - ETA: 6s 
  8757248/170498071 [>.............................] - ETA: 5s
 11968512/170498071 [=>............................] - ETA: 4s
 15179776/170498071 [=>............................] - ETA: 3s
 18391040/170498071 [==>...........................] - ETA: 3s
 21594112/170498071 [==>...........................] - ETA: 3s
 24788992/170498071 [===>..........................] - ETA: 3s
 27992064/170498071 [===>..........................] - ETA: 2s
 31203328/170498071 [====>.........................] - ETA: 2s
 34430976/170498071 [=====>........................] - ETA: 2s
 37609472/170498071 [=====>........................] - ETA: 2s
 40812544/170498071 [======>.......................] - ETA: 2s
 43982848/170498071 [======>.......................] - ETA: 2s
 47177728/170498071 [=======>......................] - ETA: 2s
 50307072/170498071 [=======>......................] - ETA: 2s
 53485568/170498071 [========>.....................] - ETA: 2s
 56664064/170498071 [========>.....................] - ETA: 2s
 59858944/170498071 [=========>....................] - ETA: 1s
 63053824/170498071 [==========>...................] - ETA: 1s
 66248704/170498071 [==========>...................] - ETA: 1s
 69459968/170498071 [===========>..................] - ETA: 1s
 72646656/170498071 [===========>..................] - ETA: 1s
 75833344/170498071 [============>.................] - ETA: 1s
 79044608/170498071 [============>.................] - ETA: 1s
 82239488/170498071 [=============>................] - ETA: 1s
 85450752/170498071 [==============>...............] - ETA: 1s
 88645632/170498071 [==============>...............] - ETA: 1s
 91865088/170498071 [===============>..............] - ETA: 1s
 95068160/170498071 [===============>..............] - ETA: 1s
 98271232/170498071 [================>.............] - ETA: 1s
101457920/170498071 [================>.............] - ETA: 1s
104685568/170498071 [=================>............] - ETA: 1s
107880448/170498071 [=================>............] - ETA: 1s
111108096/170498071 [==================>...........] - ETA: 1s
114253824/170498071 [===================>..........] - ETA: 0s
117448704/170498071 [===================>..........] - ETA: 0s
120643584/170498071 [====================>.........] - ETA: 0s
123822080/170498071 [====================>.........] - ETA: 0s
126951424/170498071 [=====================>........] - ETA: 0s
130146304/170498071 [=====================>........] - ETA: 0s
133341184/170498071 [======================>.......] - ETA: 0s
136519680/170498071 [=======================>......] - ETA: 0s
139747328/170498071 [=======================>......] - ETA: 0s
142942208/170498071 [========================>.....] - ETA: 0s
146145280/170498071 [========================>.....] - ETA: 0s
149315584/170498071 [=========================>....] - ETA: 0s
152526848/170498071 [=========================>....] - ETA: 0s
155705344/170498071 [==========================>...] - ETA: 0s
158908416/170498071 [==========================>...] - ETA: 0s
162095104/170498071 [===========================>..] - ETA: 0s
165289984/170498071 [============================>.] - ETA: 0s
168493056/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 7s
 4317184/94765736 [>.............................] - ETA: 1s
 9388032/94765736 [=>............................] - ETA: 1s
16064512/94765736 [====>.........................] - ETA: 1s
18841600/94765736 [====>.........................] - ETA: 1s
25313280/94765736 [=======>......................] - ETA: 1s
28286976/94765736 [=======>......................] - ETA: 1s
33415168/94765736 [=========>....................] - ETA: 1s
37683200/94765736 [==========>...................] - ETA: 1s
44703744/94765736 [=============>................] - ETA: 0s
48586752/94765736 [==============>...............] - ETA: 0s
56598528/94765736 [================>.............] - ETA: 0s
61775872/94765736 [==================>...........] - ETA: 0s
65986560/94765736 [===================>..........] - ETA: 0s
69148672/94765736 [====================>.........] - ETA: 0s
75440128/94765736 [======================>.......] - ETA: 0s
77438976/94765736 [=======================>......] - ETA: 0s
79462400/94765736 [========================>.....] - ETA: 0s
82386944/94765736 [=========================>....] - ETA: 0s
84893696/94765736 [=========================>....] - ETA: 0s
87195648/94765736 [==========================>...] - ETA: 0s
91357184/94765736 [===========================>..] - ETA: 0s
94281728/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 2s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 26.378693103790283
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1609168582.468754s

Real time: 1609168582.4687772
Epoch 1/5

on_train_batch_begin: 1609168583.705987s

on_train_batch_end: 1609168726.570529s

 2048/50000 [>.............................] - ETA: 56:14 - loss: 17.2555 - accuracy: 3.0136e-04
on_train_batch_begin: 1609168726.571308s

1 step training time: 142.865320s

on_train_batch_end: 1609168726.853948s

 4096/50000 [=>............................] - ETA: 26:58 - loss: 14.4501 - accuracy: 3.0422e-04
on_train_batch_begin: 1609168726.854377s

2 step training time: 0.283069s

on_train_batch_end: 1609168727.211870s

 6144/50000 [==>...........................] - ETA: 17:13 - loss: 12.6079 - accuracy: 7.5277e-04
on_train_batch_begin: 1609168727.212280s

3 step training time: 0.357903s

on_train_batch_end: 1609168727.565657s

 8192/50000 [===>..........................] - ETA: 12:20 - loss: 11.5324 - accuracy: 0.0024    
on_train_batch_begin: 1609168727.566071s

4 step training time: 0.353791s

on_train_batch_end: 1609168727.919343s

10240/50000 [=====>........................] - ETA: 9:24 - loss: 10.8676 - accuracy: 0.0055 
on_train_batch_begin: 1609168727.919747s

5 step training time: 0.353677s

on_train_batch_end: 1609168728.274160s

12288/50000 [======>.......................] - ETA: 7:27 - loss: 10.3704 - accuracy: 0.0096
on_train_batch_begin: 1609168728.274559s

6 step training time: 0.354812s

on_train_batch_end: 1609168728.633269s

14336/50000 [=======>......................] - ETA: 6:03 - loss: 9.9974 - accuracy: 0.0130 
on_train_batch_begin: 1609168728.633672s

7 step training time: 0.359113s

on_train_batch_end: 1609168728.992226s

16384/50000 [========>.....................] - ETA: 5:00 - loss: 9.6810 - accuracy: 0.0165
on_train_batch_begin: 1609168728.992614s

8 step training time: 0.358942s

on_train_batch_end: 1609168729.347142s

18432/50000 [==========>...................] - ETA: 4:11 - loss: 9.4236 - accuracy: 0.0206
on_train_batch_begin: 1609168729.347523s

9 step training time: 0.354909s

on_train_batch_end: 1609168729.702823s

20480/50000 [===========>..................] - ETA: 3:32 - loss: 9.2084 - accuracy: 0.0246
on_train_batch_begin: 1609168729.703215s

10 step training time: 0.355692s

on_train_batch_end: 1609168730.062223s

22528/50000 [============>.................] - ETA: 2:59 - loss: 9.0293 - accuracy: 0.0280
on_train_batch_begin: 1609168730.062611s

11 step training time: 0.359396s

on_train_batch_end: 1609168730.419250s

24576/50000 [=============>................] - ETA: 2:33 - loss: 8.8646 - accuracy: 0.0312
on_train_batch_begin: 1609168730.419636s

12 step training time: 0.357025s

on_train_batch_end: 1609168730.778275s

26624/50000 [==============>...............] - ETA: 2:10 - loss: 8.7195 - accuracy: 0.0345
on_train_batch_begin: 1609168730.778665s

13 step training time: 0.359028s

on_train_batch_end: 1609168731.134797s

28672/50000 [================>.............] - ETA: 1:50 - loss: 8.5930 - accuracy: 0.0369
on_train_batch_begin: 1609168731.135184s

14 step training time: 0.356519s

on_train_batch_end: 1609168731.490516s

30720/50000 [=================>............] - ETA: 1:33 - loss: 8.4790 - accuracy: 0.0392
on_train_batch_begin: 1609168731.490932s

15 step training time: 0.355748s

on_train_batch_end: 1609168731.844377s

32768/50000 [==================>...........] - ETA: 1:18 - loss: 8.3716 - accuracy: 0.0415
on_train_batch_begin: 1609168731.844766s

16 step training time: 0.353834s

on_train_batch_end: 1609168732.200462s

34816/50000 [===================>..........] - ETA: 1:05 - loss: 8.2721 - accuracy: 0.0440
on_train_batch_begin: 1609168732.200849s

17 step training time: 0.356083s

on_train_batch_end: 1609168732.557019s

36864/50000 [=====================>........] - ETA: 53s - loss: 8.1814 - accuracy: 0.0458 
on_train_batch_begin: 1609168732.557400s

18 step training time: 0.356551s

on_train_batch_end: 1609168732.912009s

38912/50000 [======================>.......] - ETA: 42s - loss: 8.0949 - accuracy: 0.0475
on_train_batch_begin: 1609168732.912397s

19 step training time: 0.354997s

on_train_batch_end: 1609168733.268241s

40960/50000 [=======================>......] - ETA: 33s - loss: 8.0177 - accuracy: 0.0495
on_train_batch_begin: 1609168733.268622s

20 step training time: 0.356225s

on_train_batch_end: 1609168733.625724s

43008/50000 [========================>.....] - ETA: 24s - loss: 7.9415 - accuracy: 0.0508
on_train_batch_begin: 1609168733.626099s

21 step training time: 0.357477s

on_train_batch_end: 1609168733.980466s

45056/50000 [==========================>...] - ETA: 16s - loss: 7.8627 - accuracy: 0.0522
on_train_batch_begin: 1609168733.980853s

22 step training time: 0.354753s

on_train_batch_end: 1609168734.337570s

47104/50000 [===========================>..] - ETA: 9s - loss: 7.7887 - accuracy: 0.0534 
on_train_batch_begin: 1609168734.337954s

23 step training time: 0.357101s

on_train_batch_end: 1609168734.689163s

49152/50000 [============================>.] - ETA: 2s - loss: 7.7203 - accuracy: 0.0541
on_train_batch_begin: 1609168734.689543s

24 step training time: 0.351589s

on_train_batch_end: 1609168742.397923s

on_test_batch_begin: 1609168742.785166s

25 step training time: 8.095623s

on_epoch_end: 1609168760.568531s

Validation time: 17.783345s

Real time: 1609168760.568531s

Epoch time: 178.09977841377258s

50000/50000 [==============================] - 178s 4ms/sample - loss: 7.6953 - accuracy: 0.0541 - val_loss: 388038.4095 - val_accuracy: 0.0000e+00

on_epoch_begin: 1609168760.568803s

Real time: 1609168760.5688114
Epoch 2/5

on_train_batch_begin: 1609168760.576529s

on_train_batch_end: 1609168760.857236s

 2048/50000 [>.............................] - ETA: 6s - loss: 5.7970 - accuracy: 0.0683
on_train_batch_begin: 1609168760.857627s

1 step training time: 0.281099s

on_train_batch_end: 1609168761.208753s

 4096/50000 [=>............................] - ETA: 7s - loss: 5.8486 - accuracy: 0.0701
on_train_batch_begin: 1609168761.209130s

2 step training time: 0.351503s

on_train_batch_end: 1609168761.568045s

 6144/50000 [==>...........................] - ETA: 7s - loss: 5.8325 - accuracy: 0.0715
on_train_batch_begin: 1609168761.568425s

3 step training time: 0.359295s

on_train_batch_end: 1609168761.921087s

 8192/50000 [===>..........................] - ETA: 6s - loss: 5.8174 - accuracy: 0.0713
on_train_batch_begin: 1609168761.921475s

4 step training time: 0.353050s

on_train_batch_end: 1609168762.277569s

10240/50000 [=====>........................] - ETA: 6s - loss: 5.8194 - accuracy: 0.0701
on_train_batch_begin: 1609168762.277954s

5 step training time: 0.356479s

on_train_batch_end: 1609168762.635674s

12288/50000 [======>.......................] - ETA: 6s - loss: 5.7628 - accuracy: 0.0694
on_train_batch_begin: 1609168762.636056s

6 step training time: 0.358102s

on_train_batch_end: 1609168762.991505s

14336/50000 [=======>......................] - ETA: 6s - loss: 5.7134 - accuracy: 0.0689
on_train_batch_begin: 1609168762.991886s

7 step training time: 0.355830s

on_train_batch_end: 1609168763.351540s

16384/50000 [========>.....................] - ETA: 5s - loss: 5.6691 - accuracy: 0.0684
on_train_batch_begin: 1609168763.351916s

8 step training time: 0.360030s

on_train_batch_end: 1609168763.709379s

18432/50000 [==========>...................] - ETA: 5s - loss: 5.6248 - accuracy: 0.0675
on_train_batch_begin: 1609168763.709765s

9 step training time: 0.357849s

on_train_batch_end: 1609168764.065009s

20480/50000 [===========>..................] - ETA: 5s - loss: 5.5847 - accuracy: 0.0670
on_train_batch_begin: 1609168764.065394s

10 step training time: 0.355629s

on_train_batch_end: 1609168764.419252s

22528/50000 [============>.................] - ETA: 4s - loss: 5.5374 - accuracy: 0.0666
on_train_batch_begin: 1609168764.419636s

11 step training time: 0.354242s

on_train_batch_end: 1609168764.775666s

24576/50000 [=============>................] - ETA: 4s - loss: 5.4925 - accuracy: 0.0663
on_train_batch_begin: 1609168764.776045s

12 step training time: 0.356409s

on_train_batch_end: 1609168765.133677s

26624/50000 [==============>...............] - ETA: 4s - loss: 5.4514 - accuracy: 0.0663
on_train_batch_begin: 1609168765.134060s

13 step training time: 0.358014s

on_train_batch_end: 1609168765.489995s

28672/50000 [================>.............] - ETA: 3s - loss: 5.4097 - accuracy: 0.0661
on_train_batch_begin: 1609168765.490378s

14 step training time: 0.356319s

on_train_batch_end: 1609168765.849037s

30720/50000 [=================>............] - ETA: 3s - loss: 5.3693 - accuracy: 0.0660
on_train_batch_begin: 1609168765.849431s

15 step training time: 0.359052s

on_train_batch_end: 1609168766.200427s

32768/50000 [==================>...........] - ETA: 2s - loss: 5.3300 - accuracy: 0.0660
on_train_batch_begin: 1609168766.200818s

16 step training time: 0.351387s

on_train_batch_end: 1609168766.556808s

34816/50000 [===================>..........] - ETA: 2s - loss: 5.2932 - accuracy: 0.0661
on_train_batch_begin: 1609168766.557193s

17 step training time: 0.356375s

on_train_batch_end: 1609168766.916107s

36864/50000 [=====================>........] - ETA: 2s - loss: 5.2539 - accuracy: 0.0662
on_train_batch_begin: 1609168766.916494s

18 step training time: 0.359302s

on_train_batch_end: 1609168767.273723s

38912/50000 [======================>.......] - ETA: 1s - loss: 5.2174 - accuracy: 0.0664
on_train_batch_begin: 1609168767.274114s

19 step training time: 0.357619s

on_train_batch_end: 1609168767.629092s

40960/50000 [=======================>......] - ETA: 1s - loss: 5.1856 - accuracy: 0.0667
on_train_batch_begin: 1609168767.629476s

20 step training time: 0.355362s

on_train_batch_end: 1609168767.986097s

43008/50000 [========================>.....] - ETA: 1s - loss: 5.1537 - accuracy: 0.0670
on_train_batch_begin: 1609168767.986482s

21 step training time: 0.357006s

on_train_batch_end: 1609168768.345026s

45056/50000 [==========================>...] - ETA: 0s - loss: 5.1278 - accuracy: 0.0673
on_train_batch_begin: 1609168768.345412s

22 step training time: 0.358931s

on_train_batch_end: 1609168768.701893s

47104/50000 [===========================>..] - ETA: 0s - loss: 5.1007 - accuracy: 0.0675
on_train_batch_begin: 1609168768.702278s

23 step training time: 0.356866s

on_train_batch_end: 1609168769.058983s

49152/50000 [============================>.] - ETA: 0s - loss: 5.0749 - accuracy: 0.0677
on_train_batch_begin: 1609168769.059370s

24 step training time: 0.357092s

on_train_batch_end: 1609168769.295693s

on_test_batch_begin: 1609168769.389390s

25 step training time: 0.330020s

on_epoch_end: 1609168769.801507s

Validation time: 0.412097s

Real time: 1609168769.801507s

Epoch time: 9.232717514038086s

50000/50000 [==============================] - 9s 185us/sample - loss: 5.0652 - accuracy: 0.0677 - val_loss: 566.7129 - val_accuracy: 0.0000e+00

on_epoch_begin: 1609168769.801761s

Real time: 1609168769.8017707
Epoch 3/5

on_train_batch_begin: 1609168769.809626s

on_train_batch_end: 1609168770.166871s

 2048/50000 [>.............................] - ETA: 8s - loss: 4.2820 - accuracy: 0.0733
on_train_batch_begin: 1609168770.167262s

1 step training time: 0.357635s

on_train_batch_end: 1609168770.527291s

 4096/50000 [=>............................] - ETA: 8s - loss: 4.2816 - accuracy: 0.0720
on_train_batch_begin: 1609168770.527678s

2 step training time: 0.360416s

on_train_batch_end: 1609168770.885504s

 6144/50000 [==>...........................] - ETA: 7s - loss: 4.2806 - accuracy: 0.0718
on_train_batch_begin: 1609168770.885905s

3 step training time: 0.358227s

on_train_batch_end: 1609168771.244212s

 8192/50000 [===>..........................] - ETA: 7s - loss: 4.2697 - accuracy: 0.0718
on_train_batch_begin: 1609168771.244614s

4 step training time: 0.358709s

on_train_batch_end: 1609168771.593717s

10240/50000 [=====>........................] - ETA: 6s - loss: 4.2675 - accuracy: 0.0711
on_train_batch_begin: 1609168771.594116s

5 step training time: 0.349502s

on_train_batch_end: 1609168771.951309s

12288/50000 [======>.......................] - ETA: 6s - loss: 4.2477 - accuracy: 0.0716
on_train_batch_begin: 1609168771.951722s

6 step training time: 0.357606s

on_train_batch_end: 1609168772.307710s

14336/50000 [=======>......................] - ETA: 6s - loss: 4.2329 - accuracy: 0.0721
on_train_batch_begin: 1609168772.308109s

7 step training time: 0.356387s

on_train_batch_end: 1609168772.666796s

16384/50000 [========>.....................] - ETA: 5s - loss: 4.2210 - accuracy: 0.0724
on_train_batch_begin: 1609168772.667197s

8 step training time: 0.359087s

on_train_batch_end: 1609168773.025015s

18432/50000 [==========>...................] - ETA: 5s - loss: 4.1969 - accuracy: 0.0729
on_train_batch_begin: 1609168773.025410s

9 step training time: 0.358213s

on_train_batch_end: 1609168773.382019s

20480/50000 [===========>..................] - ETA: 5s - loss: 4.1861 - accuracy: 0.0731
on_train_batch_begin: 1609168773.382417s

10 step training time: 0.357008s

on_train_batch_end: 1609168773.734739s

22528/50000 [============>.................] - ETA: 4s - loss: 4.1722 - accuracy: 0.0733
on_train_batch_begin: 1609168773.735165s

11 step training time: 0.352748s

on_train_batch_end: 1609168774.094998s

24576/50000 [=============>................] - ETA: 4s - loss: 4.1505 - accuracy: 0.0733
on_train_batch_begin: 1609168774.095398s

12 step training time: 0.360233s

on_train_batch_end: 1609168774.455533s

26624/50000 [==============>...............] - ETA: 4s - loss: 4.1479 - accuracy: 0.0729
on_train_batch_begin: 1609168774.455934s

13 step training time: 0.360536s

on_train_batch_end: 1609168774.812533s

28672/50000 [================>.............] - ETA: 3s - loss: 4.1273 - accuracy: 0.0731
on_train_batch_begin: 1609168774.812935s

14 step training time: 0.357001s

on_train_batch_end: 1609168775.166831s

30720/50000 [=================>............] - ETA: 3s - loss: 4.1176 - accuracy: 0.0731
on_train_batch_begin: 1609168775.167217s

15 step training time: 0.354282s

on_train_batch_end: 1609168775.523867s

32768/50000 [==================>...........] - ETA: 3s - loss: 4.1156 - accuracy: 0.0730
on_train_batch_begin: 1609168775.524255s

16 step training time: 0.357038s

on_train_batch_end: 1609168775.882342s

34816/50000 [===================>..........] - ETA: 2s - loss: 4.1105 - accuracy: 0.0730
on_train_batch_begin: 1609168775.882731s

17 step training time: 0.358476s

on_train_batch_end: 1609168776.240445s

36864/50000 [=====================>........] - ETA: 2s - loss: 4.0960 - accuracy: 0.0731
on_train_batch_begin: 1609168776.240830s

18 step training time: 0.358099s

on_train_batch_end: 1609168776.597406s

38912/50000 [======================>.......] - ETA: 1s - loss: 4.0751 - accuracy: 0.0731
on_train_batch_begin: 1609168776.597796s

19 step training time: 0.356966s

on_train_batch_end: 1609168776.956265s

40960/50000 [=======================>......] - ETA: 1s - loss: 4.0592 - accuracy: 0.0732
on_train_batch_begin: 1609168776.956668s

20 step training time: 0.358872s

on_train_batch_end: 1609168777.312260s

43008/50000 [========================>.....] - ETA: 1s - loss: 4.0369 - accuracy: 0.0734
on_train_batch_begin: 1609168777.312663s

21 step training time: 0.355995s

on_train_batch_end: 1609168777.668773s

45056/50000 [==========================>...] - ETA: 0s - loss: 4.0126 - accuracy: 0.0737
on_train_batch_begin: 1609168777.669176s

22 step training time: 0.356513s

on_train_batch_end: 1609168778.027330s

47104/50000 [===========================>..] - ETA: 0s - loss: 3.9936 - accuracy: 0.0739
on_train_batch_begin: 1609168778.027742s

23 step training time: 0.358566s

on_train_batch_end: 1609168778.381974s

49152/50000 [============================>.] - ETA: 0s - loss: 3.9720 - accuracy: 0.0741
on_train_batch_begin: 1609168778.382395s

24 step training time: 0.354652s

on_train_batch_end: 1609168778.620183s

on_test_batch_begin: 1609168778.712159s

25 step training time: 0.329765s

on_epoch_end: 1609168779.127355s

Validation time: 0.415173s

Real time: 1609168779.127355s

Epoch time: 9.325607776641846s

50000/50000 [==============================] - 9s 187us/sample - loss: 3.9634 - accuracy: 0.0742 - val_loss: 7.3258 - val_accuracy: 0.0213

on_epoch_begin: 1609168779.127608s

Real time: 1609168779.127617
Epoch 4/5

on_train_batch_begin: 1609168779.135349s

on_train_batch_end: 1609168779.494143s

 2048/50000 [>.............................] - ETA: 8s - loss: 3.2475 - accuracy: 0.0830
on_train_batch_begin: 1609168779.494530s

1 step training time: 0.359181s

on_train_batch_end: 1609168779.853073s

 4096/50000 [=>............................] - ETA: 8s - loss: 3.2016 - accuracy: 0.0832
on_train_batch_begin: 1609168779.853455s

2 step training time: 0.358925s

on_train_batch_end: 1609168780.210335s

 6144/50000 [==>...........................] - ETA: 7s - loss: 3.1105 - accuracy: 0.0841
on_train_batch_begin: 1609168780.210715s

3 step training time: 0.357260s

on_train_batch_end: 1609168780.565630s

 8192/50000 [===>..........................] - ETA: 7s - loss: 3.0884 - accuracy: 0.0844
on_train_batch_begin: 1609168780.566016s

4 step training time: 0.355301s

on_train_batch_end: 1609168780.925484s

10240/50000 [=====>........................] - ETA: 6s - loss: 3.0322 - accuracy: 0.0858
on_train_batch_begin: 1609168780.925869s

5 step training time: 0.359853s

on_train_batch_end: 1609168781.284763s

12288/50000 [======>.......................] - ETA: 6s - loss: 3.0013 - accuracy: 0.0868
on_train_batch_begin: 1609168781.285147s

6 step training time: 0.359279s

on_train_batch_end: 1609168781.643330s

14336/50000 [=======>......................] - ETA: 6s - loss: 2.9131 - accuracy: 0.0880
on_train_batch_begin: 1609168781.643717s

7 step training time: 0.358570s

on_train_batch_end: 1609168782.001138s

16384/50000 [========>.....................] - ETA: 5s - loss: 2.8556 - accuracy: 0.0891
on_train_batch_begin: 1609168782.001525s

8 step training time: 0.357808s

on_train_batch_end: 1609168782.360788s

18432/50000 [==========>...................] - ETA: 5s - loss: 2.8152 - accuracy: 0.0901
on_train_batch_begin: 1609168782.361177s

9 step training time: 0.359652s

on_train_batch_end: 1609168782.720958s

20480/50000 [===========>..................] - ETA: 5s - loss: 2.7778 - accuracy: 0.0910
on_train_batch_begin: 1609168782.721343s

10 step training time: 0.360166s

on_train_batch_end: 1609168783.079646s

22528/50000 [============>.................] - ETA: 4s - loss: 2.7332 - accuracy: 0.0918
on_train_batch_begin: 1609168783.080033s

11 step training time: 0.358690s

on_train_batch_end: 1609168783.439656s

24576/50000 [=============>................] - ETA: 4s - loss: 2.6996 - accuracy: 0.0924
on_train_batch_begin: 1609168783.440039s

12 step training time: 0.360006s

on_train_batch_end: 1609168783.796991s

26624/50000 [==============>...............] - ETA: 4s - loss: 2.6744 - accuracy: 0.0929
on_train_batch_begin: 1609168783.797375s

13 step training time: 0.357336s

on_train_batch_end: 1609168784.155915s

28672/50000 [================>.............] - ETA: 3s - loss: 2.6511 - accuracy: 0.0933
on_train_batch_begin: 1609168784.156296s

14 step training time: 0.358922s

on_train_batch_end: 1609168784.515703s

30720/50000 [=================>............] - ETA: 3s - loss: 2.6312 - accuracy: 0.0937
on_train_batch_begin: 1609168784.516085s

15 step training time: 0.359788s

on_train_batch_end: 1609168784.874530s

32768/50000 [==================>...........] - ETA: 3s - loss: 2.6112 - accuracy: 0.0941
on_train_batch_begin: 1609168784.874949s

16 step training time: 0.358865s

on_train_batch_end: 1609168785.233122s

34816/50000 [===================>..........] - ETA: 2s - loss: 2.5903 - accuracy: 0.0945
on_train_batch_begin: 1609168785.233514s

17 step training time: 0.358564s

on_train_batch_end: 1609168785.592545s

36864/50000 [=====================>........] - ETA: 2s - loss: 2.5712 - accuracy: 0.0948
on_train_batch_begin: 1609168785.592942s

18 step training time: 0.359428s

on_train_batch_end: 1609168785.951442s

38912/50000 [======================>.......] - ETA: 1s - loss: 2.5463 - accuracy: 0.0952
on_train_batch_begin: 1609168785.951847s

19 step training time: 0.358905s

on_train_batch_end: 1609168786.311584s

40960/50000 [=======================>......] - ETA: 1s - loss: 2.5259 - accuracy: 0.0954
on_train_batch_begin: 1609168786.311986s

20 step training time: 0.360139s

on_train_batch_end: 1609168786.673150s

43008/50000 [========================>.....] - ETA: 1s - loss: 2.5081 - accuracy: 0.0957
on_train_batch_begin: 1609168786.673544s

21 step training time: 0.361558s

on_train_batch_end: 1609168787.034088s

45056/50000 [==========================>...] - ETA: 0s - loss: 2.4951 - accuracy: 0.0960
on_train_batch_begin: 1609168787.034487s

22 step training time: 0.360943s

on_train_batch_end: 1609168787.391945s

47104/50000 [===========================>..] - ETA: 0s - loss: 2.4771 - accuracy: 0.0962
on_train_batch_begin: 1609168787.392368s

23 step training time: 0.357881s

on_train_batch_end: 1609168787.751565s

49152/50000 [============================>.] - ETA: 0s - loss: 2.4643 - accuracy: 0.0964
on_train_batch_begin: 1609168787.751971s

24 step training time: 0.359602s

on_train_batch_end: 1609168787.950973s

on_test_batch_begin: 1609168788.043433s

25 step training time: 0.291462s

on_epoch_end: 1609168788.458364s

Validation time: 0.414911s

Real time: 1609168788.458364s

Epoch time: 9.33077073097229s

50000/50000 [==============================] - 9s 187us/sample - loss: 2.4631 - accuracy: 0.0964 - val_loss: 8.2497 - val_accuracy: 0.0000e+00

on_epoch_begin: 1609168788.458635s

Real time: 1609168788.4586446
Epoch 5/5

on_train_batch_begin: 1609168788.466846s

on_train_batch_end: 1609168788.827349s

 2048/50000 [>.............................] - ETA: 8s - loss: 2.0734 - accuracy: 0.1005
on_train_batch_begin: 1609168788.827740s

1 step training time: 0.360893s

on_train_batch_end: 1609168789.187351s

 4096/50000 [=>............................] - ETA: 8s - loss: 2.0898 - accuracy: 0.1007
on_train_batch_begin: 1609168789.187741s

2 step training time: 0.360001s

on_train_batch_end: 1609168789.545010s

 6144/50000 [==>...........................] - ETA: 7s - loss: 2.0369 - accuracy: 0.1012
on_train_batch_begin: 1609168789.545395s

3 step training time: 0.357654s

on_train_batch_end: 1609168789.901563s

 8192/50000 [===>..........................] - ETA: 7s - loss: 2.0570 - accuracy: 0.1010
on_train_batch_begin: 1609168789.901958s

4 step training time: 0.356563s

on_train_batch_end: 1609168790.261076s

10240/50000 [=====>........................] - ETA: 6s - loss: 2.0628 - accuracy: 0.1009
on_train_batch_begin: 1609168790.261469s

5 step training time: 0.359511s

on_train_batch_end: 1609168790.621813s

12288/50000 [======>.......................] - ETA: 6s - loss: 2.0646 - accuracy: 0.1009
on_train_batch_begin: 1609168790.622194s

6 step training time: 0.360725s

on_train_batch_end: 1609168790.978026s

14336/50000 [=======>......................] - ETA: 6s - loss: 2.0605 - accuracy: 0.1010
on_train_batch_begin: 1609168790.978408s

7 step training time: 0.356214s

on_train_batch_end: 1609168791.334365s

16384/50000 [========>.....................] - ETA: 5s - loss: 2.0680 - accuracy: 0.1009
on_train_batch_begin: 1609168791.334796s

8 step training time: 0.356388s

on_train_batch_end: 1609168791.691507s

18432/50000 [==========>...................] - ETA: 5s - loss: 2.0678 - accuracy: 0.1009
on_train_batch_begin: 1609168791.691893s

9 step training time: 0.357097s

on_train_batch_end: 1609168792.051024s

20480/50000 [===========>..................] - ETA: 5s - loss: 2.0521 - accuracy: 0.1009
on_train_batch_begin: 1609168792.051410s

10 step training time: 0.359517s

on_train_batch_end: 1609168792.412210s

22528/50000 [============>.................] - ETA: 4s - loss: 2.0524 - accuracy: 0.1009
on_train_batch_begin: 1609168792.412598s

11 step training time: 0.361187s

on_train_batch_end: 1609168792.772098s

24576/50000 [=============>................] - ETA: 4s - loss: 2.0518 - accuracy: 0.1009
on_train_batch_begin: 1609168792.772476s

12 step training time: 0.359878s

on_train_batch_end: 1609168793.130109s

26624/50000 [==============>...............] - ETA: 4s - loss: 2.0387 - accuracy: 0.1008
on_train_batch_begin: 1609168793.130492s

13 step training time: 0.358016s

on_train_batch_end: 1609168793.488383s

28672/50000 [================>.............] - ETA: 3s - loss: 2.0378 - accuracy: 0.1009
on_train_batch_begin: 1609168793.488765s

14 step training time: 0.358274s

on_train_batch_end: 1609168793.847300s

30720/50000 [=================>............] - ETA: 3s - loss: 2.0355 - accuracy: 0.1009
on_train_batch_begin: 1609168793.847684s

15 step training time: 0.358919s

on_train_batch_end: 1609168794.203356s

32768/50000 [==================>...........] - ETA: 3s - loss: 2.0398 - accuracy: 0.1008
on_train_batch_begin: 1609168794.203768s

16 step training time: 0.356083s

on_train_batch_end: 1609168794.562141s

34816/50000 [===================>..........] - ETA: 2s - loss: 2.0438 - accuracy: 0.1008
on_train_batch_begin: 1609168794.562522s

17 step training time: 0.358754s

on_train_batch_end: 1609168794.922778s

36864/50000 [=====================>........] - ETA: 2s - loss: 2.0429 - accuracy: 0.1008
on_train_batch_begin: 1609168794.923161s

18 step training time: 0.360640s

on_train_batch_end: 1609168795.281667s

38912/50000 [======================>.......] - ETA: 1s - loss: 2.0421 - accuracy: 0.1008
on_train_batch_begin: 1609168795.282054s

19 step training time: 0.358893s

on_train_batch_end: 1609168795.640910s

40960/50000 [=======================>......] - ETA: 1s - loss: 2.0500 - accuracy: 0.1008
on_train_batch_begin: 1609168795.641297s

20 step training time: 0.359243s

on_train_batch_end: 1609168795.999849s

43008/50000 [========================>.....] - ETA: 1s - loss: 2.0469 - accuracy: 0.1008
on_train_batch_begin: 1609168796.000227s

21 step training time: 0.358930s

on_train_batch_end: 1609168796.361286s

45056/50000 [==========================>...] - ETA: 0s - loss: 2.0487 - accuracy: 0.1008
on_train_batch_begin: 1609168796.361668s

22 step training time: 0.361441s

on_train_batch_end: 1609168796.716527s

47104/50000 [===========================>..] - ETA: 0s - loss: 2.0429 - accuracy: 0.1008
on_train_batch_begin: 1609168796.716908s

23 step training time: 0.355241s

on_train_batch_end: 1609168797.072466s

49152/50000 [============================>.] - ETA: 0s - loss: 2.0411 - accuracy: 0.1008
on_train_batch_begin: 1609168797.072850s

24 step training time: 0.355942s

on_train_batch_end: 1609168797.309510s

on_test_batch_begin: 1609168797.399262s

25 step training time: 0.326412s

on_epoch_end: 1609168797.812809s

Validation time: 0.413526s

Real time: 1609168797.812809s

Epoch time: 9.35418701171875s

50000/50000 [==============================] - 9s 187us/sample - loss: 2.0412 - accuracy: 0.1009 - val_loss: 8.1904 - val_accuracy: 0.0987
Tempo do fit: 219.3817310333252