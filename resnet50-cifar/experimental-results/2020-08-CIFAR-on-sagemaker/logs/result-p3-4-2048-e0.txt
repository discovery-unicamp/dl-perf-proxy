wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:10
   188416/170498071 [..............................] - ETA: 1:19
  1114112/170498071 [..............................] - ETA: 20s 
  3153920/170498071 [..............................] - ETA: 9s 
  6381568/170498071 [>.............................] - ETA: 6s
  9732096/170498071 [>.............................] - ETA: 4s
 13189120/170498071 [=>............................] - ETA: 4s
 16556032/170498071 [=>............................] - ETA: 3s
 19914752/170498071 [==>...........................] - ETA: 3s
 23371776/170498071 [===>..........................] - ETA: 3s
 26779648/170498071 [===>..........................] - ETA: 2s
 30203904/170498071 [====>.........................] - ETA: 2s
 33497088/170498071 [====>.........................] - ETA: 2s
 36642816/170498071 [=====>........................] - ETA: 2s
 39755776/170498071 [=====>........................] - ETA: 2s
 43081728/170498071 [======>.......................] - ETA: 2s
 46456832/170498071 [=======>......................] - ETA: 2s
 49881088/170498071 [=======>......................] - ETA: 2s
 53239808/170498071 [========>.....................] - ETA: 2s
 56565760/170498071 [========>.....................] - ETA: 1s
 59891712/170498071 [=========>....................] - ETA: 1s
 63217664/170498071 [==========>...................] - ETA: 1s
 66592768/170498071 [==========>...................] - ETA: 1s
 69951488/170498071 [===========>..................] - ETA: 1s
 73342976/170498071 [===========>..................] - ETA: 1s
 76685312/170498071 [============>.................] - ETA: 1s
 80019456/170498071 [=============>................] - ETA: 1s
 82075648/170498071 [=============>................] - ETA: 1s
 84942848/170498071 [=============>................] - ETA: 1s
 88301568/170498071 [==============>...............] - ETA: 1s
 91594752/170498071 [===============>..............] - ETA: 1s
 95002624/170498071 [===============>..............] - ETA: 1s
 98369536/170498071 [================>.............] - ETA: 1s
101769216/170498071 [================>.............] - ETA: 1s
105095168/170498071 [=================>............] - ETA: 1s
108421120/170498071 [==================>...........] - ETA: 1s
111534080/170498071 [==================>...........] - ETA: 0s
114892800/170498071 [===================>..........] - ETA: 0s
118235136/170498071 [===================>..........] - ETA: 0s
121626624/170498071 [====================>.........] - ETA: 0s
124936192/170498071 [====================>.........] - ETA: 0s
126869504/170498071 [=====================>........] - ETA: 0s
129376256/170498071 [=====================>........] - ETA: 0s
132259840/170498071 [======================>.......] - ETA: 0s
135553024/170498071 [======================>.......] - ETA: 0s
138813440/170498071 [=======================>......] - ETA: 0s
142139392/170498071 [========================>.....] - ETA: 0s
145489920/170498071 [========================>.....] - ETA: 0s
148873216/170498071 [=========================>....] - ETA: 0s
152264704/170498071 [=========================>....] - ETA: 0s
155656192/170498071 [==========================>...] - ETA: 0s
158998528/170498071 [==========================>...] - ETA: 0s
162324480/170498071 [===========================>..] - ETA: 0s
165666816/170498071 [============================>.] - ETA: 0s
169041920/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 2s
 5472256/94765736 [>.............................] - ETA: 0s
 9388032/94765736 [=>............................] - ETA: 2s
15040512/94765736 [===>..........................] - ETA: 1s
18841600/94765736 [====>.........................] - ETA: 1s
21594112/94765736 [=====>........................] - ETA: 1s
25313280/94765736 [=======>......................] - ETA: 1s
28286976/94765736 [=======>......................] - ETA: 1s
32718848/94765736 [=========>....................] - ETA: 1s
36241408/94765736 [==========>...................] - ETA: 1s
36438016/94765736 [==========>...................] - ETA: 1s
43212800/94765736 [============>.................] - ETA: 1s
45146112/94765736 [=============>................] - ETA: 1s
47136768/94765736 [=============>................] - ETA: 1s
51027968/94765736 [===============>..............] - ETA: 1s
56598528/94765736 [================>.............] - ETA: 0s
62791680/94765736 [==================>...........] - ETA: 0s
65986560/94765736 [===================>..........] - ETA: 0s
68804608/94765736 [====================>.........] - ETA: 0s
71892992/94765736 [=====================>........] - ETA: 0s
75440128/94765736 [======================>.......] - ETA: 0s
77488128/94765736 [=======================>......] - ETA: 0s
80199680/94765736 [========================>.....] - ETA: 0s
83714048/94765736 [=========================>....] - ETA: 0s
88883200/94765736 [===========================>..] - ETA: 0s
89554944/94765736 [===========================>..] - ETA: 0s
93364224/94765736 [============================>.] - ETA: 0s
94281728/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 2s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 21.748663902282715
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1599696972.623656s

Real time: 1599696972.6236763
Epoch 1/5

on_train_batch_begin: 1599696973.540865s

on_train_batch_end: 1599697031.105243s

 2048/50000 [>.............................] - ETA: 22:49 - loss: 17.5930 - accuracy: 3.0231e-04
on_train_batch_begin: 1599697031.105997s

1 step training time: 57.565132s

on_train_batch_end: 1599697031.191515s

 4096/50000 [=>............................] - ETA: 10:56 - loss: 14.8911 - accuracy: 2.9612e-04
on_train_batch_begin: 1599697031.191931s

2 step training time: 0.085934s

on_train_batch_end: 1599697031.272927s

 6144/50000 [==>...........................] - ETA: 6:58 - loss: 12.9313 - accuracy: 4.6730e-04 
on_train_batch_begin: 1599697031.273313s

3 step training time: 0.081382s

on_train_batch_end: 1599697031.356297s

 8192/50000 [===>..........................] - ETA: 4:59 - loss: 11.7747 - accuracy: 0.0014    
on_train_batch_begin: 1599697031.356674s

4 step training time: 0.083361s

on_train_batch_end: 1599697031.436658s

10240/50000 [=====>........................] - ETA: 3:48 - loss: 11.0547 - accuracy: 0.0028
on_train_batch_begin: 1599697031.437027s

5 step training time: 0.080353s

on_train_batch_end: 1599697031.518369s

12288/50000 [======>.......................] - ETA: 3:00 - loss: 10.5416 - accuracy: 0.0062
on_train_batch_begin: 1599697031.518737s

6 step training time: 0.081710s

on_train_batch_end: 1599697031.598092s

14336/50000 [=======>......................] - ETA: 2:26 - loss: 10.1611 - accuracy: 0.0103
on_train_batch_begin: 1599697031.598459s

7 step training time: 0.079722s

on_train_batch_end: 1599697031.679703s

16384/50000 [========>.....................] - ETA: 2:01 - loss: 9.8510 - accuracy: 0.0161 
on_train_batch_begin: 1599697031.680071s

8 step training time: 0.081611s

on_train_batch_end: 1599697031.760907s

18432/50000 [==========>...................] - ETA: 1:41 - loss: 9.5967 - accuracy: 0.0221
on_train_batch_begin: 1599697031.761270s

9 step training time: 0.081199s

on_train_batch_end: 1599697031.842916s

20480/50000 [===========>..................] - ETA: 1:25 - loss: 9.3890 - accuracy: 0.0263
on_train_batch_begin: 1599697031.843285s

10 step training time: 0.082015s

on_train_batch_end: 1599697031.924191s

22528/50000 [============>.................] - ETA: 1:12 - loss: 9.2074 - accuracy: 0.0302
on_train_batch_begin: 1599697031.924559s

11 step training time: 0.081274s

on_train_batch_end: 1599697032.005373s

24576/50000 [=============>................] - ETA: 1:01 - loss: 9.0528 - accuracy: 0.0339
on_train_batch_begin: 1599697032.005772s

12 step training time: 0.081213s

on_train_batch_end: 1599697032.087601s

26624/50000 [==============>...............] - ETA: 52s - loss: 8.9056 - accuracy: 0.0373 
on_train_batch_begin: 1599697032.087967s

13 step training time: 0.082195s

on_train_batch_end: 1599697032.168692s

28672/50000 [================>.............] - ETA: 44s - loss: 8.7851 - accuracy: 0.0403
on_train_batch_begin: 1599697032.169057s

14 step training time: 0.081090s

on_train_batch_end: 1599697032.249072s

30720/50000 [=================>............] - ETA: 37s - loss: 8.6713 - accuracy: 0.0426
on_train_batch_begin: 1599697032.249433s

15 step training time: 0.080376s

on_train_batch_end: 1599697032.327920s

32768/50000 [==================>...........] - ETA: 31s - loss: 8.5637 - accuracy: 0.0453
on_train_batch_begin: 1599697032.328295s

16 step training time: 0.078861s

on_train_batch_end: 1599697032.408391s

34816/50000 [===================>..........] - ETA: 26s - loss: 8.4730 - accuracy: 0.0470
on_train_batch_begin: 1599697032.408753s

17 step training time: 0.080458s

on_train_batch_end: 1599697032.491590s

36864/50000 [=====================>........] - ETA: 21s - loss: 8.3857 - accuracy: 0.0493
on_train_batch_begin: 1599697032.491959s

18 step training time: 0.083206s

on_train_batch_end: 1599697032.574063s

38912/50000 [======================>.......] - ETA: 17s - loss: 8.3064 - accuracy: 0.0508
on_train_batch_begin: 1599697032.574424s

19 step training time: 0.082465s

on_train_batch_end: 1599697032.656319s

40960/50000 [=======================>......] - ETA: 13s - loss: 8.2366 - accuracy: 0.0525
on_train_batch_begin: 1599697032.656707s

20 step training time: 0.082283s

on_train_batch_end: 1599697032.739905s

43008/50000 [========================>.....] - ETA: 9s - loss: 8.1690 - accuracy: 0.0536 
on_train_batch_begin: 1599697032.740285s

21 step training time: 0.083578s

on_train_batch_end: 1599697032.820647s

45056/50000 [==========================>...] - ETA: 6s - loss: 8.1020 - accuracy: 0.0548
on_train_batch_begin: 1599697032.821027s

22 step training time: 0.080742s

on_train_batch_end: 1599697032.902295s

47104/50000 [===========================>..] - ETA: 3s - loss: 8.0423 - accuracy: 0.0557
on_train_batch_begin: 1599697032.902672s

23 step training time: 0.081645s

on_train_batch_end: 1599697032.986085s

49152/50000 [============================>.] - ETA: 1s - loss: 7.9828 - accuracy: 0.0565
on_train_batch_begin: 1599697032.986475s

24 step training time: 0.083803s

on_train_batch_end: 1599697034.016718s

on_test_batch_begin: 1599697034.304840s

25 step training time: 1.318365s

on_epoch_end: 1599697040.001136s

Validation time: 5.696277s

Real time: 1599697040.001136s

Epoch time: 67.3774824142456s

50000/50000 [==============================] - 67s 1ms/sample - loss: 7.9607 - accuracy: 0.0565 - val_loss: 502092.4423 - val_accuracy: 0.0000e+00

on_epoch_begin: 1599697040.001397s

Real time: 1599697040.0014062
Epoch 2/5

on_train_batch_begin: 1599697040.007032s

on_train_batch_end: 1599697040.088258s

 2048/50000 [>.............................] - ETA: 2s - loss: 6.5807 - accuracy: 0.0737
on_train_batch_begin: 1599697040.088632s

1 step training time: 0.081600s

on_train_batch_end: 1599697040.168740s

 4096/50000 [=>............................] - ETA: 1s - loss: 6.4987 - accuracy: 0.0846
on_train_batch_begin: 1599697040.169122s

2 step training time: 0.080490s

on_train_batch_end: 1599697040.252667s

 6144/50000 [==>...........................] - ETA: 1s - loss: 6.4742 - accuracy: 0.0872
on_train_batch_begin: 1599697040.253041s

3 step training time: 0.083920s

on_train_batch_end: 1599697040.333395s

 8192/50000 [===>..........................] - ETA: 1s - loss: 6.4193 - accuracy: 0.0877
on_train_batch_begin: 1599697040.333802s

4 step training time: 0.080761s

on_train_batch_end: 1599697040.413490s

10240/50000 [=====>........................] - ETA: 1s - loss: 6.3721 - accuracy: 0.0861
on_train_batch_begin: 1599697040.413897s

5 step training time: 0.080095s

on_train_batch_end: 1599697040.495589s

12288/50000 [======>.......................] - ETA: 1s - loss: 6.3410 - accuracy: 0.0854
on_train_batch_begin: 1599697040.495961s

6 step training time: 0.082064s

on_train_batch_end: 1599697040.579528s

14336/50000 [=======>......................] - ETA: 1s - loss: 6.2924 - accuracy: 0.0850
on_train_batch_begin: 1599697040.579902s

7 step training time: 0.083941s

on_train_batch_end: 1599697040.661585s

16384/50000 [========>.....................] - ETA: 1s - loss: 6.2488 - accuracy: 0.0835
on_train_batch_begin: 1599697040.661988s

8 step training time: 0.082087s

on_train_batch_end: 1599697040.741672s

18432/50000 [==========>...................] - ETA: 1s - loss: 6.2138 - accuracy: 0.0816
on_train_batch_begin: 1599697040.742050s

9 step training time: 0.080061s

on_train_batch_end: 1599697040.821911s

20480/50000 [===========>..................] - ETA: 1s - loss: 6.1889 - accuracy: 0.0797
on_train_batch_begin: 1599697040.822281s

10 step training time: 0.080231s

on_train_batch_end: 1599697040.903456s

22528/50000 [============>.................] - ETA: 1s - loss: 6.1535 - accuracy: 0.0782
on_train_batch_begin: 1599697040.903835s

11 step training time: 0.081554s

on_train_batch_end: 1599697040.983934s

24576/50000 [=============>................] - ETA: 1s - loss: 6.1057 - accuracy: 0.0773
on_train_batch_begin: 1599697040.984306s

12 step training time: 0.080470s

on_train_batch_end: 1599697041.065144s

26624/50000 [==============>...............] - ETA: 0s - loss: 6.0602 - accuracy: 0.0763
on_train_batch_begin: 1599697041.065508s

13 step training time: 0.081203s

on_train_batch_end: 1599697041.148878s

28672/50000 [================>.............] - ETA: 0s - loss: 6.0199 - accuracy: 0.0756
on_train_batch_begin: 1599697041.149239s

14 step training time: 0.083731s

on_train_batch_end: 1599697041.234649s

30720/50000 [=================>............] - ETA: 0s - loss: 5.9717 - accuracy: 0.0753
on_train_batch_begin: 1599697041.235022s

15 step training time: 0.085782s

on_train_batch_end: 1599697041.315688s

32768/50000 [==================>...........] - ETA: 0s - loss: 5.9286 - accuracy: 0.0750
on_train_batch_begin: 1599697041.316060s

16 step training time: 0.081039s

on_train_batch_end: 1599697041.395826s

34816/50000 [===================>..........] - ETA: 0s - loss: 5.8874 - accuracy: 0.0745
on_train_batch_begin: 1599697041.396193s

17 step training time: 0.080133s

on_train_batch_end: 1599697041.477342s

36864/50000 [=====================>........] - ETA: 0s - loss: 5.8449 - accuracy: 0.0739
on_train_batch_begin: 1599697041.477739s

18 step training time: 0.081546s

on_train_batch_end: 1599697041.557576s

38912/50000 [======================>.......] - ETA: 0s - loss: 5.8026 - accuracy: 0.0734
on_train_batch_begin: 1599697041.557967s

19 step training time: 0.080228s

on_train_batch_end: 1599697041.636670s

40960/50000 [=======================>......] - ETA: 0s - loss: 5.7674 - accuracy: 0.0730
on_train_batch_begin: 1599697041.637034s

20 step training time: 0.079068s

on_train_batch_end: 1599697041.717358s

43008/50000 [========================>.....] - ETA: 0s - loss: 5.7270 - accuracy: 0.0728
on_train_batch_begin: 1599697041.717759s

21 step training time: 0.080724s

on_train_batch_end: 1599697041.797876s

45056/50000 [==========================>...] - ETA: 0s - loss: 5.6884 - accuracy: 0.0726
on_train_batch_begin: 1599697041.798243s

22 step training time: 0.080484s

on_train_batch_end: 1599697041.877127s

47104/50000 [===========================>..] - ETA: 0s - loss: 5.6534 - accuracy: 0.0724
on_train_batch_begin: 1599697041.877600s

23 step training time: 0.079357s

on_train_batch_end: 1599697041.956812s

49152/50000 [============================>.] - ETA: 0s - loss: 5.6196 - accuracy: 0.0720
on_train_batch_begin: 1599697041.957181s

24 step training time: 0.079581s

on_train_batch_end: 1599697042.018916s

on_test_batch_begin: 1599697042.096797s

25 step training time: 0.139616s

on_epoch_end: 1599697042.328884s

Validation time: 0.232072s

Real time: 1599697042.328884s

Epoch time: 2.3274998664855957s

50000/50000 [==============================] - 2s 47us/sample - loss: 5.6030 - accuracy: 0.0720 - val_loss: 8.3576 - val_accuracy: 0.0000e+00

on_epoch_begin: 1599697042.329163s

Real time: 1599697042.3291736
Epoch 3/5

on_train_batch_begin: 1599697042.335709s

on_train_batch_end: 1599697042.419553s

 2048/50000 [>.............................] - ETA: 2s - loss: 4.4605 - accuracy: 0.0659
on_train_batch_begin: 1599697042.419916s

1 step training time: 0.084207s

on_train_batch_end: 1599697042.502321s

 4096/50000 [=>............................] - ETA: 1s - loss: 4.4726 - accuracy: 0.0666
on_train_batch_begin: 1599697042.502683s

2 step training time: 0.082767s

on_train_batch_end: 1599697042.587147s

 6144/50000 [==>...........................] - ETA: 1s - loss: 4.4945 - accuracy: 0.0680
on_train_batch_begin: 1599697042.587519s

3 step training time: 0.084836s

on_train_batch_end: 1599697042.667431s

 8192/50000 [===>..........................] - ETA: 1s - loss: 4.4474 - accuracy: 0.0683
on_train_batch_begin: 1599697042.667799s

4 step training time: 0.080280s

on_train_batch_end: 1599697042.750387s

10240/50000 [=====>........................] - ETA: 1s - loss: 4.4079 - accuracy: 0.0686
on_train_batch_begin: 1599697042.750750s

5 step training time: 0.082951s

on_train_batch_end: 1599697042.831309s

12288/50000 [======>.......................] - ETA: 1s - loss: 4.3954 - accuracy: 0.0682
on_train_batch_begin: 1599697042.831673s

6 step training time: 0.080923s

on_train_batch_end: 1599697042.910813s

14336/50000 [=======>......................] - ETA: 1s - loss: 4.3721 - accuracy: 0.0684
on_train_batch_begin: 1599697042.911177s

7 step training time: 0.079504s

on_train_batch_end: 1599697042.991905s

16384/50000 [========>.....................] - ETA: 1s - loss: 4.3458 - accuracy: 0.0684
on_train_batch_begin: 1599697042.992272s

8 step training time: 0.081095s

on_train_batch_end: 1599697043.075673s

18432/50000 [==========>...................] - ETA: 1s - loss: 4.3174 - accuracy: 0.0686
on_train_batch_begin: 1599697043.076037s

9 step training time: 0.083765s

on_train_batch_end: 1599697043.155435s

20480/50000 [===========>..................] - ETA: 1s - loss: 4.2735 - accuracy: 0.0692
on_train_batch_begin: 1599697043.155800s

10 step training time: 0.079763s

on_train_batch_end: 1599697043.235292s

22528/50000 [============>.................] - ETA: 1s - loss: 4.2229 - accuracy: 0.0699
on_train_batch_begin: 1599697043.235657s

11 step training time: 0.079857s

on_train_batch_end: 1599697043.315683s

24576/50000 [=============>................] - ETA: 1s - loss: 4.1806 - accuracy: 0.0708
on_train_batch_begin: 1599697043.316048s

12 step training time: 0.080391s

on_train_batch_end: 1599697043.396125s

26624/50000 [==============>...............] - ETA: 0s - loss: 4.1445 - accuracy: 0.0714
on_train_batch_begin: 1599697043.396489s

13 step training time: 0.080441s

on_train_batch_end: 1599697043.480347s

28672/50000 [================>.............] - ETA: 0s - loss: 4.0981 - accuracy: 0.0722
on_train_batch_begin: 1599697043.480716s

14 step training time: 0.084227s

on_train_batch_end: 1599697043.560963s

30720/50000 [=================>............] - ETA: 0s - loss: 4.0602 - accuracy: 0.0732
on_train_batch_begin: 1599697043.561327s

15 step training time: 0.080611s

on_train_batch_end: 1599697043.646768s

32768/50000 [==================>...........] - ETA: 0s - loss: 4.0148 - accuracy: 0.0741
on_train_batch_begin: 1599697043.647131s

16 step training time: 0.085805s

on_train_batch_end: 1599697043.727120s

34816/50000 [===================>..........] - ETA: 0s - loss: 3.9679 - accuracy: 0.0749
on_train_batch_begin: 1599697043.727489s

17 step training time: 0.080358s

on_train_batch_end: 1599697043.808018s

36864/50000 [=====================>........] - ETA: 0s - loss: 3.9226 - accuracy: 0.0758
on_train_batch_begin: 1599697043.808381s

18 step training time: 0.080892s

on_train_batch_end: 1599697043.888532s

38912/50000 [======================>.......] - ETA: 0s - loss: 3.8796 - accuracy: 0.0767
on_train_batch_begin: 1599697043.888907s

19 step training time: 0.080526s

on_train_batch_end: 1599697043.968678s

40960/50000 [=======================>......] - ETA: 0s - loss: 3.8382 - accuracy: 0.0775
on_train_batch_begin: 1599697043.969038s

20 step training time: 0.080130s

on_train_batch_end: 1599697044.050762s

43008/50000 [========================>.....] - ETA: 0s - loss: 3.7981 - accuracy: 0.0783
on_train_batch_begin: 1599697044.051138s

21 step training time: 0.082100s

on_train_batch_end: 1599697044.130454s

45056/50000 [==========================>...] - ETA: 0s - loss: 3.7529 - accuracy: 0.0792
on_train_batch_begin: 1599697044.130813s

22 step training time: 0.079676s

on_train_batch_end: 1599697044.214530s

47104/50000 [===========================>..] - ETA: 0s - loss: 3.7142 - accuracy: 0.0800
on_train_batch_begin: 1599697044.214892s

23 step training time: 0.084079s

on_train_batch_end: 1599697044.296250s

49152/50000 [============================>.] - ETA: 0s - loss: 3.6793 - accuracy: 0.0807
on_train_batch_begin: 1599697044.296611s

24 step training time: 0.081719s

on_train_batch_end: 1599697044.359103s

on_test_batch_begin: 1599697044.435411s

25 step training time: 0.138800s

on_epoch_end: 1599697044.664219s

Validation time: 0.228794s

Real time: 1599697044.664219s

Epoch time: 2.335066556930542s

50000/50000 [==============================] - 2s 47us/sample - loss: 3.6643 - accuracy: 0.0808 - val_loss: 7.3685 - val_accuracy: 0.0000e+00

on_epoch_begin: 1599697044.664464s

Real time: 1599697044.6644738
Epoch 4/5

on_train_batch_begin: 1599697044.670808s

on_train_batch_end: 1599697044.755539s

 2048/50000 [>.............................] - ETA: 2s - loss: 2.5808 - accuracy: 0.0989
on_train_batch_begin: 1599697044.755902s

1 step training time: 0.085093s

on_train_batch_end: 1599697044.839244s

 4096/50000 [=>............................] - ETA: 1s - loss: 2.5790 - accuracy: 0.0989
on_train_batch_begin: 1599697044.839610s

2 step training time: 0.083709s

on_train_batch_end: 1599697044.925042s

 6144/50000 [==>...........................] - ETA: 1s - loss: 2.5555 - accuracy: 0.0991
on_train_batch_begin: 1599697044.925405s

3 step training time: 0.085794s

on_train_batch_end: 1599697045.004784s

 8192/50000 [===>..........................] - ETA: 1s - loss: 2.5071 - accuracy: 0.0996
on_train_batch_begin: 1599697045.005147s

4 step training time: 0.079742s

on_train_batch_end: 1599697045.084704s

10240/50000 [=====>........................] - ETA: 1s - loss: 2.5036 - accuracy: 0.0996
on_train_batch_begin: 1599697045.085064s

5 step training time: 0.079917s

on_train_batch_end: 1599697045.163705s

12288/50000 [======>.......................] - ETA: 1s - loss: 2.4819 - accuracy: 0.0998
on_train_batch_begin: 1599697045.164068s

6 step training time: 0.079003s

on_train_batch_end: 1599697045.246564s

14336/50000 [=======>......................] - ETA: 1s - loss: 2.4576 - accuracy: 0.0999
on_train_batch_begin: 1599697045.246925s

7 step training time: 0.082857s

on_train_batch_end: 1599697045.327144s

16384/50000 [========>.....................] - ETA: 1s - loss: 2.4464 - accuracy: 0.0998
on_train_batch_begin: 1599697045.327511s

8 step training time: 0.080587s

on_train_batch_end: 1599697045.405810s

18432/50000 [==========>...................] - ETA: 1s - loss: 2.4282 - accuracy: 0.0998
on_train_batch_begin: 1599697045.406173s

9 step training time: 0.078662s

on_train_batch_end: 1599697045.490729s

20480/50000 [===========>..................] - ETA: 1s - loss: 2.4071 - accuracy: 0.0998
on_train_batch_begin: 1599697045.491096s

10 step training time: 0.084923s

on_train_batch_end: 1599697045.572330s

22528/50000 [============>.................] - ETA: 1s - loss: 2.3825 - accuracy: 0.0999
on_train_batch_begin: 1599697045.572696s

11 step training time: 0.081599s

on_train_batch_end: 1599697045.652593s

24576/50000 [=============>................] - ETA: 1s - loss: 2.3711 - accuracy: 0.0999
on_train_batch_begin: 1599697045.652955s

12 step training time: 0.080259s

on_train_batch_end: 1599697045.734846s

26624/50000 [==============>...............] - ETA: 0s - loss: 2.3380 - accuracy: 0.1000
on_train_batch_begin: 1599697045.735224s

13 step training time: 0.082269s

on_train_batch_end: 1599697045.815909s

28672/50000 [================>.............] - ETA: 0s - loss: 2.3174 - accuracy: 0.1000
on_train_batch_begin: 1599697045.816272s

14 step training time: 0.081048s

on_train_batch_end: 1599697045.895703s

30720/50000 [=================>............] - ETA: 0s - loss: 2.2965 - accuracy: 0.1000
on_train_batch_begin: 1599697045.896067s

15 step training time: 0.079795s

on_train_batch_end: 1599697045.975068s

32768/50000 [==================>...........] - ETA: 0s - loss: 2.2786 - accuracy: 0.1000
on_train_batch_begin: 1599697045.975434s

16 step training time: 0.079367s

on_train_batch_end: 1599697046.053746s

34816/50000 [===================>..........] - ETA: 0s - loss: 2.2602 - accuracy: 0.1001
on_train_batch_begin: 1599697046.054108s

17 step training time: 0.078675s

on_train_batch_end: 1599697046.131925s

36864/50000 [=====================>........] - ETA: 0s - loss: 2.2456 - accuracy: 0.1001
on_train_batch_begin: 1599697046.132294s

18 step training time: 0.078186s

on_train_batch_end: 1599697046.212369s

38912/50000 [======================>.......] - ETA: 0s - loss: 2.2264 - accuracy: 0.1001
on_train_batch_begin: 1599697046.212739s

19 step training time: 0.080445s

on_train_batch_end: 1599697046.292581s

40960/50000 [=======================>......] - ETA: 0s - loss: 2.2087 - accuracy: 0.1001
on_train_batch_begin: 1599697046.292945s

20 step training time: 0.080206s

on_train_batch_end: 1599697046.373861s

43008/50000 [========================>.....] - ETA: 0s - loss: 2.1874 - accuracy: 0.1001
on_train_batch_begin: 1599697046.374227s

21 step training time: 0.081282s

on_train_batch_end: 1599697046.452503s

45056/50000 [==========================>...] - ETA: 0s - loss: 2.1760 - accuracy: 0.1001
on_train_batch_begin: 1599697046.452870s

22 step training time: 0.078643s

on_train_batch_end: 1599697046.533238s

47104/50000 [===========================>..] - ETA: 0s - loss: 2.1570 - accuracy: 0.1001
on_train_batch_begin: 1599697046.533597s

23 step training time: 0.080728s

on_train_batch_end: 1599697046.610753s

49152/50000 [============================>.] - ETA: 0s - loss: 2.1343 - accuracy: 0.1001
on_train_batch_begin: 1599697046.611115s

24 step training time: 0.077518s

on_train_batch_end: 1599697046.673410s

on_test_batch_begin: 1599697046.750381s

25 step training time: 0.139266s

on_epoch_end: 1599697046.976858s

Validation time: 0.226459s

Real time: 1599697046.976858s

Epoch time: 2.3124070167541504s

50000/50000 [==============================] - 2s 46us/sample - loss: 2.1277 - accuracy: 0.1001 - val_loss: 7.3819 - val_accuracy: 0.0000e+00

on_epoch_begin: 1599697046.977108s

Real time: 1599697046.9771166
Epoch 5/5

on_train_batch_begin: 1599697046.986000s

on_train_batch_end: 1599697047.067394s

 2048/50000 [>.............................] - ETA: 2s - loss: 1.4378 - accuracy: 0.1005
on_train_batch_begin: 1599697047.067782s

1 step training time: 0.081782s

on_train_batch_end: 1599697047.148288s

 4096/50000 [=>............................] - ETA: 1s - loss: 1.5559 - accuracy: 0.1003
on_train_batch_begin: 1599697047.148661s

2 step training time: 0.080879s

on_train_batch_end: 1599697047.230664s

 6144/50000 [==>...........................] - ETA: 1s - loss: 1.5402 - accuracy: 0.1005
on_train_batch_begin: 1599697047.231041s

3 step training time: 0.082380s

on_train_batch_end: 1599697047.316696s

 8192/50000 [===>..........................] - ETA: 1s - loss: 1.5178 - accuracy: 0.1005
on_train_batch_begin: 1599697047.317069s

4 step training time: 0.086028s

on_train_batch_end: 1599697047.404150s

10240/50000 [=====>........................] - ETA: 1s - loss: 1.5289 - accuracy: 0.1006
on_train_batch_begin: 1599697047.404524s

5 step training time: 0.087455s

on_train_batch_end: 1599697047.485728s

12288/50000 [======>.......................] - ETA: 1s - loss: 1.5139 - accuracy: 0.1006
on_train_batch_begin: 1599697047.486103s

6 step training time: 0.081578s

on_train_batch_end: 1599697047.566177s

14336/50000 [=======>......................] - ETA: 1s - loss: 1.4856 - accuracy: 0.1005
on_train_batch_begin: 1599697047.566557s

7 step training time: 0.080455s

on_train_batch_end: 1599697047.645706s

16384/50000 [========>.....................] - ETA: 1s - loss: 1.4921 - accuracy: 0.1005
on_train_batch_begin: 1599697047.646086s

8 step training time: 0.079529s

on_train_batch_end: 1599697047.726027s

18432/50000 [==========>...................] - ETA: 1s - loss: 1.4865 - accuracy: 0.1005
on_train_batch_begin: 1599697047.726403s

9 step training time: 0.080317s

on_train_batch_end: 1599697047.808350s

20480/50000 [===========>..................] - ETA: 1s - loss: 1.4774 - accuracy: 0.1005
on_train_batch_begin: 1599697047.808730s

10 step training time: 0.082327s

on_train_batch_end: 1599697047.889413s

22528/50000 [============>.................] - ETA: 1s - loss: 1.4622 - accuracy: 0.1005
on_train_batch_begin: 1599697047.889825s

11 step training time: 0.081095s

on_train_batch_end: 1599697047.972602s

24576/50000 [=============>................] - ETA: 1s - loss: 1.4586 - accuracy: 0.1005
on_train_batch_begin: 1599697047.972973s

12 step training time: 0.083148s

on_train_batch_end: 1599697048.053903s

26624/50000 [==============>...............] - ETA: 0s - loss: 1.4526 - accuracy: 0.1004
on_train_batch_begin: 1599697048.054283s

13 step training time: 0.081310s

on_train_batch_end: 1599697048.135335s

28672/50000 [================>.............] - ETA: 0s - loss: 1.4417 - accuracy: 0.1004
on_train_batch_begin: 1599697048.135710s

14 step training time: 0.081426s

on_train_batch_end: 1599697048.215571s

30720/50000 [=================>............] - ETA: 0s - loss: 1.4325 - accuracy: 0.1005
on_train_batch_begin: 1599697048.215945s

15 step training time: 0.080236s

on_train_batch_end: 1599697048.295507s

32768/50000 [==================>...........] - ETA: 0s - loss: 1.4201 - accuracy: 0.1005
on_train_batch_begin: 1599697048.295873s

16 step training time: 0.079928s

on_train_batch_end: 1599697048.375522s

34816/50000 [===================>..........] - ETA: 0s - loss: 1.4120 - accuracy: 0.1005
on_train_batch_begin: 1599697048.375881s

17 step training time: 0.080008s

on_train_batch_end: 1599697048.458026s

36864/50000 [=====================>........] - ETA: 0s - loss: 1.4032 - accuracy: 0.1005
on_train_batch_begin: 1599697048.458392s

18 step training time: 0.082510s

on_train_batch_end: 1599697048.543227s

38912/50000 [======================>.......] - ETA: 0s - loss: 1.3913 - accuracy: 0.1005
on_train_batch_begin: 1599697048.543589s

19 step training time: 0.085197s

on_train_batch_end: 1599697048.623913s

40960/50000 [=======================>......] - ETA: 0s - loss: 1.3823 - accuracy: 0.1005
on_train_batch_begin: 1599697048.624274s

20 step training time: 0.080685s

on_train_batch_end: 1599697048.703370s

43008/50000 [========================>.....] - ETA: 0s - loss: 1.3687 - accuracy: 0.1005
on_train_batch_begin: 1599697048.703732s

21 step training time: 0.079458s

on_train_batch_end: 1599697048.788157s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.3579 - accuracy: 0.1005
on_train_batch_begin: 1599697048.788518s

22 step training time: 0.084787s

on_train_batch_end: 1599697048.867636s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.3506 - accuracy: 0.1005
on_train_batch_begin: 1599697048.867996s

23 step training time: 0.079478s

on_train_batch_end: 1599697048.946268s

49152/50000 [============================>.] - ETA: 0s - loss: 1.3427 - accuracy: 0.1005
on_train_batch_begin: 1599697048.946632s

24 step training time: 0.078635s

on_train_batch_end: 1599697049.008468s

on_test_batch_begin: 1599697049.085433s

25 step training time: 0.138801s

on_epoch_end: 1599697049.304083s

Validation time: 0.218634s

Real time: 1599697049.304083s

Epoch time: 2.3269882202148438s

50000/50000 [==============================] - 2s 47us/sample - loss: 1.3384 - accuracy: 0.1005 - val_loss: 7.0891 - val_accuracy: 0.1001
Tempo do fit: 80.58924388885498