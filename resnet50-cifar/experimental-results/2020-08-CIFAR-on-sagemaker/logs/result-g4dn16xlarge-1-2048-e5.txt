wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 8:01
   155648/170498071 [..............................] - ETA: 1:20
   712704/170498071 [..............................] - ETA: 29s 
  2252800/170498071 [..............................] - ETA: 13s
  5038080/170498071 [..............................] - ETA: 7s 
  8069120/170498071 [>.............................] - ETA: 5s
 11182080/170498071 [>.............................] - ETA: 4s
 14360576/170498071 [=>............................] - ETA: 4s
 17342464/170498071 [==>...........................] - ETA: 3s
 20291584/170498071 [==>...........................] - ETA: 3s
 23404544/170498071 [===>..........................] - ETA: 3s
 26337280/170498071 [===>..........................] - ETA: 3s
 29401088/170498071 [====>.........................] - ETA: 3s
 32579584/170498071 [====>.........................] - ETA: 2s
 35577856/170498071 [=====>........................] - ETA: 2s
 38625280/170498071 [=====>........................] - ETA: 2s
 41689088/170498071 [======>.......................] - ETA: 2s
 44883968/170498071 [======>.......................] - ETA: 2s
 47800320/170498071 [=======>......................] - ETA: 2s
 50929664/170498071 [=======>......................] - ETA: 2s
 54108160/170498071 [========>.....................] - ETA: 2s
 57040896/170498071 [=========>....................] - ETA: 2s
 60170240/170498071 [=========>....................] - ETA: 2s
 63332352/170498071 [==========>...................] - ETA: 2s
 66199552/170498071 [==========>...................] - ETA: 1s
 69214208/170498071 [===========>..................] - ETA: 1s
 72335360/170498071 [===========>..................] - ETA: 1s
 75292672/170498071 [============>.................] - ETA: 1s
 78323712/170498071 [============>.................] - ETA: 1s
 81502208/170498071 [=============>................] - ETA: 1s
 84369408/170498071 [=============>................] - ETA: 1s
 87498752/170498071 [==============>...............] - ETA: 1s
 90677248/170498071 [==============>...............] - ETA: 1s
 93609984/170498071 [===============>..............] - ETA: 1s
 96739328/170498071 [================>.............] - ETA: 1s
 99868672/170498071 [================>.............] - ETA: 1s
102817792/170498071 [=================>............] - ETA: 1s
105848832/170498071 [=================>............] - ETA: 1s
108978176/170498071 [==================>...........] - ETA: 1s
112025600/170498071 [==================>...........] - ETA: 1s
115023872/170498071 [===================>..........] - ETA: 0s
118153216/170498071 [===================>..........] - ETA: 0s
121135104/170498071 [====================>.........] - ETA: 0s
124264448/170498071 [====================>.........] - ETA: 0s
127377408/170498071 [=====================>........] - ETA: 0s
130424832/170498071 [=====================>........] - ETA: 0s
133357568/170498071 [======================>.......] - ETA: 0s
136421376/170498071 [=======================>......] - ETA: 0s
139354112/170498071 [=======================>......] - ETA: 0s
142467072/170498071 [========================>.....] - ETA: 0s
145612800/170498071 [========================>.....] - ETA: 0s
148660224/170498071 [=========================>....] - ETA: 0s
151789568/170498071 [=========================>....] - ETA: 0s
155049984/170498071 [==========================>...] - ETA: 0s
158310400/170498071 [==========================>...] - ETA: 0s
161579008/170498071 [===========================>..] - ETA: 0s
164814848/170498071 [============================>.] - ETA: 0s
168189952/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 1s
 7651328/94765736 [=>............................] - ETA: 0s
 9388032/94765736 [=>............................] - ETA: 1s
17203200/94765736 [====>.........................] - ETA: 0s
18841600/94765736 [====>.........................] - ETA: 0s
28286976/94765736 [=======>......................] - ETA: 0s
35315712/94765736 [==========>...................] - ETA: 0s
38993920/94765736 [===========>..................] - ETA: 0s
47136768/94765736 [=============>................] - ETA: 0s
49627136/94765736 [==============>...............] - ETA: 0s
58949632/94765736 [=================>............] - ETA: 0s
67493888/94765736 [====================>.........] - ETA: 0s
75440128/94765736 [======================>.......] - ETA: 0s
83673088/94765736 [=========================>....] - ETA: 0s
84893696/94765736 [=========================>....] - ETA: 0s
93118464/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 1s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 13.252367734909058
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1615759716.209766s

Real time: 1615759716.2097833
Epoch 1/5

on_train_batch_begin: 1615759716.973656s

on_train_batch_end: 1615759738.569433s

 2048/50000 [>.............................] - ETA: 8:43 - loss: 17.9981 - accuracy: 9.3460e-05
on_train_batch_begin: 1615759738.570081s

1 step training time: 21.596426s

on_train_batch_end: 1615759739.213538s

 4096/50000 [=>............................] - ETA: 4:17 - loss: 14.4650 - accuracy: 2.2674e-04
on_train_batch_begin: 1615759739.213866s

2 step training time: 0.643785s

on_train_batch_end: 1615759739.860828s

 6144/50000 [==>...........................] - ETA: 2:48 - loss: 12.5337 - accuracy: 4.1040e-04
on_train_batch_begin: 1615759739.861146s

3 step training time: 0.647280s

on_train_batch_end: 1615759740.506213s

 8192/50000 [===>..........................] - ETA: 2:03 - loss: 11.4207 - accuracy: 0.0015    
on_train_batch_begin: 1615759740.506520s

4 step training time: 0.645374s

on_train_batch_end: 1615759741.149244s

10240/50000 [=====>........................] - ETA: 1:36 - loss: 10.7234 - accuracy: 0.0033
on_train_batch_begin: 1615759741.149549s

5 step training time: 0.643029s

on_train_batch_end: 1615759741.791821s

12288/50000 [======>.......................] - ETA: 1:18 - loss: 10.2288 - accuracy: 0.0059
on_train_batch_begin: 1615759741.792208s

6 step training time: 0.642659s

on_train_batch_end: 1615759742.438442s

14336/50000 [=======>......................] - ETA: 1:05 - loss: 9.8729 - accuracy: 0.0078 
on_train_batch_begin: 1615759742.438812s

7 step training time: 0.646604s

on_train_batch_end: 1615759743.082478s

16384/50000 [========>.....................] - ETA: 55s - loss: 9.5901 - accuracy: 0.0104 
on_train_batch_begin: 1615759743.082851s

8 step training time: 0.644039s

on_train_batch_end: 1615759743.735415s

18432/50000 [==========>...................] - ETA: 47s - loss: 9.3710 - accuracy: 0.0129
on_train_batch_begin: 1615759743.735789s

9 step training time: 0.652938s

on_train_batch_end: 1615759744.378484s

20480/50000 [===========>..................] - ETA: 40s - loss: 9.1784 - accuracy: 0.0156
on_train_batch_begin: 1615759744.378968s

10 step training time: 0.643178s

on_train_batch_end: 1615759745.033810s

22528/50000 [============>.................] - ETA: 35s - loss: 9.0002 - accuracy: 0.0186
on_train_batch_begin: 1615759745.034183s

11 step training time: 0.655216s

on_train_batch_end: 1615759745.678252s

24576/50000 [=============>................] - ETA: 30s - loss: 8.8465 - accuracy: 0.0206
on_train_batch_begin: 1615759745.678612s

12 step training time: 0.644429s

on_train_batch_end: 1615759746.332950s

26624/50000 [==============>...............] - ETA: 26s - loss: 8.7197 - accuracy: 0.0226
on_train_batch_begin: 1615759746.333256s

13 step training time: 0.654644s

on_train_batch_end: 1615759746.981477s

28672/50000 [================>.............] - ETA: 22s - loss: 8.5936 - accuracy: 0.0248
on_train_batch_begin: 1615759746.981778s

14 step training time: 0.648522s

on_train_batch_end: 1615759747.634375s

30720/50000 [=================>............] - ETA: 19s - loss: 8.4910 - accuracy: 0.0258
on_train_batch_begin: 1615759747.634672s

15 step training time: 0.652894s

on_train_batch_end: 1615759748.282905s

32768/50000 [==================>...........] - ETA: 16s - loss: 8.3897 - accuracy: 0.0279
on_train_batch_begin: 1615759748.283206s

16 step training time: 0.648535s

on_train_batch_end: 1615759748.935941s

34816/50000 [===================>..........] - ETA: 14s - loss: 8.2964 - accuracy: 0.0305
on_train_batch_begin: 1615759748.936238s

17 step training time: 0.653032s

on_train_batch_end: 1615759749.584373s

36864/50000 [=====================>........] - ETA: 11s - loss: 8.2115 - accuracy: 0.0325
on_train_batch_begin: 1615759749.584669s

18 step training time: 0.648431s

on_train_batch_end: 1615759750.236900s

38912/50000 [======================>.......] - ETA: 9s - loss: 8.1295 - accuracy: 0.0343 
on_train_batch_begin: 1615759750.237197s

19 step training time: 0.652529s

on_train_batch_end: 1615759750.886417s

40960/50000 [=======================>......] - ETA: 7s - loss: 8.0499 - accuracy: 0.0360
on_train_batch_begin: 1615759750.886721s

20 step training time: 0.649524s

on_train_batch_end: 1615759751.536927s

43008/50000 [========================>.....] - ETA: 5s - loss: 7.9709 - accuracy: 0.0375
on_train_batch_begin: 1615759751.537220s

21 step training time: 0.650499s

on_train_batch_end: 1615759752.190188s

45056/50000 [==========================>...] - ETA: 3s - loss: 7.8969 - accuracy: 0.0388
on_train_batch_begin: 1615759752.190483s

22 step training time: 0.653263s

on_train_batch_end: 1615759752.840054s

47104/50000 [===========================>..] - ETA: 2s - loss: 7.8197 - accuracy: 0.0398
on_train_batch_begin: 1615759752.840350s

23 step training time: 0.649867s

on_train_batch_end: 1615759753.486840s

49152/50000 [============================>.] - ETA: 0s - loss: 7.7441 - accuracy: 0.0405
on_train_batch_begin: 1615759753.487149s

24 step training time: 0.646799s

on_train_batch_end: 1615759759.313234s

on_test_batch_begin: 1615759759.502949s

25 step training time: 6.015800s

on_epoch_end: 1615759764.873657s

Validation time: 5.370692s

Real time: 1615759764.873657s

Epoch time: 48.66389727592468s

50000/50000 [==============================] - 49s 973us/sample - loss: 7.7094 - accuracy: 0.0406 - val_loss: 68932.5307 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615759764.873901s

Real time: 1615759764.8739066
Epoch 2/5

on_train_batch_begin: 1615759764.877540s

on_train_batch_end: 1615759765.525359s

 2048/50000 [>.............................] - ETA: 15s - loss: 5.4997 - accuracy: 0.0557
on_train_batch_begin: 1615759765.525747s

1 step training time: 0.648207s

on_train_batch_end: 1615759766.182806s

 4096/50000 [=>............................] - ETA: 14s - loss: 5.4971 - accuracy: 0.0551
on_train_batch_begin: 1615759766.183163s

2 step training time: 0.657416s

on_train_batch_end: 1615759766.833482s

 6144/50000 [==>...........................] - ETA: 13s - loss: 5.4262 - accuracy: 0.0565
on_train_batch_begin: 1615759766.833797s

3 step training time: 0.650634s

on_train_batch_end: 1615759767.490978s

 8192/50000 [===>..........................] - ETA: 13s - loss: 5.3758 - accuracy: 0.0587
on_train_batch_begin: 1615759767.491279s

4 step training time: 0.657483s

on_train_batch_end: 1615759768.146485s

10240/50000 [=====>........................] - ETA: 12s - loss: 5.3140 - accuracy: 0.0605
on_train_batch_begin: 1615759768.146784s

5 step training time: 0.655504s

on_train_batch_end: 1615759768.800799s

12288/50000 [======>.......................] - ETA: 12s - loss: 5.2528 - accuracy: 0.0613
on_train_batch_begin: 1615759768.801099s

6 step training time: 0.654315s

on_train_batch_end: 1615759769.450044s

14336/50000 [=======>......................] - ETA: 11s - loss: 5.1791 - accuracy: 0.0627
on_train_batch_begin: 1615759769.450340s

7 step training time: 0.649241s

on_train_batch_end: 1615759770.111729s

16384/50000 [========>.....................] - ETA: 10s - loss: 5.1137 - accuracy: 0.0641
on_train_batch_begin: 1615759770.112022s

8 step training time: 0.661682s

on_train_batch_end: 1615759770.765757s

18432/50000 [==========>...................] - ETA: 10s - loss: 5.0436 - accuracy: 0.0655
on_train_batch_begin: 1615759770.766049s

9 step training time: 0.654027s

on_train_batch_end: 1615759771.417469s

20480/50000 [===========>..................] - ETA: 9s - loss: 4.9778 - accuracy: 0.0664 
on_train_batch_begin: 1615759771.417765s

10 step training time: 0.651716s

on_train_batch_end: 1615759772.071966s

22528/50000 [============>.................] - ETA: 8s - loss: 4.9116 - accuracy: 0.0676
on_train_batch_begin: 1615759772.072269s

11 step training time: 0.654504s

on_train_batch_end: 1615759772.733793s

24576/50000 [=============>................] - ETA: 8s - loss: 4.8781 - accuracy: 0.0686
on_train_batch_begin: 1615759772.734086s

12 step training time: 0.661817s

on_train_batch_end: 1615759773.386772s

26624/50000 [==============>...............] - ETA: 7s - loss: 4.8324 - accuracy: 0.0693
on_train_batch_begin: 1615759773.387069s

13 step training time: 0.652983s

on_train_batch_end: 1615759774.045270s

28672/50000 [================>.............] - ETA: 6s - loss: 4.7810 - accuracy: 0.0701
on_train_batch_begin: 1615759774.045564s

14 step training time: 0.658494s

on_train_batch_end: 1615759774.698150s

30720/50000 [=================>............] - ETA: 6s - loss: 4.7329 - accuracy: 0.0711
on_train_batch_begin: 1615759774.698445s

15 step training time: 0.652881s

on_train_batch_end: 1615759775.354060s

32768/50000 [==================>...........] - ETA: 5s - loss: 4.6759 - accuracy: 0.0720
on_train_batch_begin: 1615759775.354358s

16 step training time: 0.655912s

on_train_batch_end: 1615759776.008508s

34816/50000 [===================>..........] - ETA: 4s - loss: 4.6295 - accuracy: 0.0727
on_train_batch_begin: 1615759776.008819s

17 step training time: 0.654461s

on_train_batch_end: 1615759776.669758s

36864/50000 [=====================>........] - ETA: 4s - loss: 4.5917 - accuracy: 0.0733
on_train_batch_begin: 1615759776.670056s

18 step training time: 0.661238s

on_train_batch_end: 1615759777.323438s

38912/50000 [======================>.......] - ETA: 3s - loss: 4.5532 - accuracy: 0.0740
on_train_batch_begin: 1615759777.323732s

19 step training time: 0.653676s

on_train_batch_end: 1615759777.981003s

40960/50000 [=======================>......] - ETA: 2s - loss: 4.5107 - accuracy: 0.0746
on_train_batch_begin: 1615759777.981298s

20 step training time: 0.657566s

on_train_batch_end: 1615759778.635686s

43008/50000 [========================>.....] - ETA: 2s - loss: 4.4724 - accuracy: 0.0752
on_train_batch_begin: 1615759778.635978s

21 step training time: 0.654681s

on_train_batch_end: 1615759779.295267s

45056/50000 [==========================>...] - ETA: 1s - loss: 4.4306 - accuracy: 0.0758
on_train_batch_begin: 1615759779.295565s

22 step training time: 0.659587s

on_train_batch_end: 1615759779.951490s

47104/50000 [===========================>..] - ETA: 0s - loss: 4.3902 - accuracy: 0.0765
on_train_batch_begin: 1615759779.951788s

23 step training time: 0.656223s

on_train_batch_end: 1615759780.605628s

49152/50000 [============================>.] - ETA: 0s - loss: 4.3522 - accuracy: 0.0771
on_train_batch_begin: 1615759780.605924s

24 step training time: 0.654136s

on_train_batch_end: 1615759780.887588s

on_test_batch_begin: 1615759781.011506s

25 step training time: 0.405582s

on_epoch_end: 1615759781.930411s

Validation time: 0.918890s

Real time: 1615759781.930411s

Epoch time: 17.05652666091919s

50000/50000 [==============================] - 17s 341us/sample - loss: 4.3388 - accuracy: 0.0772 - val_loss: 10.3435 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615759781.930641s

Real time: 1615759781.9306474
Epoch 3/5

on_train_batch_begin: 1615759781.934245s

on_train_batch_end: 1615759782.588498s

 2048/50000 [>.............................] - ETA: 15s - loss: 3.4016 - accuracy: 0.0889
on_train_batch_begin: 1615759782.588887s

1 step training time: 0.654642s

on_train_batch_end: 1615759783.252764s

 4096/50000 [=>............................] - ETA: 14s - loss: 3.2969 - accuracy: 0.0921
on_train_batch_begin: 1615759783.253150s

2 step training time: 0.664263s

on_train_batch_end: 1615759783.908520s

 6144/50000 [==>...........................] - ETA: 14s - loss: 3.2611 - accuracy: 0.0927
on_train_batch_begin: 1615759783.908849s

3 step training time: 0.655699s

on_train_batch_end: 1615759784.570999s

 8192/50000 [===>..........................] - ETA: 13s - loss: 3.2743 - accuracy: 0.0927
on_train_batch_begin: 1615759784.571297s

4 step training time: 0.662449s

on_train_batch_end: 1615759785.225961s

10240/50000 [=====>........................] - ETA: 12s - loss: 3.2572 - accuracy: 0.0932
on_train_batch_begin: 1615759785.226267s

5 step training time: 0.654969s

on_train_batch_end: 1615759785.889055s

12288/50000 [======>.......................] - ETA: 12s - loss: 3.2127 - accuracy: 0.0938
on_train_batch_begin: 1615759785.889364s

6 step training time: 0.663098s

on_train_batch_end: 1615759786.544388s

14336/50000 [=======>......................] - ETA: 11s - loss: 3.2049 - accuracy: 0.0938
on_train_batch_begin: 1615759786.544689s

7 step training time: 0.655324s

on_train_batch_end: 1615759787.210026s

16384/50000 [========>.....................] - ETA: 10s - loss: 3.1848 - accuracy: 0.0943
on_train_batch_begin: 1615759787.210348s

8 step training time: 0.665659s

on_train_batch_end: 1615759787.865999s

18432/50000 [==========>...................] - ETA: 10s - loss: 3.1724 - accuracy: 0.0945
on_train_batch_begin: 1615759787.866299s

9 step training time: 0.655951s

on_train_batch_end: 1615759788.528757s

20480/50000 [===========>..................] - ETA: 9s - loss: 3.1535 - accuracy: 0.0949 
on_train_batch_begin: 1615759788.529102s

10 step training time: 0.662803s

on_train_batch_end: 1615759789.191427s

22528/50000 [============>.................] - ETA: 8s - loss: 3.1615 - accuracy: 0.0950
on_train_batch_begin: 1615759789.191736s

11 step training time: 0.662634s

on_train_batch_end: 1615759789.852460s

24576/50000 [=============>................] - ETA: 8s - loss: 3.1515 - accuracy: 0.0951
on_train_batch_begin: 1615759789.852760s

12 step training time: 0.661024s

on_train_batch_end: 1615759790.517193s

26624/50000 [==============>...............] - ETA: 7s - loss: 3.1407 - accuracy: 0.0952
on_train_batch_begin: 1615759790.517507s

13 step training time: 0.664747s

on_train_batch_end: 1615759791.180515s

28672/50000 [================>.............] - ETA: 6s - loss: 3.1411 - accuracy: 0.0952
on_train_batch_begin: 1615759791.180843s

14 step training time: 0.663337s

on_train_batch_end: 1615759791.845327s

30720/50000 [=================>............] - ETA: 6s - loss: 3.1366 - accuracy: 0.0954
on_train_batch_begin: 1615759791.845636s

15 step training time: 0.664793s

on_train_batch_end: 1615759792.506399s

32768/50000 [==================>...........] - ETA: 5s - loss: 3.1277 - accuracy: 0.0956
on_train_batch_begin: 1615759792.506712s

16 step training time: 0.661075s

on_train_batch_end: 1615759793.172427s

34816/50000 [===================>..........] - ETA: 4s - loss: 3.1170 - accuracy: 0.0959
on_train_batch_begin: 1615759793.172740s

17 step training time: 0.666028s

on_train_batch_end: 1615759793.836679s

36864/50000 [=====================>........] - ETA: 4s - loss: 3.1027 - accuracy: 0.0961
on_train_batch_begin: 1615759793.836997s

18 step training time: 0.664257s

on_train_batch_end: 1615759794.500569s

38912/50000 [======================>.......] - ETA: 3s - loss: 3.0880 - accuracy: 0.0963
on_train_batch_begin: 1615759794.500910s

19 step training time: 0.663913s

on_train_batch_end: 1615759795.161387s

40960/50000 [=======================>......] - ETA: 2s - loss: 3.0760 - accuracy: 0.0965
on_train_batch_begin: 1615759795.161679s

20 step training time: 0.660769s

on_train_batch_end: 1615759795.827871s

43008/50000 [========================>.....] - ETA: 2s - loss: 3.0717 - accuracy: 0.0966
on_train_batch_begin: 1615759795.828164s

21 step training time: 0.666485s

on_train_batch_end: 1615759796.495111s

45056/50000 [==========================>...] - ETA: 1s - loss: 3.0620 - accuracy: 0.0967
on_train_batch_begin: 1615759796.495404s

22 step training time: 0.667241s

on_train_batch_end: 1615759797.161743s

47104/50000 [===========================>..] - ETA: 0s - loss: 3.0502 - accuracy: 0.0968
on_train_batch_begin: 1615759797.162038s

23 step training time: 0.666634s

on_train_batch_end: 1615759797.822283s

49152/50000 [============================>.] - ETA: 0s - loss: 3.0340 - accuracy: 0.0969
on_train_batch_begin: 1615759797.822582s

24 step training time: 0.660544s

on_train_batch_end: 1615759798.107162s

on_test_batch_begin: 1615759798.235934s

25 step training time: 0.413352s

on_epoch_end: 1615759799.171779s

Validation time: 0.935830s

Real time: 1615759799.171779s

Epoch time: 17.24115014076233s

50000/50000 [==============================] - 17s 345us/sample - loss: 3.0295 - accuracy: 0.0969 - val_loss: 6.9952 - val_accuracy: 0.1001

on_epoch_begin: 1615759799.171972s

Real time: 1615759799.1719778
Epoch 4/5

on_train_batch_begin: 1615759799.175326s

on_train_batch_end: 1615759799.839176s

 2048/50000 [>.............................] - ETA: 15s - loss: 2.5586 - accuracy: 0.0995
on_train_batch_begin: 1615759799.839549s

1 step training time: 0.664222s

on_train_batch_end: 1615759800.506391s

 4096/50000 [=>............................] - ETA: 14s - loss: 2.6215 - accuracy: 0.0990
on_train_batch_begin: 1615759800.506756s

2 step training time: 0.667207s

on_train_batch_end: 1615759801.167831s

 6144/50000 [==>...........................] - ETA: 14s - loss: 2.6254 - accuracy: 0.0993
on_train_batch_begin: 1615759801.168202s

3 step training time: 0.661446s

on_train_batch_end: 1615759801.834499s

 8192/50000 [===>..........................] - ETA: 13s - loss: 2.6418 - accuracy: 0.0992
on_train_batch_begin: 1615759801.834865s

4 step training time: 0.666663s

on_train_batch_end: 1615759802.499190s

10240/50000 [=====>........................] - ETA: 12s - loss: 2.6646 - accuracy: 0.0991
on_train_batch_begin: 1615759802.499556s

5 step training time: 0.664691s

on_train_batch_end: 1615759803.165752s

12288/50000 [======>.......................] - ETA: 12s - loss: 2.6860 - accuracy: 0.0993
on_train_batch_begin: 1615759803.166118s

6 step training time: 0.666562s

on_train_batch_end: 1615759803.831529s

14336/50000 [=======>......................] - ETA: 11s - loss: 2.6773 - accuracy: 0.0993
on_train_batch_begin: 1615759803.831896s

7 step training time: 0.665778s

on_train_batch_end: 1615759804.496156s

16384/50000 [========>.....................] - ETA: 10s - loss: 2.6777 - accuracy: 0.0992
on_train_batch_begin: 1615759804.496536s

8 step training time: 0.664640s

on_train_batch_end: 1615759805.166888s

18432/50000 [==========>...................] - ETA: 10s - loss: 2.6789 - accuracy: 0.0992
on_train_batch_begin: 1615759805.167254s

9 step training time: 0.670717s

on_train_batch_end: 1615759805.835565s

20480/50000 [===========>..................] - ETA: 9s - loss: 2.6731 - accuracy: 0.0993 
on_train_batch_begin: 1615759805.835949s

10 step training time: 0.668695s

on_train_batch_end: 1615759806.505754s

22528/50000 [============>.................] - ETA: 8s - loss: 2.6678 - accuracy: 0.0994
on_train_batch_begin: 1615759806.506108s

11 step training time: 0.670159s

on_train_batch_end: 1615759807.176459s

24576/50000 [=============>................] - ETA: 8s - loss: 2.6701 - accuracy: 0.0994
on_train_batch_begin: 1615759807.176865s

12 step training time: 0.670757s

on_train_batch_end: 1615759807.843448s

26624/50000 [==============>...............] - ETA: 7s - loss: 2.6699 - accuracy: 0.0994
on_train_batch_begin: 1615759807.843813s

13 step training time: 0.666948s

on_train_batch_end: 1615759808.511745s

28672/50000 [================>.............] - ETA: 6s - loss: 2.6655 - accuracy: 0.0995
on_train_batch_begin: 1615759808.512117s

14 step training time: 0.668304s

on_train_batch_end: 1615759809.183855s

30720/50000 [=================>............] - ETA: 6s - loss: 2.6612 - accuracy: 0.0995
on_train_batch_begin: 1615759809.184237s

15 step training time: 0.672120s

on_train_batch_end: 1615759809.849272s

32768/50000 [==================>...........] - ETA: 5s - loss: 2.6573 - accuracy: 0.0995
on_train_batch_begin: 1615759809.849647s

16 step training time: 0.665410s

on_train_batch_end: 1615759810.515319s

34816/50000 [===================>..........] - ETA: 4s - loss: 2.6544 - accuracy: 0.0996
on_train_batch_begin: 1615759810.515685s

17 step training time: 0.666039s

on_train_batch_end: 1615759811.186574s

36864/50000 [=====================>........] - ETA: 4s - loss: 2.6556 - accuracy: 0.0996
on_train_batch_begin: 1615759811.186941s

18 step training time: 0.671256s

on_train_batch_end: 1615759811.850384s

38912/50000 [======================>.......] - ETA: 3s - loss: 2.6514 - accuracy: 0.0997
on_train_batch_begin: 1615759811.850758s

19 step training time: 0.663817s

on_train_batch_end: 1615759812.519697s

40960/50000 [=======================>......] - ETA: 2s - loss: 2.6460 - accuracy: 0.0997
on_train_batch_begin: 1615759812.520072s

20 step training time: 0.669314s

on_train_batch_end: 1615759813.188527s

43008/50000 [========================>.....] - ETA: 2s - loss: 2.6322 - accuracy: 0.0997
on_train_batch_begin: 1615759813.188943s

21 step training time: 0.668871s

on_train_batch_end: 1615759813.858907s

45056/50000 [==========================>...] - ETA: 1s - loss: 2.6190 - accuracy: 0.0997
on_train_batch_begin: 1615759813.859277s

22 step training time: 0.670335s

on_train_batch_end: 1615759814.530034s

47104/50000 [===========================>..] - ETA: 0s - loss: 2.6132 - accuracy: 0.0997
on_train_batch_begin: 1615759814.530437s

23 step training time: 0.671160s

on_train_batch_end: 1615759815.194637s

49152/50000 [============================>.] - ETA: 0s - loss: 2.6074 - accuracy: 0.0998
on_train_batch_begin: 1615759815.195013s

24 step training time: 0.664575s

on_train_batch_end: 1615759815.474959s

on_test_batch_begin: 1615759815.604353s

25 step training time: 0.409341s

on_epoch_end: 1615759816.557900s

Validation time: 0.953526s

Real time: 1615759816.557900s

Epoch time: 17.385942935943604s

50000/50000 [==============================] - 17s 348us/sample - loss: 2.6060 - accuracy: 0.0998 - val_loss: 7.3905 - val_accuracy: 0.1000

on_epoch_begin: 1615759816.558116s

Real time: 1615759816.5581217
Epoch 5/5

on_train_batch_begin: 1615759816.561627s

on_train_batch_end: 1615759817.233250s

 2048/50000 [>.............................] - ETA: 15s - loss: 2.3536 - accuracy: 0.0996
on_train_batch_begin: 1615759817.233617s

1 step training time: 0.671989s

on_train_batch_end: 1615759817.909798s

 4096/50000 [=>............................] - ETA: 15s - loss: 2.3356 - accuracy: 0.0997
on_train_batch_begin: 1615759817.910175s

2 step training time: 0.676558s

on_train_batch_end: 1615759818.580951s

 6144/50000 [==>...........................] - ETA: 14s - loss: 2.3079 - accuracy: 0.0997
on_train_batch_begin: 1615759818.581321s

3 step training time: 0.671146s

on_train_batch_end: 1615759819.258252s

 8192/50000 [===>..........................] - ETA: 13s - loss: 2.3291 - accuracy: 0.0999
on_train_batch_begin: 1615759819.258623s

4 step training time: 0.677302s

on_train_batch_end: 1615759819.933416s

10240/50000 [=====>........................] - ETA: 13s - loss: 2.3343 - accuracy: 0.0999
on_train_batch_begin: 1615759819.933787s

5 step training time: 0.675164s

on_train_batch_end: 1615759820.605005s

12288/50000 [======>.......................] - ETA: 12s - loss: 2.3097 - accuracy: 0.0999
on_train_batch_begin: 1615759820.605377s

6 step training time: 0.671590s

on_train_batch_end: 1615759821.280864s

14336/50000 [=======>......................] - ETA: 11s - loss: 2.3081 - accuracy: 0.0999
on_train_batch_begin: 1615759821.281235s

7 step training time: 0.675858s

on_train_batch_end: 1615759821.955432s

16384/50000 [========>.....................] - ETA: 11s - loss: 2.3052 - accuracy: 0.0999
on_train_batch_begin: 1615759821.955814s

8 step training time: 0.674580s

on_train_batch_end: 1615759822.631215s

18432/50000 [==========>...................] - ETA: 10s - loss: 2.2961 - accuracy: 0.0999
on_train_batch_begin: 1615759822.631593s

9 step training time: 0.675778s

on_train_batch_end: 1615759823.307373s

20480/50000 [===========>..................] - ETA: 9s - loss: 2.3020 - accuracy: 0.0999 
on_train_batch_begin: 1615759823.307752s

10 step training time: 0.676159s

on_train_batch_end: 1615759823.981940s

22528/50000 [============>.................] - ETA: 9s - loss: 2.2898 - accuracy: 0.0999
on_train_batch_begin: 1615759823.982317s

11 step training time: 0.674565s

on_train_batch_end: 1615759824.661572s

24576/50000 [=============>................] - ETA: 8s - loss: 2.2789 - accuracy: 0.0999
on_train_batch_begin: 1615759824.661949s

12 step training time: 0.679632s

on_train_batch_end: 1615759825.336344s

26624/50000 [==============>...............] - ETA: 7s - loss: 2.2559 - accuracy: 0.0999
on_train_batch_begin: 1615759825.336722s

13 step training time: 0.674773s

on_train_batch_end: 1615759826.007228s

28672/50000 [================>.............] - ETA: 7s - loss: 2.2493 - accuracy: 0.1000
on_train_batch_begin: 1615759826.007596s

14 step training time: 0.670873s

on_train_batch_end: 1615759826.686493s

30720/50000 [=================>............] - ETA: 6s - loss: 2.2418 - accuracy: 0.0999
on_train_batch_begin: 1615759826.686865s

15 step training time: 0.679269s

on_train_batch_end: 1615759827.358507s

32768/50000 [==================>...........] - ETA: 5s - loss: 2.2244 - accuracy: 0.0999
on_train_batch_begin: 1615759827.358871s

16 step training time: 0.672006s

on_train_batch_end: 1615759828.035807s

34816/50000 [===================>..........] - ETA: 5s - loss: 2.2005 - accuracy: 0.0999
on_train_batch_begin: 1615759828.036176s

17 step training time: 0.677305s

on_train_batch_end: 1615759828.710477s

36864/50000 [=====================>........] - ETA: 4s - loss: 2.1808 - accuracy: 0.1000
on_train_batch_begin: 1615759828.710841s

18 step training time: 0.674665s

on_train_batch_end: 1615759829.388513s

38912/50000 [======================>.......] - ETA: 3s - loss: 2.1708 - accuracy: 0.1000
on_train_batch_begin: 1615759829.388902s

19 step training time: 0.678060s

on_train_batch_end: 1615759830.065199s

40960/50000 [=======================>......] - ETA: 2s - loss: 2.1577 - accuracy: 0.1000
on_train_batch_begin: 1615759830.065571s

20 step training time: 0.676669s

on_train_batch_end: 1615759830.747473s

43008/50000 [========================>.....] - ETA: 2s - loss: 2.1455 - accuracy: 0.1000
on_train_batch_begin: 1615759830.747841s

21 step training time: 0.682271s

on_train_batch_end: 1615759831.424688s

45056/50000 [==========================>...] - ETA: 1s - loss: 2.1345 - accuracy: 0.1000
on_train_batch_begin: 1615759831.425082s

22 step training time: 0.677241s

on_train_batch_end: 1615759832.102135s

47104/50000 [===========================>..] - ETA: 0s - loss: 2.1192 - accuracy: 0.1000
on_train_batch_begin: 1615759832.102516s

23 step training time: 0.677433s

on_train_batch_end: 1615759832.774222s

49152/50000 [============================>.] - ETA: 0s - loss: 2.1077 - accuracy: 0.1000
on_train_batch_begin: 1615759832.774595s

24 step training time: 0.672080s

on_train_batch_end: 1615759833.059840s

on_test_batch_begin: 1615759833.191348s

25 step training time: 0.416753s

on_epoch_end: 1615759834.127318s

Validation time: 0.935950s

Real time: 1615759834.127318s

Epoch time: 17.56921696662903s

50000/50000 [==============================] - 18s 351us/sample - loss: 2.1041 - accuracy: 0.1000 - val_loss: 7.6614 - val_accuracy: 0.1001
Tempo do fit: 121.59416627883911