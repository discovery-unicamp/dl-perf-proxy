wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:49
   139264/170498071 [..............................] - ETA: 1:32
   565248/170498071 [..............................] - ETA: 37s 
  1384448/170498071 [..............................] - ETA: 21s
  2351104/170498071 [..............................] - ETA: 16s
  3268608/170498071 [..............................] - ETA: 14s
  4349952/170498071 [..............................] - ETA: 12s
  5627904/170498071 [..............................] - ETA: 11s
  6955008/170498071 [>.............................] - ETA: 10s
  8265728/170498071 [>.............................] - ETA: 9s 
  9658368/170498071 [>.............................] - ETA: 8s
 11354112/170498071 [>.............................] - ETA: 8s
 13148160/170498071 [=>............................] - ETA: 7s
 14376960/170498071 [=>............................] - ETA: 7s
 16457728/170498071 [=>............................] - ETA: 6s
 18358272/170498071 [==>...........................] - ETA: 6s
 20258816/170498071 [==>...........................] - ETA: 6s
 21553152/170498071 [==>...........................] - ETA: 6s
 24068096/170498071 [===>..........................] - ETA: 5s
 26501120/170498071 [===>..........................] - ETA: 5s
 28147712/170498071 [===>..........................] - ETA: 5s
 30089216/170498071 [====>.........................] - ETA: 5s
 32727040/170498071 [====>.........................] - ETA: 4s
 35037184/170498071 [=====>........................] - ETA: 4s
 36921344/170498071 [=====>........................] - ETA: 4s
 39608320/170498071 [=====>........................] - ETA: 4s
 42237952/170498071 [======>.......................] - ETA: 4s
 44343296/170498071 [======>.......................] - ETA: 3s
 46424064/170498071 [=======>......................] - ETA: 3s
 49250304/170498071 [=======>......................] - ETA: 3s
 51945472/170498071 [========>.....................] - ETA: 3s
 54042624/170498071 [========>.....................] - ETA: 3s
 56279040/170498071 [========>.....................] - ETA: 3s
 58843136/170498071 [=========>....................] - ETA: 3s
 61513728/170498071 [=========>....................] - ETA: 3s
 63528960/170498071 [==========>...................] - ETA: 3s
 66134016/170498071 [==========>...................] - ETA: 2s
 69017600/170498071 [===========>..................] - ETA: 2s
 71213056/170498071 [===========>..................] - ETA: 2s
 73326592/170498071 [===========>..................] - ETA: 2s
 75898880/170498071 [============>.................] - ETA: 2s
 78897152/170498071 [============>.................] - ETA: 2s
 80855040/170498071 [=============>................] - ETA: 2s
 83189760/170498071 [=============>................] - ETA: 2s
 86024192/170498071 [==============>...............] - ETA: 2s
 88449024/170498071 [==============>...............] - ETA: 2s
 90415104/170498071 [==============>...............] - ETA: 2s
 93151232/170498071 [===============>..............] - ETA: 1s
 96231424/170498071 [===============>..............] - ETA: 1s
 98344960/170498071 [================>.............] - ETA: 1s
100360192/170498071 [================>.............] - ETA: 1s
103112704/170498071 [=================>............] - ETA: 1s
105701376/170498071 [=================>............] - ETA: 1s
107651072/170498071 [=================>............] - ETA: 1s
110198784/170498071 [==================>...........] - ETA: 1s
113188864/170498071 [==================>...........] - ETA: 1s
115204096/170498071 [===================>..........] - ETA: 1s
117284864/170498071 [===================>..........] - ETA: 1s
120020992/170498071 [====================>.........] - ETA: 1s
122494976/170498071 [====================>.........] - ETA: 1s
124248064/170498071 [====================>.........] - ETA: 1s
126459904/170498071 [=====================>........] - ETA: 1s
129449984/170498071 [=====================>........] - ETA: 0s
132259840/170498071 [======================>.......] - ETA: 0s
135307264/170498071 [======================>.......] - ETA: 0s
138698752/170498071 [=======================>......] - ETA: 0s
141877248/170498071 [=======================>......] - ETA: 0s
144809984/170498071 [========================>.....] - ETA: 0s
148094976/170498071 [=========================>....] - ETA: 0s
151363584/170498071 [=========================>....] - ETA: 0s
154247168/170498071 [==========================>...] - ETA: 0s
157048832/170498071 [==========================>...] - ETA: 0s
160407552/170498071 [===========================>..] - ETA: 0s
163733504/170498071 [===========================>..] - ETA: 0s
166543360/170498071 [============================>.] - ETA: 0s
169172992/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 4s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 1s
 3465216/94765736 [>.............................] - ETA: 1s
10010624/94765736 [==>...........................] - ETA: 0s
18202624/94765736 [====>.........................] - ETA: 0s
25804800/94765736 [=======>......................] - ETA: 0s
30597120/94765736 [========>.....................] - ETA: 0s
36937728/94765736 [==========>...................] - ETA: 0s
39354368/94765736 [===========>..................] - ETA: 0s
44613632/94765736 [=============>................] - ETA: 0s
50814976/94765736 [===============>..............] - ETA: 0s
56647680/94765736 [================>.............] - ETA: 0s
61259776/94765736 [==================>...........] - ETA: 0s
65986560/94765736 [===================>..........] - ETA: 0s
70221824/94765736 [=====================>........] - ETA: 0s
77864960/94765736 [=======================>......] - ETA: 0s
85745664/94765736 [==========================>...] - ETA: 0s
91570176/94765736 [===========================>..] - ETA: 0s
94773248/94765736 [==============================] - 1s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 28.75985598564148
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1607800463.460072s

Real time: 1607800463.4600952
Epoch 1/5

on_train_batch_begin: 1607800464.506961s

on_train_batch_end: 1607800607.195411s

 2048/50000 [>.............................] - ETA: 56:05 - loss: 17.2037 - accuracy: 2.6512e-04
on_train_batch_begin: 1607800607.196233s

1 step training time: 142.689273s

on_train_batch_end: 1607800607.555733s

 4096/50000 [=>............................] - ETA: 26:54 - loss: 15.0552 - accuracy: 3.5667e-04
on_train_batch_begin: 1607800607.556190s

2 step training time: 0.359957s

on_train_batch_end: 1607800607.913716s

 6144/50000 [==>...........................] - ETA: 17:11 - loss: 13.0742 - accuracy: 5.1816e-04
on_train_batch_begin: 1607800607.914124s

3 step training time: 0.357934s

on_train_batch_end: 1607800608.269064s

 8192/50000 [===>..........................] - ETA: 12:19 - loss: 11.8877 - accuracy: 9.0075e-04
on_train_batch_begin: 1607800608.269475s

4 step training time: 0.355350s

on_train_batch_end: 1607800608.624329s

10240/50000 [=====>........................] - ETA: 9:23 - loss: 11.0954 - accuracy: 0.0031     
on_train_batch_begin: 1607800608.624738s

5 step training time: 0.355264s

on_train_batch_end: 1607800608.981569s

12288/50000 [======>.......................] - ETA: 7:26 - loss: 10.5442 - accuracy: 0.0049
on_train_batch_begin: 1607800608.981982s

6 step training time: 0.357243s

on_train_batch_end: 1607800609.336179s

14336/50000 [=======>......................] - ETA: 6:02 - loss: 10.1003 - accuracy: 0.0086
on_train_batch_begin: 1607800609.336578s

7 step training time: 0.354597s

on_train_batch_end: 1607800609.693149s

16384/50000 [========>.....................] - ETA: 5:00 - loss: 9.7600 - accuracy: 0.0126 
on_train_batch_begin: 1607800609.693559s

8 step training time: 0.356981s

on_train_batch_end: 1607800610.049491s

18432/50000 [==========>...................] - ETA: 4:11 - loss: 9.4765 - accuracy: 0.0173
on_train_batch_begin: 1607800610.049896s

9 step training time: 0.356337s

on_train_batch_end: 1607800610.406229s

20480/50000 [===========>..................] - ETA: 3:31 - loss: 9.2254 - accuracy: 0.0218
on_train_batch_begin: 1607800610.406639s

10 step training time: 0.356743s

on_train_batch_end: 1607800610.763766s

22528/50000 [============>.................] - ETA: 2:59 - loss: 9.0173 - accuracy: 0.0261
on_train_batch_begin: 1607800610.764204s

11 step training time: 0.357565s

on_train_batch_end: 1607800611.120803s

24576/50000 [=============>................] - ETA: 2:32 - loss: 8.8415 - accuracy: 0.0302
on_train_batch_begin: 1607800611.121209s

12 step training time: 0.357004s

on_train_batch_end: 1607800611.475309s

26624/50000 [==============>...............] - ETA: 2:09 - loss: 8.6909 - accuracy: 0.0342
on_train_batch_begin: 1607800611.475712s

13 step training time: 0.354503s

on_train_batch_end: 1607800611.829450s

28672/50000 [================>.............] - ETA: 1:50 - loss: 8.5468 - accuracy: 0.0375
on_train_batch_begin: 1607800611.829858s

14 step training time: 0.354146s

on_train_batch_end: 1607800612.184626s

30720/50000 [=================>............] - ETA: 1:33 - loss: 8.4181 - accuracy: 0.0404
on_train_batch_begin: 1607800612.185032s

15 step training time: 0.355175s

on_train_batch_end: 1607800612.540455s

32768/50000 [==================>...........] - ETA: 1:18 - loss: 8.3058 - accuracy: 0.0432
on_train_batch_begin: 1607800612.540865s

16 step training time: 0.355833s

on_train_batch_end: 1607800612.897141s

34816/50000 [===================>..........] - ETA: 1:05 - loss: 8.2105 - accuracy: 0.0456
on_train_batch_begin: 1607800612.897551s

17 step training time: 0.356686s

on_train_batch_end: 1607800613.252105s

36864/50000 [=====================>........] - ETA: 53s - loss: 8.1109 - accuracy: 0.0476 
on_train_batch_begin: 1607800613.252513s

18 step training time: 0.354962s

on_train_batch_end: 1607800613.610281s

38912/50000 [======================>.......] - ETA: 42s - loss: 8.0202 - accuracy: 0.0495
on_train_batch_begin: 1607800613.610693s

19 step training time: 0.358179s

on_train_batch_end: 1607800613.967256s

40960/50000 [=======================>......] - ETA: 33s - loss: 7.9307 - accuracy: 0.0511
on_train_batch_begin: 1607800613.967656s

20 step training time: 0.356964s

on_train_batch_end: 1607800614.323217s

43008/50000 [========================>.....] - ETA: 24s - loss: 7.8374 - accuracy: 0.0525
on_train_batch_begin: 1607800614.323622s

21 step training time: 0.355965s

on_train_batch_end: 1607800614.680570s

45056/50000 [==========================>...] - ETA: 16s - loss: 7.7565 - accuracy: 0.0537
on_train_batch_begin: 1607800614.680972s

22 step training time: 0.357350s

on_train_batch_end: 1607800615.038024s

47104/50000 [===========================>..] - ETA: 9s - loss: 7.6726 - accuracy: 0.0548 
on_train_batch_begin: 1607800615.038425s

23 step training time: 0.357454s

on_train_batch_end: 1607800615.392624s

49152/50000 [============================>.] - ETA: 2s - loss: 7.5915 - accuracy: 0.0560
on_train_batch_begin: 1607800615.393031s

24 step training time: 0.354605s

on_train_batch_end: 1607800622.727381s

on_test_batch_begin: 1607800623.124511s

25 step training time: 7.731480s

on_epoch_end: 1607800642.028139s

Validation time: 18.903606s

Real time: 1607800642.028139s

Epoch time: 178.56807136535645s

50000/50000 [==============================] - 179s 4ms/sample - loss: 7.5611 - accuracy: 0.0561 - val_loss: 11484.1112 - val_accuracy: 0.1001

on_epoch_begin: 1607800642.028423s

Real time: 1607800642.0284324
Epoch 2/5

on_train_batch_begin: 1607800642.036446s

on_train_batch_end: 1607800642.314571s

 2048/50000 [>.............................] - ETA: 6s - loss: 5.6720 - accuracy: 0.0744
on_train_batch_begin: 1607800642.314973s

1 step training time: 0.278527s

on_train_batch_end: 1607800642.592328s

 4096/50000 [=>............................] - ETA: 6s - loss: 5.6426 - accuracy: 0.0683
on_train_batch_begin: 1607800642.592718s

2 step training time: 0.277745s

on_train_batch_end: 1607800642.871744s

 6144/50000 [==>...........................] - ETA: 6s - loss: 5.5922 - accuracy: 0.0677
on_train_batch_begin: 1607800642.872171s

3 step training time: 0.279453s

on_train_batch_end: 1607800643.150686s

 8192/50000 [===>..........................] - ETA: 5s - loss: 5.5281 - accuracy: 0.0691
on_train_batch_begin: 1607800643.151079s

4 step training time: 0.278908s

on_train_batch_end: 1607800643.429455s

10240/50000 [=====>........................] - ETA: 5s - loss: 5.4806 - accuracy: 0.0701
on_train_batch_begin: 1607800643.429844s

5 step training time: 0.278765s

on_train_batch_end: 1607800643.784595s

12288/50000 [======>.......................] - ETA: 5s - loss: 5.4391 - accuracy: 0.0702
on_train_batch_begin: 1607800643.784988s

6 step training time: 0.355144s

on_train_batch_end: 1607800644.136873s

14336/50000 [=======>......................] - ETA: 5s - loss: 5.3954 - accuracy: 0.0706
on_train_batch_begin: 1607800644.137267s

7 step training time: 0.352279s

on_train_batch_end: 1607800644.491073s

16384/50000 [========>.....................] - ETA: 5s - loss: 5.3444 - accuracy: 0.0709
on_train_batch_begin: 1607800644.491468s

8 step training time: 0.354201s

on_train_batch_end: 1607800644.844821s

18432/50000 [==========>...................] - ETA: 4s - loss: 5.3106 - accuracy: 0.0710
on_train_batch_begin: 1607800644.845212s

9 step training time: 0.353744s

on_train_batch_end: 1607800645.199510s

20480/50000 [===========>..................] - ETA: 4s - loss: 5.2676 - accuracy: 0.0710
on_train_batch_begin: 1607800645.199932s

10 step training time: 0.354721s

on_train_batch_end: 1607800645.553667s

22528/50000 [============>.................] - ETA: 4s - loss: 5.2182 - accuracy: 0.0713
on_train_batch_begin: 1607800645.554060s

11 step training time: 0.354128s

on_train_batch_end: 1607800645.909791s

24576/50000 [=============>................] - ETA: 4s - loss: 5.1779 - accuracy: 0.0714
on_train_batch_begin: 1607800645.910211s

12 step training time: 0.356151s

on_train_batch_end: 1607800646.263385s

26624/50000 [==============>...............] - ETA: 3s - loss: 5.1286 - accuracy: 0.0717
on_train_batch_begin: 1607800646.263808s

13 step training time: 0.353597s

on_train_batch_end: 1607800646.619195s

28672/50000 [================>.............] - ETA: 3s - loss: 5.0882 - accuracy: 0.0721
on_train_batch_begin: 1607800646.619593s

14 step training time: 0.355785s

on_train_batch_end: 1607800646.971595s

30720/50000 [=================>............] - ETA: 3s - loss: 5.0357 - accuracy: 0.0725
on_train_batch_begin: 1607800646.972025s

15 step training time: 0.352432s

on_train_batch_end: 1607800647.321964s

32768/50000 [==================>...........] - ETA: 2s - loss: 4.9877 - accuracy: 0.0729
on_train_batch_begin: 1607800647.322361s

16 step training time: 0.350336s

on_train_batch_end: 1607800647.673049s

34816/50000 [===================>..........] - ETA: 2s - loss: 4.9357 - accuracy: 0.0734
on_train_batch_begin: 1607800647.673441s

17 step training time: 0.351080s

on_train_batch_end: 1607800648.022783s

36864/50000 [=====================>........] - ETA: 2s - loss: 4.8829 - accuracy: 0.0740
on_train_batch_begin: 1607800648.023177s

18 step training time: 0.349736s

on_train_batch_end: 1607800648.379626s

38912/50000 [======================>.......] - ETA: 1s - loss: 4.8403 - accuracy: 0.0744
on_train_batch_begin: 1607800648.380050s

19 step training time: 0.356873s

on_train_batch_end: 1607800648.737801s

40960/50000 [=======================>......] - ETA: 1s - loss: 4.7955 - accuracy: 0.0749
on_train_batch_begin: 1607800648.738196s

20 step training time: 0.358146s

on_train_batch_end: 1607800649.095606s

43008/50000 [========================>.....] - ETA: 1s - loss: 4.7492 - accuracy: 0.0754
on_train_batch_begin: 1607800649.096030s

21 step training time: 0.357835s

on_train_batch_end: 1607800649.454807s

45056/50000 [==========================>...] - ETA: 0s - loss: 4.6978 - accuracy: 0.0758
on_train_batch_begin: 1607800649.455205s

22 step training time: 0.359174s

on_train_batch_end: 1607800649.812850s

47104/50000 [===========================>..] - ETA: 0s - loss: 4.6502 - accuracy: 0.0764
on_train_batch_begin: 1607800649.813271s

23 step training time: 0.358066s

on_train_batch_end: 1607800650.164942s

49152/50000 [============================>.] - ETA: 0s - loss: 4.6031 - accuracy: 0.0768
on_train_batch_begin: 1607800650.165340s

24 step training time: 0.352069s

on_train_batch_end: 1607800650.403911s

on_test_batch_begin: 1607800650.503855s

25 step training time: 0.338515s

on_epoch_end: 1607800650.920909s

Validation time: 0.417032s

Real time: 1607800650.920909s

Epoch time: 8.892500638961792s

50000/50000 [==============================] - 9s 178us/sample - loss: 4.5853 - accuracy: 0.0769 - val_loss: 268.8594 - val_accuracy: 0.0000e+00

on_epoch_begin: 1607800650.921188s

Real time: 1607800650.9211974
Epoch 3/5

on_train_batch_begin: 1607800650.929335s

on_train_batch_end: 1607800651.289500s

 2048/50000 [>.............................] - ETA: 8s - loss: 3.1821 - accuracy: 0.0919
on_train_batch_begin: 1607800651.289907s

1 step training time: 0.360572s

on_train_batch_end: 1607800651.645711s

 4096/50000 [=>............................] - ETA: 8s - loss: 3.1915 - accuracy: 0.0914
on_train_batch_begin: 1607800651.646120s

2 step training time: 0.356213s

on_train_batch_end: 1607800652.006004s

 6144/50000 [==>...........................] - ETA: 7s - loss: 3.1396 - accuracy: 0.0920
on_train_batch_begin: 1607800652.006409s

3 step training time: 0.360289s

on_train_batch_end: 1607800652.362671s

 8192/50000 [===>..........................] - ETA: 7s - loss: 3.1237 - accuracy: 0.0922
on_train_batch_begin: 1607800652.363069s

4 step training time: 0.356660s

on_train_batch_end: 1607800652.719204s

10240/50000 [=====>........................] - ETA: 6s - loss: 3.1082 - accuracy: 0.0927
on_train_batch_begin: 1607800652.719611s

5 step training time: 0.356543s

on_train_batch_end: 1607800653.077554s

12288/50000 [======>.......................] - ETA: 6s - loss: 3.0674 - accuracy: 0.0933
on_train_batch_begin: 1607800653.077950s

6 step training time: 0.358338s

on_train_batch_end: 1607800653.436090s

14336/50000 [=======>......................] - ETA: 6s - loss: 3.0605 - accuracy: 0.0936
on_train_batch_begin: 1607800653.436495s

7 step training time: 0.358545s

on_train_batch_end: 1607800653.795161s

16384/50000 [========>.....................] - ETA: 5s - loss: 3.0510 - accuracy: 0.0941
on_train_batch_begin: 1607800653.795610s

8 step training time: 0.359115s

on_train_batch_end: 1607800654.151435s

18432/50000 [==========>...................] - ETA: 5s - loss: 3.0160 - accuracy: 0.0946
on_train_batch_begin: 1607800654.151855s

9 step training time: 0.356246s

on_train_batch_end: 1607800654.509603s

20480/50000 [===========>..................] - ETA: 5s - loss: 2.9888 - accuracy: 0.0949
on_train_batch_begin: 1607800654.510010s

10 step training time: 0.358155s

on_train_batch_end: 1607800654.868127s

22528/50000 [============>.................] - ETA: 4s - loss: 2.9577 - accuracy: 0.0952
on_train_batch_begin: 1607800654.868528s

11 step training time: 0.358518s

on_train_batch_end: 1607800655.226845s

24576/50000 [=============>................] - ETA: 4s - loss: 2.9211 - accuracy: 0.0955
on_train_batch_begin: 1607800655.227252s

12 step training time: 0.358725s

on_train_batch_end: 1607800655.586465s

26624/50000 [==============>...............] - ETA: 4s - loss: 2.8950 - accuracy: 0.0959
on_train_batch_begin: 1607800655.586864s

13 step training time: 0.359612s

on_train_batch_end: 1607800655.946018s

28672/50000 [================>.............] - ETA: 3s - loss: 2.8616 - accuracy: 0.0962
on_train_batch_begin: 1607800655.946415s

14 step training time: 0.359552s

on_train_batch_end: 1607800656.302384s

30720/50000 [=================>............] - ETA: 3s - loss: 2.8268 - accuracy: 0.0964
on_train_batch_begin: 1607800656.302778s

15 step training time: 0.356363s

on_train_batch_end: 1607800656.659742s

32768/50000 [==================>...........] - ETA: 3s - loss: 2.7956 - accuracy: 0.0967
on_train_batch_begin: 1607800656.660165s

16 step training time: 0.357387s

on_train_batch_end: 1607800657.015229s

34816/50000 [===================>..........] - ETA: 2s - loss: 2.7711 - accuracy: 0.0969
on_train_batch_begin: 1607800657.015622s

17 step training time: 0.355457s

on_train_batch_end: 1607800657.371792s

36864/50000 [=====================>........] - ETA: 2s - loss: 2.7441 - accuracy: 0.0970
on_train_batch_begin: 1607800657.372182s

18 step training time: 0.356560s

on_train_batch_end: 1607800657.729707s

38912/50000 [======================>.......] - ETA: 1s - loss: 2.7250 - accuracy: 0.0972
on_train_batch_begin: 1607800657.730095s

19 step training time: 0.357914s

on_train_batch_end: 1607800658.087280s

40960/50000 [=======================>......] - ETA: 1s - loss: 2.7004 - accuracy: 0.0974
on_train_batch_begin: 1607800658.087672s

20 step training time: 0.357577s

on_train_batch_end: 1607800658.445960s

43008/50000 [========================>.....] - ETA: 1s - loss: 2.6773 - accuracy: 0.0975
on_train_batch_begin: 1607800658.446353s

21 step training time: 0.358681s

on_train_batch_end: 1607800658.804179s

45056/50000 [==========================>...] - ETA: 0s - loss: 2.6584 - accuracy: 0.0977
on_train_batch_begin: 1607800658.804575s

22 step training time: 0.358222s

on_train_batch_end: 1607800659.162372s

47104/50000 [===========================>..] - ETA: 0s - loss: 2.6379 - accuracy: 0.0978
on_train_batch_begin: 1607800659.162762s

23 step training time: 0.358187s

on_train_batch_end: 1607800659.517180s

49152/50000 [============================>.] - ETA: 0s - loss: 2.6227 - accuracy: 0.0979
on_train_batch_begin: 1607800659.517570s

24 step training time: 0.354808s

on_train_batch_end: 1607800659.754938s

on_test_batch_begin: 1607800659.850040s

25 step training time: 0.332469s

on_epoch_end: 1607800660.260393s

Validation time: 0.410335s

Real time: 1607800660.260393s

Epoch time: 9.339221239089966s

50000/50000 [==============================] - 9s 187us/sample - loss: 2.6145 - accuracy: 0.0979 - val_loss: 11.7171 - val_accuracy: 0.0000e+00

on_epoch_begin: 1607800660.260669s

Real time: 1607800660.260678
Epoch 4/5

on_train_batch_begin: 1607800660.268946s

on_train_batch_end: 1607800660.628914s

 2048/50000 [>.............................] - ETA: 8s - loss: 2.0342 - accuracy: 0.1008
on_train_batch_begin: 1607800660.629304s

1 step training time: 0.360358s

on_train_batch_end: 1607800660.988435s

 4096/50000 [=>............................] - ETA: 8s - loss: 2.0639 - accuracy: 0.1010
on_train_batch_begin: 1607800660.988821s

2 step training time: 0.359517s

on_train_batch_end: 1607800661.343682s

 6144/50000 [==>...........................] - ETA: 7s - loss: 2.0234 - accuracy: 0.1011
on_train_batch_begin: 1607800661.344103s

3 step training time: 0.355283s

on_train_batch_end: 1607800661.695650s

 8192/50000 [===>..........................] - ETA: 7s - loss: 1.9990 - accuracy: 0.1013
on_train_batch_begin: 1607800661.696062s

4 step training time: 0.351959s

on_train_batch_end: 1607800662.053106s

10240/50000 [=====>........................] - ETA: 6s - loss: 1.9738 - accuracy: 0.1013
on_train_batch_begin: 1607800662.053498s

5 step training time: 0.357435s

on_train_batch_end: 1607800662.406684s

12288/50000 [======>.......................] - ETA: 6s - loss: 1.9760 - accuracy: 0.1013
on_train_batch_begin: 1607800662.407070s

6 step training time: 0.353572s

on_train_batch_end: 1607800662.761741s

14336/50000 [=======>......................] - ETA: 6s - loss: 1.9586 - accuracy: 0.1013
on_train_batch_begin: 1607800662.762125s

7 step training time: 0.355055s

on_train_batch_end: 1607800663.118745s

16384/50000 [========>.....................] - ETA: 5s - loss: 1.9432 - accuracy: 0.1014
on_train_batch_begin: 1607800663.119128s

8 step training time: 0.357003s

on_train_batch_end: 1607800663.474141s

18432/50000 [==========>...................] - ETA: 5s - loss: 1.9402 - accuracy: 0.1014
on_train_batch_begin: 1607800663.474526s

9 step training time: 0.355398s

on_train_batch_end: 1607800663.830942s

20480/50000 [===========>..................] - ETA: 5s - loss: 1.9487 - accuracy: 0.1013
on_train_batch_begin: 1607800663.831319s

10 step training time: 0.356794s

on_train_batch_end: 1607800664.188022s

22528/50000 [============>.................] - ETA: 4s - loss: 1.9461 - accuracy: 0.1012
on_train_batch_begin: 1607800664.188400s

11 step training time: 0.357081s

on_train_batch_end: 1607800664.546374s

24576/50000 [=============>................] - ETA: 4s - loss: 1.9293 - accuracy: 0.1013
on_train_batch_begin: 1607800664.546821s

12 step training time: 0.358421s

on_train_batch_end: 1607800664.903430s

26624/50000 [==============>...............] - ETA: 4s - loss: 1.9222 - accuracy: 0.1012
on_train_batch_begin: 1607800664.903839s

13 step training time: 0.357018s

on_train_batch_end: 1607800665.259801s

28672/50000 [================>.............] - ETA: 3s - loss: 1.9153 - accuracy: 0.1012
on_train_batch_begin: 1607800665.260183s

14 step training time: 0.356345s

on_train_batch_end: 1607800665.613288s

30720/50000 [=================>............] - ETA: 3s - loss: 1.8991 - accuracy: 0.1012
on_train_batch_begin: 1607800665.613688s

15 step training time: 0.353505s

on_train_batch_end: 1607800665.966377s

32768/50000 [==================>...........] - ETA: 3s - loss: 1.8919 - accuracy: 0.1012
on_train_batch_begin: 1607800665.966757s

16 step training time: 0.353069s

on_train_batch_end: 1607800666.323481s

34816/50000 [===================>..........] - ETA: 2s - loss: 1.8892 - accuracy: 0.1012
on_train_batch_begin: 1607800666.323934s

17 step training time: 0.357177s

on_train_batch_end: 1607800666.683531s

36864/50000 [=====================>........] - ETA: 2s - loss: 1.8826 - accuracy: 0.1012
on_train_batch_begin: 1607800666.683973s

18 step training time: 0.360039s

on_train_batch_end: 1607800667.040068s

38912/50000 [======================>.......] - ETA: 1s - loss: 1.8737 - accuracy: 0.1012
on_train_batch_begin: 1607800667.040469s

19 step training time: 0.356497s

on_train_batch_end: 1607800667.393905s

40960/50000 [=======================>......] - ETA: 1s - loss: 1.8703 - accuracy: 0.1011
on_train_batch_begin: 1607800667.394321s

20 step training time: 0.353852s

on_train_batch_end: 1607800667.752108s

43008/50000 [========================>.....] - ETA: 1s - loss: 1.8631 - accuracy: 0.1011
on_train_batch_begin: 1607800667.752521s

21 step training time: 0.358200s

on_train_batch_end: 1607800668.107792s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.8598 - accuracy: 0.1011
on_train_batch_begin: 1607800668.108193s

22 step training time: 0.355672s

on_train_batch_end: 1607800668.468777s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.8520 - accuracy: 0.1011
on_train_batch_begin: 1607800668.469177s

23 step training time: 0.360983s

on_train_batch_end: 1607800668.824811s

49152/50000 [============================>.] - ETA: 0s - loss: 1.8400 - accuracy: 0.1011
on_train_batch_begin: 1607800668.825221s

24 step training time: 0.356044s

on_train_batch_end: 1607800669.065144s

on_test_batch_begin: 1607800669.160224s

25 step training time: 0.335004s

on_epoch_end: 1607800669.575688s

Validation time: 0.415443s

Real time: 1607800669.575688s

Epoch time: 9.315032482147217s

50000/50000 [==============================] - 9s 186us/sample - loss: 1.8381 - accuracy: 0.1011 - val_loss: 7.3185 - val_accuracy: 0.1001

on_epoch_begin: 1607800669.575970s

Real time: 1607800669.5759797
Epoch 5/5

on_train_batch_begin: 1607800669.583845s

on_train_batch_end: 1607800669.942034s

 2048/50000 [>.............................] - ETA: 8s - loss: 1.6595 - accuracy: 0.1008
on_train_batch_begin: 1607800669.942421s

1 step training time: 0.358576s

on_train_batch_end: 1607800670.299075s

 4096/50000 [=>............................] - ETA: 8s - loss: 1.5886 - accuracy: 0.1009
on_train_batch_begin: 1607800670.299466s

2 step training time: 0.357045s

on_train_batch_end: 1607800670.655195s

 6144/50000 [==>...........................] - ETA: 7s - loss: 1.5812 - accuracy: 0.1010
on_train_batch_begin: 1607800670.655579s

3 step training time: 0.356113s

on_train_batch_end: 1607800671.014090s

 8192/50000 [===>..........................] - ETA: 7s - loss: 1.5478 - accuracy: 0.1011
on_train_batch_begin: 1607800671.014471s

4 step training time: 0.358892s

on_train_batch_end: 1607800671.372617s

10240/50000 [=====>........................] - ETA: 6s - loss: 1.5463 - accuracy: 0.1012
on_train_batch_begin: 1607800671.372992s

5 step training time: 0.358522s

on_train_batch_end: 1607800671.730269s

12288/50000 [======>.......................] - ETA: 6s - loss: 1.5390 - accuracy: 0.1012
on_train_batch_begin: 1607800671.730646s

6 step training time: 0.357654s

on_train_batch_end: 1607800672.088897s

14336/50000 [=======>......................] - ETA: 6s - loss: 1.5112 - accuracy: 0.1011
on_train_batch_begin: 1607800672.089277s

7 step training time: 0.358631s

on_train_batch_end: 1607800672.446263s

16384/50000 [========>.....................] - ETA: 5s - loss: 1.4944 - accuracy: 0.1012
on_train_batch_begin: 1607800672.446649s

8 step training time: 0.357372s

on_train_batch_end: 1607800672.802825s

18432/50000 [==========>...................] - ETA: 5s - loss: 1.4912 - accuracy: 0.1011
on_train_batch_begin: 1607800672.803207s

9 step training time: 0.356558s

on_train_batch_end: 1607800673.159425s

20480/50000 [===========>..................] - ETA: 5s - loss: 1.4912 - accuracy: 0.1011
on_train_batch_begin: 1607800673.159841s

10 step training time: 0.356634s

on_train_batch_end: 1607800673.517503s

22528/50000 [============>.................] - ETA: 4s - loss: 1.4774 - accuracy: 0.1011
on_train_batch_begin: 1607800673.517893s

11 step training time: 0.358051s

on_train_batch_end: 1607800673.874624s

24576/50000 [=============>................] - ETA: 4s - loss: 1.4698 - accuracy: 0.1012
on_train_batch_begin: 1607800673.875009s

12 step training time: 0.357117s

on_train_batch_end: 1607800674.230265s

26624/50000 [==============>...............] - ETA: 4s - loss: 1.4578 - accuracy: 0.1012
on_train_batch_begin: 1607800674.230651s

13 step training time: 0.355642s

on_train_batch_end: 1607800674.585276s

28672/50000 [================>.............] - ETA: 3s - loss: 1.4589 - accuracy: 0.1012
on_train_batch_begin: 1607800674.585651s

14 step training time: 0.355000s

on_train_batch_end: 1607800674.942383s

30720/50000 [=================>............] - ETA: 3s - loss: 1.4547 - accuracy: 0.1012
on_train_batch_begin: 1607800674.942762s

15 step training time: 0.357110s

on_train_batch_end: 1607800675.298765s

32768/50000 [==================>...........] - ETA: 3s - loss: 1.4542 - accuracy: 0.1011
on_train_batch_begin: 1607800675.299193s

16 step training time: 0.356432s

on_train_batch_end: 1607800675.655308s

34816/50000 [===================>..........] - ETA: 2s - loss: 1.4498 - accuracy: 0.1011
on_train_batch_begin: 1607800675.655692s

17 step training time: 0.356499s

on_train_batch_end: 1607800676.011119s

36864/50000 [=====================>........] - ETA: 2s - loss: 1.4495 - accuracy: 0.1011
on_train_batch_begin: 1607800676.011509s

18 step training time: 0.355817s

on_train_batch_end: 1607800676.366010s

38912/50000 [======================>.......] - ETA: 1s - loss: 1.4403 - accuracy: 0.1011
on_train_batch_begin: 1607800676.366400s

19 step training time: 0.354890s

on_train_batch_end: 1607800676.723505s

40960/50000 [=======================>......] - ETA: 1s - loss: 1.4348 - accuracy: 0.1011
on_train_batch_begin: 1607800676.723922s

20 step training time: 0.357522s

on_train_batch_end: 1607800677.076305s

43008/50000 [========================>.....] - ETA: 1s - loss: 1.4307 - accuracy: 0.1011
on_train_batch_begin: 1607800677.076685s

21 step training time: 0.352763s

on_train_batch_end: 1607800677.434430s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.4272 - accuracy: 0.1011
on_train_batch_begin: 1607800677.434811s

22 step training time: 0.358126s

on_train_batch_end: 1607800677.793799s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.4221 - accuracy: 0.1011
on_train_batch_begin: 1607800677.794180s

23 step training time: 0.359369s

on_train_batch_end: 1607800678.148109s

49152/50000 [============================>.] - ETA: 0s - loss: 1.4145 - accuracy: 0.1011
on_train_batch_begin: 1607800678.148491s

24 step training time: 0.354311s

on_train_batch_end: 1607800678.387214s

on_test_batch_begin: 1607800678.479678s

25 step training time: 0.331187s

on_epoch_end: 1607800678.893417s

Validation time: 0.413722s

Real time: 1607800678.893417s

Epoch time: 9.317459106445312s

50000/50000 [==============================] - 9s 186us/sample - loss: 1.4144 - accuracy: 0.1011 - val_loss: 7.1453 - val_accuracy: 0.1001
Tempo do fit: 219.49545907974243