wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:49
   221184/170498071 [..............................] - ETA: 1:12
  1171456/170498071 [..............................] - ETA: 20s 
  3252224/170498071 [..............................] - ETA: 9s 
  6529024/170498071 [>.............................] - ETA: 6s
  9773056/170498071 [>.............................] - ETA: 4s
 12943360/170498071 [=>............................] - ETA: 4s
 16031744/170498071 [=>............................] - ETA: 3s
 19267584/170498071 [==>...........................] - ETA: 3s
 22536192/170498071 [==>...........................] - ETA: 3s
 25501696/170498071 [===>..........................] - ETA: 3s
 28540928/170498071 [====>.........................] - ETA: 2s
 31866880/170498071 [====>.........................] - ETA: 2s
 35184640/170498071 [=====>........................] - ETA: 2s
 38035456/170498071 [=====>........................] - ETA: 2s
 41279488/170498071 [======>.......................] - ETA: 2s
 44548096/170498071 [======>.......................] - ETA: 2s
 47669248/170498071 [=======>......................] - ETA: 2s
 50847744/170498071 [=======>......................] - ETA: 2s
 54050816/170498071 [========>.....................] - ETA: 2s
 57352192/170498071 [=========>....................] - ETA: 2s
 60399616/170498071 [=========>....................] - ETA: 2s
 63660032/170498071 [==========>...................] - ETA: 1s
 66936832/170498071 [==========>...................] - ETA: 1s
 70033408/170498071 [===========>..................] - ETA: 1s
 73261056/170498071 [===========>..................] - ETA: 1s
 76472320/170498071 [============>.................] - ETA: 1s
 79732736/170498071 [=============>................] - ETA: 1s
 82731008/170498071 [=============>................] - ETA: 1s
 85999616/170498071 [==============>...............] - ETA: 1s
 89292800/170498071 [==============>...............] - ETA: 1s
 92504064/170498071 [===============>..............] - ETA: 1s
 95412224/170498071 [===============>..............] - ETA: 1s
 98738176/170498071 [================>.............] - ETA: 1s
101982208/170498071 [================>.............] - ETA: 1s
105054208/170498071 [=================>............] - ETA: 1s
108208128/170498071 [==================>...........] - ETA: 1s
111484928/170498071 [==================>...........] - ETA: 1s
114761728/170498071 [===================>..........] - ETA: 0s
117858304/170498071 [===================>..........] - ETA: 0s
121053184/170498071 [====================>.........] - ETA: 0s
124313600/170498071 [====================>.........] - ETA: 0s
127500288/170498071 [=====================>........] - ETA: 0s
130654208/170498071 [=====================>........] - ETA: 0s
133996544/170498071 [======================>.......] - ETA: 0s
137191424/170498071 [=======================>......] - ETA: 0s
140107776/170498071 [=======================>......] - ETA: 0s
143319040/170498071 [========================>.....] - ETA: 0s
146595840/170498071 [========================>.....] - ETA: 0s
149889024/170498071 [=========================>....] - ETA: 0s
152805376/170498071 [=========================>....] - ETA: 0s
156065792/170498071 [==========================>...] - ETA: 0s
159309824/170498071 [===========================>..] - ETA: 0s
162357248/170498071 [===========================>..] - ETA: 0s
165593088/170498071 [============================>.] - ETA: 0s
168878080/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 4s
 1114112/94765736 [..............................] - ETA: 4s
 3465216/94765736 [>.............................] - ETA: 3s
 8134656/94765736 [=>............................] - ETA: 1s
10321920/94765736 [==>...........................] - ETA: 1s
12926976/94765736 [===>..........................] - ETA: 1s
15294464/94765736 [===>..........................] - ETA: 1s
18841600/94765736 [====>.........................] - ETA: 1s
20021248/94765736 [=====>........................] - ETA: 1s
22388736/94765736 [======>.......................] - ETA: 1s
25591808/94765736 [=======>......................] - ETA: 1s
29671424/94765736 [========>.....................] - ETA: 1s
36413440/94765736 [==========>...................] - ETA: 1s
37683200/94765736 [==========>...................] - ETA: 1s
43917312/94765736 [============>.................] - ETA: 1s
47136768/94765736 [=============>................] - ETA: 1s
52781056/94765736 [===============>..............] - ETA: 0s
56598528/94765736 [================>.............] - ETA: 0s
59236352/94765736 [=================>............] - ETA: 0s
65986560/94765736 [===================>..........] - ETA: 0s
69206016/94765736 [====================>.........] - ETA: 0s
75440128/94765736 [======================>.......] - ETA: 0s
75964416/94765736 [=======================>......] - ETA: 0s
81141760/94765736 [========================>.....] - ETA: 0s
84893696/94765736 [=========================>....] - ETA: 0s
85409792/94765736 [==========================>...] - ETA: 0s
89915392/94765736 [===========================>..] - ETA: 0s
94281728/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 2s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 16.42942190170288
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1598457149.040797s

Real time: 1598457149.0408192
Epoch 1/5

on_train_batch_begin: 1598457149.867712s

on_train_batch_end: 1598457177.821233s

 2048/50000 [>.............................] - ETA: 11:13 - loss: 17.7630 - accuracy: 2.4629e-04
on_train_batch_begin: 1598457177.821936s

1 step training time: 27.954223s

on_train_batch_end: 1598457179.187103s

 4096/50000 [=>............................] - ETA: 5:37 - loss: 13.8009 - accuracy: 3.0339e-04 
on_train_batch_begin: 1598457179.187546s

2 step training time: 1.365611s

on_train_batch_end: 1598457180.536173s

 6144/50000 [==>...........................] - ETA: 3:44 - loss: 11.9473 - accuracy: 8.4877e-04
on_train_batch_begin: 1598457180.536565s

3 step training time: 1.349019s

on_train_batch_end: 1598457181.894852s

 8192/50000 [===>..........................] - ETA: 2:47 - loss: 10.9890 - accuracy: 0.0028    
on_train_batch_begin: 1598457181.895239s

4 step training time: 1.358674s

on_train_batch_end: 1598457183.244310s

10240/50000 [=====>........................] - ETA: 2:12 - loss: 10.3986 - accuracy: 0.0067
on_train_batch_begin: 1598457183.244723s

5 step training time: 1.349484s

on_train_batch_end: 1598457184.604828s

12288/50000 [======>.......................] - ETA: 1:49 - loss: 9.9914 - accuracy: 0.0111 
on_train_batch_begin: 1598457184.605214s

6 step training time: 1.360491s

on_train_batch_end: 1598457185.961129s

14336/50000 [=======>......................] - ETA: 1:31 - loss: 9.6729 - accuracy: 0.0147
on_train_batch_begin: 1598457185.961511s

7 step training time: 1.356297s

on_train_batch_end: 1598457187.311492s

16384/50000 [========>.....................] - ETA: 1:18 - loss: 9.4194 - accuracy: 0.0197
on_train_batch_begin: 1598457187.311870s

8 step training time: 1.350359s

on_train_batch_end: 1598457188.661198s

18432/50000 [==========>...................] - ETA: 1:07 - loss: 9.1967 - accuracy: 0.0232
on_train_batch_begin: 1598457188.661584s

9 step training time: 1.349714s

on_train_batch_end: 1598457190.017583s

20480/50000 [===========>..................] - ETA: 59s - loss: 9.0281 - accuracy: 0.0256 
on_train_batch_begin: 1598457190.017980s

10 step training time: 1.356395s

on_train_batch_end: 1598457191.374557s

22528/50000 [============>.................] - ETA: 51s - loss: 8.8982 - accuracy: 0.0283
on_train_batch_begin: 1598457191.374942s

11 step training time: 1.356962s

on_train_batch_end: 1598457192.726966s

24576/50000 [=============>................] - ETA: 45s - loss: 8.7724 - accuracy: 0.0308
on_train_batch_begin: 1598457192.727342s

12 step training time: 1.352400s

on_train_batch_end: 1598457194.084655s

26624/50000 [==============>...............] - ETA: 39s - loss: 8.6640 - accuracy: 0.0336
on_train_batch_begin: 1598457194.085063s

13 step training time: 1.357721s

on_train_batch_end: 1598457195.444951s

28672/50000 [================>.............] - ETA: 34s - loss: 8.5657 - accuracy: 0.0363
on_train_batch_begin: 1598457195.445333s

14 step training time: 1.360270s

on_train_batch_end: 1598457196.803888s

30720/50000 [=================>............] - ETA: 29s - loss: 8.4855 - accuracy: 0.0386
on_train_batch_begin: 1598457196.804267s

15 step training time: 1.358935s

on_train_batch_end: 1598457198.158768s

32768/50000 [==================>...........] - ETA: 25s - loss: 8.4036 - accuracy: 0.0405
on_train_batch_begin: 1598457198.159171s

16 step training time: 1.354903s

on_train_batch_end: 1598457199.517079s

34816/50000 [===================>..........] - ETA: 22s - loss: 8.3319 - accuracy: 0.0421
on_train_batch_begin: 1598457199.517454s

17 step training time: 1.358283s

on_train_batch_end: 1598457200.882817s

36864/50000 [=====================>........] - ETA: 18s - loss: 8.2904 - accuracy: 0.0425
on_train_batch_begin: 1598457200.883191s

18 step training time: 1.365737s

on_train_batch_end: 1598457202.244804s

38912/50000 [======================>.......] - ETA: 15s - loss: 8.2316 - accuracy: 0.0433
on_train_batch_begin: 1598457202.245188s

19 step training time: 1.361998s

on_train_batch_end: 1598457203.604447s

40960/50000 [=======================>......] - ETA: 12s - loss: 8.1736 - accuracy: 0.0441
on_train_batch_begin: 1598457203.605035s

20 step training time: 1.359847s

on_train_batch_end: 1598457204.971425s

43008/50000 [========================>.....] - ETA: 9s - loss: 8.1206 - accuracy: 0.0453 
on_train_batch_begin: 1598457204.971807s

21 step training time: 1.366772s

on_train_batch_end: 1598457206.325201s

45056/50000 [==========================>...] - ETA: 6s - loss: 8.0699 - accuracy: 0.0464
on_train_batch_begin: 1598457206.325581s

22 step training time: 1.353774s

on_train_batch_end: 1598457207.693926s

47104/50000 [===========================>..] - ETA: 3s - loss: 8.0231 - accuracy: 0.0473
on_train_batch_begin: 1598457207.694301s

23 step training time: 1.368720s

on_train_batch_end: 1598457209.053597s

49152/50000 [============================>.] - ETA: 1s - loss: 7.9774 - accuracy: 0.0482
on_train_batch_begin: 1598457209.053983s

24 step training time: 1.359682s

on_train_batch_end: 1598457216.791982s

on_test_batch_begin: 1598457216.999754s

25 step training time: 7.945771s

on_epoch_end: 1598457223.495318s

Validation time: 6.495545s

Real time: 1598457223.495318s

Epoch time: 74.4545202255249s

50000/50000 [==============================] - 74s 1ms/sample - loss: 7.9591 - accuracy: 0.0484 - val_loss: 15975.2640 - val_accuracy: 0.0000e+00

on_epoch_begin: 1598457223.495574s

Real time: 1598457223.495583
Epoch 2/5

on_train_batch_begin: 1598457223.499803s

on_train_batch_end: 1598457224.869866s

 2048/50000 [>.............................] - ETA: 32s - loss: 6.6561 - accuracy: 0.0791
on_train_batch_begin: 1598457224.870257s

1 step training time: 1.370455s

on_train_batch_end: 1598457226.231304s

 4096/50000 [=>............................] - ETA: 30s - loss: 6.6506 - accuracy: 0.0786
on_train_batch_begin: 1598457226.231717s

2 step training time: 1.361460s

on_train_batch_end: 1598457227.598552s

 6144/50000 [==>...........................] - ETA: 29s - loss: 6.6324 - accuracy: 0.0787
on_train_batch_begin: 1598457227.598929s

3 step training time: 1.367212s

on_train_batch_end: 1598457228.958029s

 8192/50000 [===>..........................] - ETA: 27s - loss: 6.6107 - accuracy: 0.0783
on_train_batch_begin: 1598457228.958402s

4 step training time: 1.359473s

on_train_batch_end: 1598457230.312849s

10240/50000 [=====>........................] - ETA: 26s - loss: 6.5889 - accuracy: 0.0774
on_train_batch_begin: 1598457230.313223s

5 step training time: 1.354821s

on_train_batch_end: 1598457231.674628s

12288/50000 [======>.......................] - ETA: 25s - loss: 6.5483 - accuracy: 0.0773
on_train_batch_begin: 1598457231.674996s

6 step training time: 1.361773s

on_train_batch_end: 1598457233.042351s

14336/50000 [=======>......................] - ETA: 23s - loss: 6.5185 - accuracy: 0.0763
on_train_batch_begin: 1598457233.042724s

7 step training time: 1.367728s

on_train_batch_end: 1598457234.408654s

16384/50000 [========>.....................] - ETA: 22s - loss: 6.4788 - accuracy: 0.0762
on_train_batch_begin: 1598457234.409064s

8 step training time: 1.366340s

on_train_batch_end: 1598457235.777660s

18432/50000 [==========>...................] - ETA: 21s - loss: 6.4406 - accuracy: 0.0757
on_train_batch_begin: 1598457235.778069s

9 step training time: 1.369005s

on_train_batch_end: 1598457237.137513s

20480/50000 [===========>..................] - ETA: 19s - loss: 6.4152 - accuracy: 0.0747
on_train_batch_begin: 1598457237.137893s

10 step training time: 1.359823s

on_train_batch_end: 1598457238.501741s

22528/50000 [============>.................] - ETA: 18s - loss: 6.3959 - accuracy: 0.0741
on_train_batch_begin: 1598457238.502124s

11 step training time: 1.364231s

on_train_batch_end: 1598457239.867220s

24576/50000 [=============>................] - ETA: 16s - loss: 6.3708 - accuracy: 0.0738
on_train_batch_begin: 1598457239.867606s

12 step training time: 1.365482s

on_train_batch_end: 1598457241.227380s

26624/50000 [==============>...............] - ETA: 15s - loss: 6.3434 - accuracy: 0.0731
on_train_batch_begin: 1598457241.227763s

13 step training time: 1.360158s

on_train_batch_end: 1598457242.588849s

28672/50000 [================>.............] - ETA: 14s - loss: 6.3181 - accuracy: 0.0730
on_train_batch_begin: 1598457242.589226s

14 step training time: 1.361462s

on_train_batch_end: 1598457243.962794s

30720/50000 [=================>............] - ETA: 12s - loss: 6.2873 - accuracy: 0.0725
on_train_batch_begin: 1598457243.963167s

15 step training time: 1.373942s

on_train_batch_end: 1598457245.323853s

32768/50000 [==================>...........] - ETA: 11s - loss: 6.2564 - accuracy: 0.0721
on_train_batch_begin: 1598457245.324228s

16 step training time: 1.361061s

on_train_batch_end: 1598457246.689445s

34816/50000 [===================>..........] - ETA: 10s - loss: 6.2304 - accuracy: 0.0716
on_train_batch_begin: 1598457246.689832s

17 step training time: 1.365604s

on_train_batch_end: 1598457248.051643s

36864/50000 [=====================>........] - ETA: 8s - loss: 6.1969 - accuracy: 0.0709 
on_train_batch_begin: 1598457248.052174s

18 step training time: 1.362342s

on_train_batch_end: 1598457249.416008s

38912/50000 [======================>.......] - ETA: 7s - loss: 6.1664 - accuracy: 0.0703
on_train_batch_begin: 1598457249.416475s

19 step training time: 1.364300s

on_train_batch_end: 1598457250.782818s

40960/50000 [=======================>......] - ETA: 6s - loss: 6.1363 - accuracy: 0.0696
on_train_batch_begin: 1598457250.783197s

20 step training time: 1.366723s

on_train_batch_end: 1598457252.149982s

43008/50000 [========================>.....] - ETA: 4s - loss: 6.1055 - accuracy: 0.0689
on_train_batch_begin: 1598457252.150384s

21 step training time: 1.367187s

on_train_batch_end: 1598457253.520051s

45056/50000 [==========================>...] - ETA: 3s - loss: 6.0727 - accuracy: 0.0682
on_train_batch_begin: 1598457253.520422s

22 step training time: 1.370038s

on_train_batch_end: 1598457254.885492s

47104/50000 [===========================>..] - ETA: 1s - loss: 6.0386 - accuracy: 0.0679
on_train_batch_begin: 1598457254.885864s

23 step training time: 1.365442s

on_train_batch_end: 1598457256.259455s

49152/50000 [============================>.] - ETA: 0s - loss: 6.0074 - accuracy: 0.0675
on_train_batch_begin: 1598457256.259827s

24 step training time: 1.373963s

on_train_batch_end: 1598457256.874510s

on_test_batch_begin: 1598457256.898231s

25 step training time: 0.638404s

on_epoch_end: 1598457258.712934s

Validation time: 1.814688s

Real time: 1598457258.712934s

Epoch time: 35.21737241744995s

50000/50000 [==============================] - 35s 704us/sample - loss: 5.9963 - accuracy: 0.0675 - val_loss: 624.3502 - val_accuracy: 0.0000e+00

on_epoch_begin: 1598457258.713188s

Real time: 1598457258.7132
Epoch 3/5

on_train_batch_begin: 1598457258.717591s

on_train_batch_end: 1598457260.082510s

 2048/50000 [>.............................] - ETA: 32s - loss: 5.0462 - accuracy: 0.0621
on_train_batch_begin: 1598457260.082894s

1 step training time: 1.365303s

on_train_batch_end: 1598457261.459764s

 4096/50000 [=>............................] - ETA: 30s - loss: 4.9451 - accuracy: 0.0640
on_train_batch_begin: 1598457261.460137s

2 step training time: 1.377243s

on_train_batch_end: 1598457262.829753s

 6144/50000 [==>...........................] - ETA: 29s - loss: 4.9287 - accuracy: 0.0673
on_train_batch_begin: 1598457262.830120s

3 step training time: 1.369983s

on_train_batch_end: 1598457264.201362s

 8192/50000 [===>..........................] - ETA: 28s - loss: 4.9127 - accuracy: 0.0672
on_train_batch_begin: 1598457264.201732s

4 step training time: 1.371612s

on_train_batch_end: 1598457265.581678s

10240/50000 [=====>........................] - ETA: 26s - loss: 4.8687 - accuracy: 0.0695
on_train_batch_begin: 1598457265.582091s

5 step training time: 1.380359s

on_train_batch_end: 1598457266.943969s

12288/50000 [======>.......................] - ETA: 25s - loss: 4.8319 - accuracy: 0.0690
on_train_batch_begin: 1598457266.944336s

6 step training time: 1.362245s

on_train_batch_end: 1598457268.313969s

14336/50000 [=======>......................] - ETA: 23s - loss: 4.7851 - accuracy: 0.0692
on_train_batch_begin: 1598457268.314340s

7 step training time: 1.370004s

on_train_batch_end: 1598457269.692594s

16384/50000 [========>.....................] - ETA: 22s - loss: 4.7357 - accuracy: 0.0697
on_train_batch_begin: 1598457269.693007s

8 step training time: 1.378667s

on_train_batch_end: 1598457271.079031s

18432/50000 [==========>...................] - ETA: 21s - loss: 4.7142 - accuracy: 0.0693
on_train_batch_begin: 1598457271.079406s

9 step training time: 1.386399s

on_train_batch_end: 1598457272.454410s

20480/50000 [===========>..................] - ETA: 19s - loss: 4.6883 - accuracy: 0.0691
on_train_batch_begin: 1598457272.454776s

10 step training time: 1.375370s

on_train_batch_end: 1598457273.825181s

22528/50000 [============>.................] - ETA: 18s - loss: 4.6651 - accuracy: 0.0698
on_train_batch_begin: 1598457273.825736s

11 step training time: 1.370960s

on_train_batch_end: 1598457275.200460s

24576/50000 [=============>................] - ETA: 17s - loss: 4.6386 - accuracy: 0.0699
on_train_batch_begin: 1598457275.200900s

12 step training time: 1.375164s

on_train_batch_end: 1598457276.569903s

26624/50000 [==============>...............] - ETA: 15s - loss: 4.6191 - accuracy: 0.0699
on_train_batch_begin: 1598457276.570275s

13 step training time: 1.369375s

on_train_batch_end: 1598457277.939602s

28672/50000 [================>.............] - ETA: 14s - loss: 4.5844 - accuracy: 0.0698
on_train_batch_begin: 1598457277.939971s

14 step training time: 1.369697s

on_train_batch_end: 1598457279.312228s

30720/50000 [=================>............] - ETA: 12s - loss: 4.5635 - accuracy: 0.0698
on_train_batch_begin: 1598457279.312606s

15 step training time: 1.372634s

on_train_batch_end: 1598457280.688777s

32768/50000 [==================>...........] - ETA: 11s - loss: 4.5340 - accuracy: 0.0701
on_train_batch_begin: 1598457280.689155s

16 step training time: 1.376549s

on_train_batch_end: 1598457282.061033s

34816/50000 [===================>..........] - ETA: 10s - loss: 4.4987 - accuracy: 0.0702
on_train_batch_begin: 1598457282.061426s

17 step training time: 1.372271s

on_train_batch_end: 1598457283.449822s

36864/50000 [=====================>........] - ETA: 8s - loss: 4.4662 - accuracy: 0.0704 
on_train_batch_begin: 1598457283.450215s

18 step training time: 1.388789s

on_train_batch_end: 1598457284.830374s

38912/50000 [======================>.......] - ETA: 7s - loss: 4.4337 - accuracy: 0.0707
on_train_batch_begin: 1598457284.830751s

19 step training time: 1.380536s

on_train_batch_end: 1598457286.210043s

40960/50000 [=======================>......] - ETA: 6s - loss: 4.3989 - accuracy: 0.0711
on_train_batch_begin: 1598457286.210419s

20 step training time: 1.379668s

on_train_batch_end: 1598457287.581904s

43008/50000 [========================>.....] - ETA: 4s - loss: 4.3666 - accuracy: 0.0715
on_train_batch_begin: 1598457287.582288s

21 step training time: 1.371869s

on_train_batch_end: 1598457288.970008s

45056/50000 [==========================>...] - ETA: 3s - loss: 4.3359 - accuracy: 0.0719
on_train_batch_begin: 1598457288.970384s

22 step training time: 1.388096s

on_train_batch_end: 1598457290.349410s

47104/50000 [===========================>..] - ETA: 1s - loss: 4.3031 - accuracy: 0.0723
on_train_batch_begin: 1598457290.349784s

23 step training time: 1.379400s

on_train_batch_end: 1598457291.723189s

49152/50000 [============================>.] - ETA: 0s - loss: 4.2680 - accuracy: 0.0729
on_train_batch_begin: 1598457291.723559s

24 step training time: 1.373775s

on_train_batch_end: 1598457292.343924s

on_test_batch_begin: 1598457292.367302s

25 step training time: 0.643742s

on_epoch_end: 1598457294.187128s

Validation time: 1.819811s

Real time: 1598457294.187128s

Epoch time: 35.473947048187256s

50000/50000 [==============================] - 35s 709us/sample - loss: 4.2525 - accuracy: 0.0730 - val_loss: 7.3978 - val_accuracy: 0.0982

on_epoch_begin: 1598457294.187377s

Real time: 1598457294.187386
Epoch 4/5

on_train_batch_begin: 1598457294.193290s

on_train_batch_end: 1598457295.559380s

 2048/50000 [>.............................] - ETA: 32s - loss: 3.3077 - accuracy: 0.0857
on_train_batch_begin: 1598457295.559749s

1 step training time: 1.366459s

on_train_batch_end: 1598457296.928704s

 4096/50000 [=>............................] - ETA: 30s - loss: 3.2661 - accuracy: 0.0873
on_train_batch_begin: 1598457296.929099s

2 step training time: 1.369349s

on_train_batch_end: 1598457298.303705s

 6144/50000 [==>...........................] - ETA: 29s - loss: 3.1568 - accuracy: 0.0884
on_train_batch_begin: 1598457298.304098s

3 step training time: 1.374999s

on_train_batch_end: 1598457299.681735s

 8192/50000 [===>..........................] - ETA: 28s - loss: 3.1460 - accuracy: 0.0886
on_train_batch_begin: 1598457299.682105s

4 step training time: 1.378007s

on_train_batch_end: 1598457301.053646s

10240/50000 [=====>........................] - ETA: 26s - loss: 3.0972 - accuracy: 0.0895
on_train_batch_begin: 1598457301.054034s

5 step training time: 1.371928s

on_train_batch_end: 1598457302.436146s

12288/50000 [======>.......................] - ETA: 25s - loss: 3.0582 - accuracy: 0.0901
on_train_batch_begin: 1598457302.436521s

6 step training time: 1.382488s

on_train_batch_end: 1598457303.808893s

14336/50000 [=======>......................] - ETA: 23s - loss: 3.0274 - accuracy: 0.0907
on_train_batch_begin: 1598457303.809286s

7 step training time: 1.372765s

on_train_batch_end: 1598457305.180412s

16384/50000 [========>.....................] - ETA: 22s - loss: 3.0074 - accuracy: 0.0910
on_train_batch_begin: 1598457305.180828s

8 step training time: 1.371543s

on_train_batch_end: 1598457306.554646s

18432/50000 [==========>...................] - ETA: 21s - loss: 2.9744 - accuracy: 0.0914
on_train_batch_begin: 1598457306.555047s

9 step training time: 1.374218s

on_train_batch_end: 1598457307.930756s

20480/50000 [===========>..................] - ETA: 19s - loss: 2.9393 - accuracy: 0.0921
on_train_batch_begin: 1598457307.931159s

10 step training time: 1.376112s

on_train_batch_end: 1598457309.316989s

22528/50000 [============>.................] - ETA: 18s - loss: 2.9030 - accuracy: 0.0926
on_train_batch_begin: 1598457309.317404s

11 step training time: 1.386245s

on_train_batch_end: 1598457310.693559s

24576/50000 [=============>................] - ETA: 17s - loss: 2.8708 - accuracy: 0.0930
on_train_batch_begin: 1598457310.693946s

12 step training time: 1.376542s

on_train_batch_end: 1598457312.065478s

26624/50000 [==============>...............] - ETA: 15s - loss: 2.8312 - accuracy: 0.0935
on_train_batch_begin: 1598457312.065859s

13 step training time: 1.371913s

on_train_batch_end: 1598457313.448400s

28672/50000 [================>.............] - ETA: 14s - loss: 2.7993 - accuracy: 0.0940
on_train_batch_begin: 1598457313.448807s

14 step training time: 1.382948s

on_train_batch_end: 1598457314.824407s

30720/50000 [=================>............] - ETA: 12s - loss: 2.7655 - accuracy: 0.0945
on_train_batch_begin: 1598457314.824810s

15 step training time: 1.376003s

on_train_batch_end: 1598457316.197946s

32768/50000 [==================>...........] - ETA: 11s - loss: 2.7402 - accuracy: 0.0948
on_train_batch_begin: 1598457316.198327s

16 step training time: 1.373518s

on_train_batch_end: 1598457317.579162s

34816/50000 [===================>..........] - ETA: 10s - loss: 2.7105 - accuracy: 0.0951
on_train_batch_begin: 1598457317.579532s

17 step training time: 1.381205s

on_train_batch_end: 1598457318.967257s

36864/50000 [=====================>........] - ETA: 8s - loss: 2.6743 - accuracy: 0.0954 
on_train_batch_begin: 1598457318.967625s

18 step training time: 1.388094s

on_train_batch_end: 1598457320.341780s

38912/50000 [======================>.......] - ETA: 7s - loss: 2.6468 - accuracy: 0.0956
on_train_batch_begin: 1598457320.342165s

19 step training time: 1.374540s

on_train_batch_end: 1598457321.722780s

40960/50000 [=======================>......] - ETA: 6s - loss: 2.6150 - accuracy: 0.0959
on_train_batch_begin: 1598457321.723164s

20 step training time: 1.380999s

on_train_batch_end: 1598457323.106387s

43008/50000 [========================>.....] - ETA: 4s - loss: 2.5829 - accuracy: 0.0961
on_train_batch_begin: 1598457323.106791s

21 step training time: 1.383627s

on_train_batch_end: 1598457324.481600s

45056/50000 [==========================>...] - ETA: 3s - loss: 2.5534 - accuracy: 0.0962
on_train_batch_begin: 1598457324.481971s

22 step training time: 1.375180s

on_train_batch_end: 1598457325.855873s

47104/50000 [===========================>..] - ETA: 1s - loss: 2.5270 - accuracy: 0.0964
on_train_batch_begin: 1598457325.856245s

23 step training time: 1.374274s

on_train_batch_end: 1598457327.226137s

49152/50000 [============================>.] - ETA: 0s - loss: 2.5056 - accuracy: 0.0966
on_train_batch_begin: 1598457327.226513s

24 step training time: 1.370269s

on_train_batch_end: 1598457327.840966s

on_test_batch_begin: 1598457327.864314s

25 step training time: 0.637801s

on_epoch_end: 1598457329.708366s

Validation time: 1.844036s

Real time: 1598457329.708366s

Epoch time: 35.52100086212158s

50000/50000 [==============================] - 36s 710us/sample - loss: 2.4972 - accuracy: 0.0966 - val_loss: 7.2451 - val_accuracy: 0.1001

on_epoch_begin: 1598457329.708608s

Real time: 1598457329.7086167
Epoch 5/5

on_train_batch_begin: 1598457329.713041s

on_train_batch_end: 1598457331.084833s

 2048/50000 [>.............................] - ETA: 32s - loss: 1.8918 - accuracy: 0.1003
on_train_batch_begin: 1598457331.085223s

1 step training time: 1.372182s

on_train_batch_end: 1598457332.468148s

 4096/50000 [=>............................] - ETA: 30s - loss: 1.9168 - accuracy: 0.1001
on_train_batch_begin: 1598457332.468522s

2 step training time: 1.383299s

on_train_batch_end: 1598457333.851222s

 6144/50000 [==>...........................] - ETA: 29s - loss: 1.8694 - accuracy: 0.0998
on_train_batch_begin: 1598457333.851592s

3 step training time: 1.383070s

on_train_batch_end: 1598457335.238759s

 8192/50000 [===>..........................] - ETA: 28s - loss: 1.8743 - accuracy: 0.0999
on_train_batch_begin: 1598457335.239129s

4 step training time: 1.387538s

on_train_batch_end: 1598457336.618627s

10240/50000 [=====>........................] - ETA: 26s - loss: 1.8535 - accuracy: 0.1000
on_train_batch_begin: 1598457336.619000s

5 step training time: 1.379871s

on_train_batch_end: 1598457338.000287s

12288/50000 [======>.......................] - ETA: 25s - loss: 1.8625 - accuracy: 0.1000
on_train_batch_begin: 1598457338.000655s

6 step training time: 1.381655s

on_train_batch_end: 1598457339.376066s

14336/50000 [=======>......................] - ETA: 24s - loss: 1.8459 - accuracy: 0.1001
on_train_batch_begin: 1598457339.376436s

7 step training time: 1.375781s

on_train_batch_end: 1598457340.755598s

16384/50000 [========>.....................] - ETA: 22s - loss: 1.8423 - accuracy: 0.1001
on_train_batch_begin: 1598457340.755975s

8 step training time: 1.379539s

on_train_batch_end: 1598457342.129744s

18432/50000 [==========>...................] - ETA: 21s - loss: 1.8599 - accuracy: 0.1001
on_train_batch_begin: 1598457342.130118s

9 step training time: 1.374142s

on_train_batch_end: 1598457343.523731s

20480/50000 [===========>..................] - ETA: 19s - loss: 1.8720 - accuracy: 0.1001
on_train_batch_begin: 1598457343.524123s

10 step training time: 1.394005s

on_train_batch_end: 1598457344.911426s

22528/50000 [============>.................] - ETA: 18s - loss: 1.8700 - accuracy: 0.1001
on_train_batch_begin: 1598457344.911800s

11 step training time: 1.387677s

on_train_batch_end: 1598457346.291812s

24576/50000 [=============>................] - ETA: 17s - loss: 1.8602 - accuracy: 0.1001
on_train_batch_begin: 1598457346.292202s

12 step training time: 1.380403s

on_train_batch_end: 1598457347.677092s

26624/50000 [==============>...............] - ETA: 15s - loss: 1.8468 - accuracy: 0.1001
on_train_batch_begin: 1598457347.677482s

13 step training time: 1.385279s

on_train_batch_end: 1598457349.056890s

28672/50000 [================>.............] - ETA: 14s - loss: 1.8318 - accuracy: 0.1001
on_train_batch_begin: 1598457349.057263s

14 step training time: 1.379781s

on_train_batch_end: 1598457350.436602s

30720/50000 [=================>............] - ETA: 13s - loss: 1.8228 - accuracy: 0.1001
on_train_batch_begin: 1598457350.437001s

15 step training time: 1.379738s

on_train_batch_end: 1598457351.822724s

32768/50000 [==================>...........] - ETA: 11s - loss: 1.8093 - accuracy: 0.1001
on_train_batch_begin: 1598457351.823090s

16 step training time: 1.386089s

on_train_batch_end: 1598457353.206387s

34816/50000 [===================>..........] - ETA: 10s - loss: 1.7986 - accuracy: 0.1001
on_train_batch_begin: 1598457353.206758s

17 step training time: 1.383668s

on_train_batch_end: 1598457354.587328s

36864/50000 [=====================>........] - ETA: 8s - loss: 1.7910 - accuracy: 0.1001 
on_train_batch_begin: 1598457354.587695s

18 step training time: 1.380937s

on_train_batch_end: 1598457355.963268s

38912/50000 [======================>.......] - ETA: 7s - loss: 1.7810 - accuracy: 0.1001
on_train_batch_begin: 1598457355.963645s

19 step training time: 1.375950s

on_train_batch_end: 1598457357.349217s

40960/50000 [=======================>......] - ETA: 6s - loss: 1.7798 - accuracy: 0.1001
on_train_batch_begin: 1598457357.349588s

20 step training time: 1.385943s

on_train_batch_end: 1598457358.734916s

43008/50000 [========================>.....] - ETA: 4s - loss: 1.7717 - accuracy: 0.1001
on_train_batch_begin: 1598457358.735290s

21 step training time: 1.385702s

on_train_batch_end: 1598457360.118748s

45056/50000 [==========================>...] - ETA: 3s - loss: 1.7624 - accuracy: 0.1000
on_train_batch_begin: 1598457360.119131s

22 step training time: 1.383840s

on_train_batch_end: 1598457361.499519s

47104/50000 [===========================>..] - ETA: 1s - loss: 1.7509 - accuracy: 0.1000
on_train_batch_begin: 1598457361.499907s

23 step training time: 1.380776s

on_train_batch_end: 1598457362.887754s

49152/50000 [============================>.] - ETA: 0s - loss: 1.7388 - accuracy: 0.1000
on_train_batch_begin: 1598457362.888134s

24 step training time: 1.388227s

on_train_batch_end: 1598457363.502486s

on_test_batch_begin: 1598457363.525964s

25 step training time: 0.637830s

on_epoch_end: 1598457365.368700s

Validation time: 1.842722s

Real time: 1598457365.368700s

Epoch time: 35.660104513168335s

50000/50000 [==============================] - 36s 713us/sample - loss: 1.7371 - accuracy: 0.1000 - val_loss: 7.6565 - val_accuracy: 0.1001
Tempo do fit: 220.04130744934082