wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:14
   139264/170498071 [..............................] - ETA: 1:28
   516096/170498071 [..............................] - ETA: 40s 
  1007616/170498071 [..............................] - ETA: 29s
  1548288/170498071 [..............................] - ETA: 24s
  2154496/170498071 [..............................] - ETA: 21s
  2826240/170498071 [..............................] - ETA: 19s
  3563520/170498071 [..............................] - ETA: 17s
  4399104/170498071 [..............................] - ETA: 16s
  5316608/170498071 [..............................] - ETA: 15s
  6348800/170498071 [>.............................] - ETA: 13s
  7462912/170498071 [>.............................] - ETA: 12s
  8445952/170498071 [>.............................] - ETA: 12s
  9740288/170498071 [>.............................] - ETA: 11s
 11018240/170498071 [>.............................] - ETA: 10s
 12296192/170498071 [=>............................] - ETA: 10s
 13574144/170498071 [=>............................] - ETA: 9s 
 15122432/170498071 [=>............................] - ETA: 9s
 16637952/170498071 [=>............................] - ETA: 8s
 18243584/170498071 [==>...........................] - ETA: 8s
 19800064/170498071 [==>...........................] - ETA: 7s
 21504000/170498071 [==>...........................] - ETA: 7s
 22994944/170498071 [===>..........................] - ETA: 7s
 24371200/170498071 [===>..........................] - ETA: 7s
 26263552/170498071 [===>..........................] - ETA: 6s
 28073984/170498071 [===>..........................] - ETA: 6s
 29777920/170498071 [====>.........................] - ETA: 6s
 31465472/170498071 [====>.........................] - ETA: 6s
 33423360/170498071 [====>.........................] - ETA: 5s
 35037184/170498071 [=====>........................] - ETA: 5s
 36904960/170498071 [=====>........................] - ETA: 5s
 38559744/170498071 [=====>........................] - ETA: 5s
 40509440/170498071 [======>.......................] - ETA: 5s
 42131456/170498071 [======>.......................] - ETA: 5s
 44539904/170498071 [======>.......................] - ETA: 4s
 47292416/170498071 [=======>......................] - ETA: 4s
 50176000/170498071 [=======>......................] - ETA: 4s
 53354496/170498071 [========>.....................] - ETA: 4s
 56893440/170498071 [=========>....................] - ETA: 3s
 60514304/170498071 [=========>....................] - ETA: 3s
 63938560/170498071 [==========>...................] - ETA: 3s
 67395584/170498071 [==========>...................] - ETA: 3s
 71319552/170498071 [===========>..................] - ETA: 2s
 74801152/170498071 [============>.................] - ETA: 2s
 78323712/170498071 [============>.................] - ETA: 2s
 81846272/170498071 [=============>................] - ETA: 2s
 85860352/170498071 [==============>...............] - ETA: 2s
 89309184/170498071 [==============>...............] - ETA: 2s
 92708864/170498071 [===============>..............] - ETA: 2s
 96280576/170498071 [===============>..............] - ETA: 1s
100212736/170498071 [================>.............] - ETA: 1s
103710720/170498071 [=================>............] - ETA: 1s
107290624/170498071 [=================>............] - ETA: 1s
110927872/170498071 [==================>...........] - ETA: 1s
114434048/170498071 [===================>..........] - ETA: 1s
118284288/170498071 [===================>..........] - ETA: 1s
121987072/170498071 [====================>.........] - ETA: 1s
125476864/170498071 [=====================>........] - ETA: 1s
129097728/170498071 [=====================>........] - ETA: 0s
132669440/170498071 [======================>.......] - ETA: 0s
136437760/170498071 [=======================>......] - ETA: 0s
140083200/170498071 [=======================>......] - ETA: 0s
143441920/170498071 [========================>.....] - ETA: 0s
147185664/170498071 [========================>.....] - ETA: 0s
150732800/170498071 [=========================>....] - ETA: 0s
154460160/170498071 [==========================>...] - ETA: 0s
157917184/170498071 [==========================>...] - ETA: 0s
161587200/170498071 [===========================>..] - ETA: 0s
165109760/170498071 [============================>.] - ETA: 0s
168943616/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 4s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 2s
 2285568/94765736 [..............................] - ETA: 2s
 9388032/94765736 [=>............................] - ETA: 1s
15138816/94765736 [===>..........................] - ETA: 0s
18841600/94765736 [====>.........................] - ETA: 1s
23896064/94765736 [======>.......................] - ETA: 0s
29106176/94765736 [========>.....................] - ETA: 0s
35495936/94765736 [==========>...................] - ETA: 0s
39804928/94765736 [===========>..................] - ETA: 0s
46948352/94765736 [=============>................] - ETA: 0s
48627712/94765736 [==============>...............] - ETA: 0s
54386688/94765736 [================>.............] - ETA: 0s
57663488/94765736 [=================>............] - ETA: 0s
64331776/94765736 [===================>..........] - ETA: 0s
68591616/94765736 [====================>.........] - ETA: 0s
74768384/94765736 [======================>.......] - ETA: 0s
81092608/94765736 [========================>.....] - ETA: 0s
86941696/94765736 [==========================>...] - ETA: 0s
94773248/94765736 [==============================] - 1s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 21.01516842842102
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1607807956.602354s

Real time: 1607807956.6023753
Epoch 1/5

on_train_batch_begin: 1607807957.546912s

on_train_batch_end: 1607808017.634498s

 2048/50000 [>.............................] - ETA: 23:49 - loss: 17.7421 - accuracy: 4.8637e-05
on_train_batch_begin: 1607808017.635243s

1 step training time: 60.088331s

on_train_batch_end: 1607808017.724825s

 4096/50000 [=>............................] - ETA: 11:25 - loss: 15.3176 - accuracy: 3.4380e-04
on_train_batch_begin: 1607808017.725243s

2 step training time: 0.090000s

on_train_batch_end: 1607808017.806013s

 6144/50000 [==>...........................] - ETA: 7:16 - loss: 13.3608 - accuracy: 5.6299e-04 
on_train_batch_begin: 1607808017.806439s

3 step training time: 0.081195s

on_train_batch_end: 1607808017.889665s

 8192/50000 [===>..........................] - ETA: 5:12 - loss: 12.0957 - accuracy: 0.0013    
on_train_batch_begin: 1607808017.890067s

4 step training time: 0.083628s

on_train_batch_end: 1607808017.971426s

10240/50000 [=====>........................] - ETA: 3:58 - loss: 11.2352 - accuracy: 0.0027
on_train_batch_begin: 1607808017.971822s

5 step training time: 0.081756s

on_train_batch_end: 1607808018.052916s

12288/50000 [======>.......................] - ETA: 3:08 - loss: 10.6063 - accuracy: 0.0044
on_train_batch_begin: 1607808018.053311s

6 step training time: 0.081489s

on_train_batch_end: 1607808018.135041s

14336/50000 [=======>......................] - ETA: 2:33 - loss: 10.1017 - accuracy: 0.0074
on_train_batch_begin: 1607808018.135437s

7 step training time: 0.082125s

on_train_batch_end: 1607808018.215990s

16384/50000 [========>.....................] - ETA: 2:06 - loss: 9.6907 - accuracy: 0.0108 
on_train_batch_begin: 1607808018.216387s

8 step training time: 0.080950s

on_train_batch_end: 1607808018.297312s

18432/50000 [==========>...................] - ETA: 1:45 - loss: 9.3631 - accuracy: 0.0138
on_train_batch_begin: 1607808018.297703s

9 step training time: 0.081316s

on_train_batch_end: 1607808018.379403s

20480/50000 [===========>..................] - ETA: 1:29 - loss: 9.0714 - accuracy: 0.0168
on_train_batch_begin: 1607808018.379801s

10 step training time: 0.082098s

on_train_batch_end: 1607808018.460088s

22528/50000 [============>.................] - ETA: 1:15 - loss: 8.8115 - accuracy: 0.0196
on_train_batch_begin: 1607808018.460480s

11 step training time: 0.080680s

on_train_batch_end: 1607808018.543170s

24576/50000 [=============>................] - ETA: 1:04 - loss: 8.5832 - accuracy: 0.0220
on_train_batch_begin: 1607808018.543574s

12 step training time: 0.083094s

on_train_batch_end: 1607808018.629773s

26624/50000 [==============>...............] - ETA: 54s - loss: 8.3812 - accuracy: 0.0236 
on_train_batch_begin: 1607808018.630166s

13 step training time: 0.086592s

on_train_batch_end: 1607808018.717110s

28672/50000 [================>.............] - ETA: 46s - loss: 8.1982 - accuracy: 0.0253
on_train_batch_begin: 1607808018.717503s

14 step training time: 0.087337s

on_train_batch_end: 1607808018.799801s

30720/50000 [=================>............] - ETA: 39s - loss: 8.0268 - accuracy: 0.0270
on_train_batch_begin: 1607808018.800195s

15 step training time: 0.082692s

on_train_batch_end: 1607808018.879343s

32768/50000 [==================>...........] - ETA: 32s - loss: 7.8705 - accuracy: 0.0286
on_train_batch_begin: 1607808018.879734s

16 step training time: 0.079540s

on_train_batch_end: 1607808018.960413s

34816/50000 [===================>..........] - ETA: 27s - loss: 7.7279 - accuracy: 0.0302
on_train_batch_begin: 1607808018.960804s

17 step training time: 0.081069s

on_train_batch_end: 1607808019.044110s

36864/50000 [=====================>........] - ETA: 22s - loss: 7.5908 - accuracy: 0.0318
on_train_batch_begin: 1607808019.044502s

18 step training time: 0.083699s

on_train_batch_end: 1607808019.125827s

38912/50000 [======================>.......] - ETA: 17s - loss: 7.4629 - accuracy: 0.0333
on_train_batch_begin: 1607808019.126217s

19 step training time: 0.081715s

on_train_batch_end: 1607808019.206668s

40960/50000 [=======================>......] - ETA: 13s - loss: 7.3428 - accuracy: 0.0347
on_train_batch_begin: 1607808019.207060s

20 step training time: 0.080843s

on_train_batch_end: 1607808019.287752s

43008/50000 [========================>.....] - ETA: 10s - loss: 7.2245 - accuracy: 0.0361
on_train_batch_begin: 1607808019.288143s

21 step training time: 0.081083s

on_train_batch_end: 1607808019.368343s

45056/50000 [==========================>...] - ETA: 6s - loss: 7.1111 - accuracy: 0.0375 
on_train_batch_begin: 1607808019.368739s

22 step training time: 0.080597s

on_train_batch_end: 1607808019.449400s

47104/50000 [===========================>..] - ETA: 3s - loss: 7.0064 - accuracy: 0.0388
on_train_batch_begin: 1607808019.449790s

23 step training time: 0.081051s

on_train_batch_end: 1607808019.528461s

49152/50000 [============================>.] - ETA: 1s - loss: 6.8999 - accuracy: 0.0402
on_train_batch_begin: 1607808019.528858s

24 step training time: 0.079068s

on_train_batch_end: 1607808020.545396s

on_test_batch_begin: 1607808020.843288s

25 step training time: 1.314430s

on_epoch_end: 1607808027.694563s

Validation time: 6.851256s

Real time: 1607808027.694563s

Epoch time: 71.09221386909485s

50000/50000 [==============================] - 71s 1ms/sample - loss: 6.8601 - accuracy: 0.0405 - val_loss: 83.2001 - val_accuracy: 0.0000e+00

on_epoch_begin: 1607808027.694844s

Real time: 1607808027.6948538
Epoch 2/5

on_train_batch_begin: 1607808027.700675s

on_train_batch_end: 1607808027.784080s

 2048/50000 [>.............................] - ETA: 2s - loss: 4.3240 - accuracy: 0.0754
on_train_batch_begin: 1607808027.784478s

1 step training time: 0.083803s

on_train_batch_end: 1607808027.866724s

 4096/50000 [=>............................] - ETA: 1s - loss: 4.2306 - accuracy: 0.0765
on_train_batch_begin: 1607808027.867118s

2 step training time: 0.082641s

on_train_batch_end: 1607808027.947687s

 6144/50000 [==>...........................] - ETA: 1s - loss: 4.2022 - accuracy: 0.0764
on_train_batch_begin: 1607808027.948075s

3 step training time: 0.080957s

on_train_batch_end: 1607808028.033868s

 8192/50000 [===>..........................] - ETA: 1s - loss: 4.1563 - accuracy: 0.0768
on_train_batch_begin: 1607808028.034255s

4 step training time: 0.086180s

on_train_batch_end: 1607808028.119088s

10240/50000 [=====>........................] - ETA: 1s - loss: 4.1106 - accuracy: 0.0774
on_train_batch_begin: 1607808028.119478s

5 step training time: 0.085223s

on_train_batch_end: 1607808028.202248s

12288/50000 [======>.......................] - ETA: 1s - loss: 4.0799 - accuracy: 0.0780
on_train_batch_begin: 1607808028.202668s

6 step training time: 0.083190s

on_train_batch_end: 1607808028.283777s

14336/50000 [=======>......................] - ETA: 1s - loss: 4.0091 - accuracy: 0.0788
on_train_batch_begin: 1607808028.284168s

7 step training time: 0.081500s

on_train_batch_end: 1607808028.365457s

16384/50000 [========>.....................] - ETA: 1s - loss: 3.9813 - accuracy: 0.0796
on_train_batch_begin: 1607808028.365845s

8 step training time: 0.081677s

on_train_batch_end: 1607808028.446920s

18432/50000 [==========>...................] - ETA: 1s - loss: 3.9595 - accuracy: 0.0801
on_train_batch_begin: 1607808028.447309s

9 step training time: 0.081464s

on_train_batch_end: 1607808028.529303s

20480/50000 [===========>..................] - ETA: 1s - loss: 3.9193 - accuracy: 0.0808
on_train_batch_begin: 1607808028.529711s

10 step training time: 0.082401s

on_train_batch_end: 1607808028.611835s

22528/50000 [============>.................] - ETA: 1s - loss: 3.9012 - accuracy: 0.0815
on_train_batch_begin: 1607808028.612234s

11 step training time: 0.082523s

on_train_batch_end: 1607808028.694997s

24576/50000 [=============>................] - ETA: 1s - loss: 3.8616 - accuracy: 0.0822
on_train_batch_begin: 1607808028.695395s

12 step training time: 0.083161s

on_train_batch_end: 1607808028.776873s

26624/50000 [==============>...............] - ETA: 0s - loss: 3.8173 - accuracy: 0.0830
on_train_batch_begin: 1607808028.777268s

13 step training time: 0.081873s

on_train_batch_end: 1607808028.857377s

28672/50000 [================>.............] - ETA: 0s - loss: 3.7873 - accuracy: 0.0834
on_train_batch_begin: 1607808028.857774s

14 step training time: 0.080506s

on_train_batch_end: 1607808028.936934s

30720/50000 [=================>............] - ETA: 0s - loss: 3.7620 - accuracy: 0.0838
on_train_batch_begin: 1607808028.937326s

15 step training time: 0.079552s

on_train_batch_end: 1607808029.022216s

32768/50000 [==================>...........] - ETA: 0s - loss: 3.7288 - accuracy: 0.0844
on_train_batch_begin: 1607808029.022641s

16 step training time: 0.085315s

on_train_batch_end: 1607808029.106453s

34816/50000 [===================>..........] - ETA: 0s - loss: 3.6926 - accuracy: 0.0848
on_train_batch_begin: 1607808029.106840s

17 step training time: 0.084199s

on_train_batch_end: 1607808029.192276s

36864/50000 [=====================>........] - ETA: 0s - loss: 3.6604 - accuracy: 0.0853
on_train_batch_begin: 1607808029.192655s

18 step training time: 0.085815s

on_train_batch_end: 1607808029.273488s

38912/50000 [======================>.......] - ETA: 0s - loss: 3.6247 - accuracy: 0.0858
on_train_batch_begin: 1607808029.273876s

19 step training time: 0.081221s

on_train_batch_end: 1607808029.354101s

40960/50000 [=======================>......] - ETA: 0s - loss: 3.5955 - accuracy: 0.0862
on_train_batch_begin: 1607808029.354505s

20 step training time: 0.080629s

on_train_batch_end: 1607808029.435135s

43008/50000 [========================>.....] - ETA: 0s - loss: 3.5648 - accuracy: 0.0867
on_train_batch_begin: 1607808029.435511s

21 step training time: 0.081006s

on_train_batch_end: 1607808029.516836s

45056/50000 [==========================>...] - ETA: 0s - loss: 3.5340 - accuracy: 0.0872
on_train_batch_begin: 1607808029.517216s

22 step training time: 0.081705s

on_train_batch_end: 1607808029.600284s

47104/50000 [===========================>..] - ETA: 0s - loss: 3.4980 - accuracy: 0.0877
on_train_batch_begin: 1607808029.600666s

23 step training time: 0.083450s

on_train_batch_end: 1607808029.678845s

49152/50000 [============================>.] - ETA: 0s - loss: 3.4607 - accuracy: 0.0882
on_train_batch_begin: 1607808029.679226s

24 step training time: 0.078560s

on_train_batch_end: 1607808029.749499s

on_test_batch_begin: 1607808029.829416s

25 step training time: 0.150190s

on_epoch_end: 1607808030.055820s

Validation time: 0.226388s

Real time: 1607808030.055820s

Epoch time: 2.3609883785247803s

50000/50000 [==============================] - 2s 47us/sample - loss: 3.4470 - accuracy: 0.0882 - val_loss: 14.2057 - val_accuracy: 0.0000e+00

on_epoch_begin: 1607808030.056081s

Real time: 1607808030.0560896
Epoch 3/5

on_train_batch_begin: 1607808030.062531s

on_train_batch_end: 1607808030.147926s

 2048/50000 [>.............................] - ETA: 2s - loss: 2.5210 - accuracy: 0.1003
on_train_batch_begin: 1607808030.148307s

1 step training time: 0.085775s

on_train_batch_end: 1607808030.231050s

 4096/50000 [=>............................] - ETA: 1s - loss: 2.4318 - accuracy: 0.1003
on_train_batch_begin: 1607808030.231428s

2 step training time: 0.083121s

on_train_batch_end: 1607808030.312589s

 6144/50000 [==>...........................] - ETA: 1s - loss: 2.4041 - accuracy: 0.1002
on_train_batch_begin: 1607808030.312970s

3 step training time: 0.081542s

on_train_batch_end: 1607808030.395334s

 8192/50000 [===>..........................] - ETA: 1s - loss: 2.4203 - accuracy: 0.0999
on_train_batch_begin: 1607808030.395711s

4 step training time: 0.082741s

on_train_batch_end: 1607808030.476152s

10240/50000 [=====>........................] - ETA: 1s - loss: 2.4404 - accuracy: 0.0997
on_train_batch_begin: 1607808030.476531s

5 step training time: 0.080820s

on_train_batch_end: 1607808030.558144s

12288/50000 [======>.......................] - ETA: 1s - loss: 2.4256 - accuracy: 0.0998
on_train_batch_begin: 1607808030.558548s

6 step training time: 0.082017s

on_train_batch_end: 1607808030.638810s

14336/50000 [=======>......................] - ETA: 1s - loss: 2.3967 - accuracy: 0.0997
on_train_batch_begin: 1607808030.639190s

7 step training time: 0.080642s

on_train_batch_end: 1607808030.719379s

16384/50000 [========>.....................] - ETA: 1s - loss: 2.3513 - accuracy: 0.0998
on_train_batch_begin: 1607808030.719759s

8 step training time: 0.080569s

on_train_batch_end: 1607808030.801741s

18432/50000 [==========>...................] - ETA: 1s - loss: 2.3394 - accuracy: 0.0998
on_train_batch_begin: 1607808030.802118s

9 step training time: 0.082359s

on_train_batch_end: 1607808030.885360s

20480/50000 [===========>..................] - ETA: 1s - loss: 2.3246 - accuracy: 0.0998
on_train_batch_begin: 1607808030.885738s

10 step training time: 0.083621s

on_train_batch_end: 1607808030.968681s

22528/50000 [============>.................] - ETA: 1s - loss: 2.3069 - accuracy: 0.0999
on_train_batch_begin: 1607808030.969062s

11 step training time: 0.083324s

on_train_batch_end: 1607808031.049856s

24576/50000 [=============>................] - ETA: 1s - loss: 2.2932 - accuracy: 0.1000
on_train_batch_begin: 1607808031.050233s

12 step training time: 0.081171s

on_train_batch_end: 1607808031.132675s

26624/50000 [==============>...............] - ETA: 0s - loss: 2.2898 - accuracy: 0.1000
on_train_batch_begin: 1607808031.133060s

13 step training time: 0.082827s

on_train_batch_end: 1607808031.216156s

28672/50000 [================>.............] - ETA: 0s - loss: 2.2721 - accuracy: 0.1000
on_train_batch_begin: 1607808031.216540s

14 step training time: 0.083480s

on_train_batch_end: 1607808031.302207s

30720/50000 [=================>............] - ETA: 0s - loss: 2.2570 - accuracy: 0.1000
on_train_batch_begin: 1607808031.302616s

15 step training time: 0.086076s

on_train_batch_end: 1607808031.381222s

32768/50000 [==================>...........] - ETA: 0s - loss: 2.2383 - accuracy: 0.1000
on_train_batch_begin: 1607808031.381599s

16 step training time: 0.078983s

on_train_batch_end: 1607808031.462604s

34816/50000 [===================>..........] - ETA: 0s - loss: 2.2261 - accuracy: 0.1001
on_train_batch_begin: 1607808031.462984s

17 step training time: 0.081385s

on_train_batch_end: 1607808031.543563s

36864/50000 [=====================>........] - ETA: 0s - loss: 2.2116 - accuracy: 0.1001
on_train_batch_begin: 1607808031.543941s

18 step training time: 0.080957s

on_train_batch_end: 1607808031.625436s

38912/50000 [======================>.......] - ETA: 0s - loss: 2.1928 - accuracy: 0.1001
on_train_batch_begin: 1607808031.625820s

19 step training time: 0.081880s

on_train_batch_end: 1607808031.706907s

40960/50000 [=======================>......] - ETA: 0s - loss: 2.1707 - accuracy: 0.1001
on_train_batch_begin: 1607808031.707286s

20 step training time: 0.081466s

on_train_batch_end: 1607808031.788216s

43008/50000 [========================>.....] - ETA: 0s - loss: 2.1608 - accuracy: 0.1001
on_train_batch_begin: 1607808031.788594s

21 step training time: 0.081308s

on_train_batch_end: 1607808031.871027s

45056/50000 [==========================>...] - ETA: 0s - loss: 2.1448 - accuracy: 0.1001
on_train_batch_begin: 1607808031.871401s

22 step training time: 0.082807s

on_train_batch_end: 1607808031.952297s

47104/50000 [===========================>..] - ETA: 0s - loss: 2.1319 - accuracy: 0.1001
on_train_batch_begin: 1607808031.952673s

23 step training time: 0.081272s

on_train_batch_end: 1607808032.030995s

49152/50000 [============================>.] - ETA: 0s - loss: 2.1170 - accuracy: 0.1001
on_train_batch_begin: 1607808032.031369s

24 step training time: 0.078696s

on_train_batch_end: 1607808032.095911s

on_test_batch_begin: 1607808032.174152s

25 step training time: 0.142783s

on_epoch_end: 1607808032.404951s

Validation time: 0.230783s

Real time: 1607808032.404951s

Epoch time: 2.348881959915161s

50000/50000 [==============================] - 2s 47us/sample - loss: 2.1099 - accuracy: 0.1001 - val_loss: 7.3637 - val_accuracy: 0.1001

on_epoch_begin: 1607808032.405209s

Real time: 1607808032.4052184
Epoch 4/5

on_train_batch_begin: 1607808032.411662s

on_train_batch_end: 1607808032.497334s

 2048/50000 [>.............................] - ETA: 2s - loss: 1.6395 - accuracy: 0.1004
on_train_batch_begin: 1607808032.497714s

1 step training time: 0.086052s

on_train_batch_end: 1607808032.578061s

 4096/50000 [=>............................] - ETA: 1s - loss: 1.6425 - accuracy: 0.1009
on_train_batch_begin: 1607808032.578466s

2 step training time: 0.080753s

on_train_batch_end: 1607808032.657964s

 6144/50000 [==>...........................] - ETA: 1s - loss: 1.6884 - accuracy: 0.1007
on_train_batch_begin: 1607808032.658340s

3 step training time: 0.079873s

on_train_batch_end: 1607808032.738780s

 8192/50000 [===>..........................] - ETA: 1s - loss: 1.6811 - accuracy: 0.1006
on_train_batch_begin: 1607808032.739156s

4 step training time: 0.080817s

on_train_batch_end: 1607808032.821218s

10240/50000 [=====>........................] - ETA: 1s - loss: 1.6992 - accuracy: 0.1005
on_train_batch_begin: 1607808032.821593s

5 step training time: 0.082436s

on_train_batch_end: 1607808032.902136s

12288/50000 [======>.......................] - ETA: 1s - loss: 1.6888 - accuracy: 0.1005
on_train_batch_begin: 1607808032.902538s

6 step training time: 0.080946s

on_train_batch_end: 1607808032.983331s

14336/50000 [=======>......................] - ETA: 1s - loss: 1.6861 - accuracy: 0.1004
on_train_batch_begin: 1607808032.983709s

7 step training time: 0.081171s

on_train_batch_end: 1607808033.068532s

16384/50000 [========>.....................] - ETA: 1s - loss: 1.6936 - accuracy: 0.1004
on_train_batch_begin: 1607808033.068910s

8 step training time: 0.085201s

on_train_batch_end: 1607808033.149899s

18432/50000 [==========>...................] - ETA: 1s - loss: 1.7000 - accuracy: 0.1004
on_train_batch_begin: 1607808033.150273s

9 step training time: 0.081363s

on_train_batch_end: 1607808033.228921s

20480/50000 [===========>..................] - ETA: 1s - loss: 1.6869 - accuracy: 0.1004
on_train_batch_begin: 1607808033.229324s

10 step training time: 0.079051s

on_train_batch_end: 1607808033.312112s

22528/50000 [============>.................] - ETA: 1s - loss: 1.6685 - accuracy: 0.1004
on_train_batch_begin: 1607808033.312490s

11 step training time: 0.083166s

on_train_batch_end: 1607808033.396557s

24576/50000 [=============>................] - ETA: 1s - loss: 1.6613 - accuracy: 0.1004
on_train_batch_begin: 1607808033.396936s

12 step training time: 0.084447s

on_train_batch_end: 1607808033.477695s

26624/50000 [==============>...............] - ETA: 0s - loss: 1.6552 - accuracy: 0.1005
on_train_batch_begin: 1607808033.478074s

13 step training time: 0.081138s

on_train_batch_end: 1607808033.560886s

28672/50000 [================>.............] - ETA: 0s - loss: 1.6455 - accuracy: 0.1005
on_train_batch_begin: 1607808033.561262s

14 step training time: 0.083188s

on_train_batch_end: 1607808033.642042s

30720/50000 [=================>............] - ETA: 0s - loss: 1.6341 - accuracy: 0.1005
on_train_batch_begin: 1607808033.642445s

15 step training time: 0.081182s

on_train_batch_end: 1607808033.721066s

32768/50000 [==================>...........] - ETA: 0s - loss: 1.6256 - accuracy: 0.1005
on_train_batch_begin: 1607808033.721440s

16 step training time: 0.078995s

on_train_batch_end: 1607808033.803019s

34816/50000 [===================>..........] - ETA: 0s - loss: 1.6162 - accuracy: 0.1005
on_train_batch_begin: 1607808033.803399s

17 step training time: 0.081959s

on_train_batch_end: 1607808033.884231s

36864/50000 [=====================>........] - ETA: 0s - loss: 1.6121 - accuracy: 0.1004
on_train_batch_begin: 1607808033.884616s

18 step training time: 0.081217s

on_train_batch_end: 1607808033.970293s

38912/50000 [======================>.......] - ETA: 0s - loss: 1.6096 - accuracy: 0.1004
on_train_batch_begin: 1607808033.970698s

19 step training time: 0.086082s

on_train_batch_end: 1607808034.051663s

40960/50000 [=======================>......] - ETA: 0s - loss: 1.6001 - accuracy: 0.1004
on_train_batch_begin: 1607808034.052041s

20 step training time: 0.081342s

on_train_batch_end: 1607808034.132405s

43008/50000 [========================>.....] - ETA: 0s - loss: 1.5926 - accuracy: 0.1004
on_train_batch_begin: 1607808034.132789s

21 step training time: 0.080748s

on_train_batch_end: 1607808034.214530s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.5830 - accuracy: 0.1005
on_train_batch_begin: 1607808034.214907s

22 step training time: 0.082118s

on_train_batch_end: 1607808034.294840s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.5735 - accuracy: 0.1005
on_train_batch_begin: 1607808034.295231s

23 step training time: 0.080323s

on_train_batch_end: 1607808034.375415s

49152/50000 [============================>.] - ETA: 0s - loss: 1.5690 - accuracy: 0.1005
on_train_batch_begin: 1607808034.375797s

24 step training time: 0.080566s

on_train_batch_end: 1607808034.438613s

on_test_batch_begin: 1607808034.516693s

25 step training time: 0.140896s

on_epoch_end: 1607808034.745470s

Validation time: 0.228761s

Real time: 1607808034.745470s

Epoch time: 2.3402719497680664s

50000/50000 [==============================] - 2s 47us/sample - loss: 1.5687 - accuracy: 0.1005 - val_loss: 6.9177 - val_accuracy: 0.1001

on_epoch_begin: 1607808034.745714s

Real time: 1607808034.7457232
Epoch 5/5

on_train_batch_begin: 1607808034.751430s

on_train_batch_end: 1607808034.836022s

 2048/50000 [>.............................] - ETA: 2s - loss: 1.2109 - accuracy: 0.1008
on_train_batch_begin: 1607808034.836396s

1 step training time: 0.084966s

on_train_batch_end: 1607808034.917040s

 4096/50000 [=>............................] - ETA: 1s - loss: 1.2960 - accuracy: 0.1006
on_train_batch_begin: 1607808034.917416s

2 step training time: 0.081020s

on_train_batch_end: 1607808035.000466s

 6144/50000 [==>...........................] - ETA: 1s - loss: 1.2941 - accuracy: 0.1006
on_train_batch_begin: 1607808035.000840s

3 step training time: 0.083424s

on_train_batch_end: 1607808035.080551s

 8192/50000 [===>..........................] - ETA: 1s - loss: 1.3015 - accuracy: 0.1006
on_train_batch_begin: 1607808035.080927s

4 step training time: 0.080087s

on_train_batch_end: 1607808035.161160s

10240/50000 [=====>........................] - ETA: 1s - loss: 1.2878 - accuracy: 0.1006
on_train_batch_begin: 1607808035.161533s

5 step training time: 0.080606s

on_train_batch_end: 1607808035.241276s

12288/50000 [======>.......................] - ETA: 1s - loss: 1.2791 - accuracy: 0.1006
on_train_batch_begin: 1607808035.241652s

6 step training time: 0.080119s

on_train_batch_end: 1607808035.322830s

14336/50000 [=======>......................] - ETA: 1s - loss: 1.2868 - accuracy: 0.1006
on_train_batch_begin: 1607808035.323203s

7 step training time: 0.081551s

on_train_batch_end: 1607808035.403661s

16384/50000 [========>.....................] - ETA: 1s - loss: 1.2772 - accuracy: 0.1006
on_train_batch_begin: 1607808035.404036s

8 step training time: 0.080832s

on_train_batch_end: 1607808035.486393s

18432/50000 [==========>...................] - ETA: 1s - loss: 1.2683 - accuracy: 0.1005
on_train_batch_begin: 1607808035.486797s

9 step training time: 0.082761s

on_train_batch_end: 1607808035.570553s

20480/50000 [===========>..................] - ETA: 1s - loss: 1.2649 - accuracy: 0.1005
on_train_batch_begin: 1607808035.570925s

10 step training time: 0.084128s

on_train_batch_end: 1607808035.657228s

22528/50000 [============>.................] - ETA: 1s - loss: 1.2586 - accuracy: 0.1005
on_train_batch_begin: 1607808035.657606s

11 step training time: 0.086681s

on_train_batch_end: 1607808035.737583s

24576/50000 [=============>................] - ETA: 1s - loss: 1.2670 - accuracy: 0.1005
on_train_batch_begin: 1607808035.737959s

12 step training time: 0.080353s

on_train_batch_end: 1607808035.819933s

26624/50000 [==============>...............] - ETA: 0s - loss: 1.2707 - accuracy: 0.1005
on_train_batch_begin: 1607808035.820309s

13 step training time: 0.082350s

on_train_batch_end: 1607808035.901800s

28672/50000 [================>.............] - ETA: 0s - loss: 1.2739 - accuracy: 0.1005
on_train_batch_begin: 1607808035.902178s

14 step training time: 0.081868s

on_train_batch_end: 1607808035.982617s

30720/50000 [=================>............] - ETA: 0s - loss: 1.2661 - accuracy: 0.1005
on_train_batch_begin: 1607808035.983011s

15 step training time: 0.080834s

on_train_batch_end: 1607808036.063143s

32768/50000 [==================>...........] - ETA: 0s - loss: 1.2565 - accuracy: 0.1005
on_train_batch_begin: 1607808036.063533s

16 step training time: 0.080522s

on_train_batch_end: 1607808036.144385s

34816/50000 [===================>..........] - ETA: 0s - loss: 1.2495 - accuracy: 0.1005
on_train_batch_begin: 1607808036.144774s

17 step training time: 0.081241s

on_train_batch_end: 1607808036.226787s

36864/50000 [=====================>........] - ETA: 0s - loss: 1.2418 - accuracy: 0.1005
on_train_batch_begin: 1607808036.227177s

18 step training time: 0.082403s

on_train_batch_end: 1607808036.308115s

38912/50000 [======================>.......] - ETA: 0s - loss: 1.2337 - accuracy: 0.1005
on_train_batch_begin: 1607808036.308500s

19 step training time: 0.081323s

on_train_batch_end: 1607808036.390330s

40960/50000 [=======================>......] - ETA: 0s - loss: 1.2301 - accuracy: 0.1005
on_train_batch_begin: 1607808036.390745s

20 step training time: 0.082246s

on_train_batch_end: 1607808036.473713s

43008/50000 [========================>.....] - ETA: 0s - loss: 1.2257 - accuracy: 0.1005
on_train_batch_begin: 1607808036.474096s

21 step training time: 0.083350s

on_train_batch_end: 1607808036.554650s

45056/50000 [==========================>...] - ETA: 0s - loss: 1.2220 - accuracy: 0.1005
on_train_batch_begin: 1607808036.555037s

22 step training time: 0.080942s

on_train_batch_end: 1607808036.635487s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.2175 - accuracy: 0.1005
on_train_batch_begin: 1607808036.635878s

23 step training time: 0.080841s

on_train_batch_end: 1607808036.716835s

49152/50000 [============================>.] - ETA: 0s - loss: 1.2161 - accuracy: 0.1005
on_train_batch_begin: 1607808036.717227s

24 step training time: 0.081348s

on_train_batch_end: 1607808036.778194s

on_test_batch_begin: 1607808036.856925s

25 step training time: 0.139699s

on_epoch_end: 1607808037.088363s

Validation time: 0.231422s

Real time: 1607808037.088363s

Epoch time: 2.3426618576049805s

50000/50000 [==============================] - 2s 47us/sample - loss: 1.2128 - accuracy: 0.1005 - val_loss: 7.3535 - val_accuracy: 0.1001
Tempo do fit: 84.52098369598389