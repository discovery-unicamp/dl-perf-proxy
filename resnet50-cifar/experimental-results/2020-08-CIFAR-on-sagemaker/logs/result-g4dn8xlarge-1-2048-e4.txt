wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:14
   221184/170498071 [..............................] - ETA: 1:07
  1548288/170498071 [..............................] - ETA: 14s 
  4644864/170498071 [..............................] - ETA: 6s 
  8085504/170498071 [>.............................] - ETA: 4s
 11624448/170498071 [=>............................] - ETA: 4s
 15261696/170498071 [=>............................] - ETA: 3s
 18931712/170498071 [==>...........................] - ETA: 3s
 22142976/170498071 [==>...........................] - ETA: 2s
 25501696/170498071 [===>..........................] - ETA: 2s
 29122560/170498071 [====>.........................] - ETA: 2s
 32784384/170498071 [====>.........................] - ETA: 2s
 36184064/170498071 [=====>........................] - ETA: 2s
 39444480/170498071 [=====>........................] - ETA: 2s
 42950656/170498071 [======>.......................] - ETA: 2s
 46587904/170498071 [=======>......................] - ETA: 2s
 50241536/170498071 [=======>......................] - ETA: 2s
 53485568/170498071 [========>.....................] - ETA: 1s
 56893440/170498071 [=========>....................] - ETA: 1s
 60465152/170498071 [=========>....................] - ETA: 1s
 64069632/170498071 [==========>...................] - ETA: 1s
 67477504/170498071 [==========>...................] - ETA: 1s
 70803456/170498071 [===========>..................] - ETA: 1s
 74317824/170498071 [============>.................] - ETA: 1s
 77848576/170498071 [============>.................] - ETA: 1s
 81469440/170498071 [=============>................] - ETA: 1s
 84729856/170498071 [=============>................] - ETA: 1s
 88121344/170498071 [==============>...............] - ETA: 1s
 91643904/170498071 [===============>..............] - ETA: 1s
 95232000/170498071 [===============>..............] - ETA: 1s
 98189312/170498071 [================>.............] - ETA: 1s
101023744/170498071 [================>.............] - ETA: 1s
103940096/170498071 [=================>............] - ETA: 1s
106848256/170498071 [=================>............] - ETA: 1s
109780992/170498071 [==================>...........] - ETA: 0s
112713728/170498071 [==================>...........] - ETA: 0s
115613696/170498071 [===================>..........] - ETA: 0s
118497280/170498071 [===================>..........] - ETA: 0s
121430016/170498071 [====================>.........] - ETA: 0s
124362752/170498071 [====================>.........] - ETA: 0s
127311872/170498071 [=====================>........] - ETA: 0s
130211840/170498071 [=====================>........] - ETA: 0s
133111808/170498071 [======================>.......] - ETA: 0s
136011776/170498071 [======================>.......] - ETA: 0s
138928128/170498071 [=======================>......] - ETA: 0s
141893632/170498071 [=======================>......] - ETA: 0s
144793600/170498071 [========================>.....] - ETA: 0s
147709952/170498071 [========================>.....] - ETA: 0s
150642688/170498071 [=========================>....] - ETA: 0s
153575424/170498071 [==========================>...] - ETA: 0s
156508160/170498071 [==========================>...] - ETA: 0s
159424512/170498071 [===========================>..] - ETA: 0s
162357248/170498071 [===========================>..] - ETA: 0s
165289984/170498071 [============================>.] - ETA: 0s
168206336/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 1s
 6971392/94765736 [=>............................] - ETA: 0s
 9388032/94765736 [=>............................] - ETA: 1s
14860288/94765736 [===>..........................] - ETA: 1s
18841600/94765736 [====>.........................] - ETA: 1s
26091520/94765736 [=======>......................] - ETA: 0s
30793728/94765736 [========>.....................] - ETA: 0s
39297024/94765736 [===========>..................] - ETA: 0s
44490752/94765736 [=============>................] - ETA: 0s
47136768/94765736 [=============>................] - ETA: 0s
53051392/94765736 [===============>..............] - ETA: 0s
56598528/94765736 [================>.............] - ETA: 0s
65667072/94765736 [===================>..........] - ETA: 0s
65986560/94765736 [===================>..........] - ETA: 0s
74498048/94765736 [======================>.......] - ETA: 0s
75440128/94765736 [======================>.......] - ETA: 0s
84893696/94765736 [=========================>....] - ETA: 0s
91529216/94765736 [===========================>..] - ETA: 0s
94773248/94765736 [==============================] - 1s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 14.14636516571045
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1615688041.740129s

Real time: 1615688041.7401457
Epoch 1/5

on_train_batch_begin: 1615688042.499827s

on_train_batch_end: 1615688062.981215s

 2048/50000 [>.............................] - ETA: 8:17 - loss: 18.1931 - accuracy: 1.5306e-04
on_train_batch_begin: 1615688062.981823s

1 step training time: 20.481997s

on_train_batch_end: 1615688063.636996s

 4096/50000 [=>............................] - ETA: 4:05 - loss: 15.3402 - accuracy: 1.9574e-04
on_train_batch_begin: 1615688063.637317s

2 step training time: 0.655494s

on_train_batch_end: 1615688064.292499s

 6144/50000 [==>...........................] - ETA: 2:40 - loss: 13.2051 - accuracy: 5.8603e-04
on_train_batch_begin: 1615688064.292806s

3 step training time: 0.655489s

on_train_batch_end: 1615688064.946959s

 8192/50000 [===>..........................] - ETA: 1:58 - loss: 11.9515 - accuracy: 0.0014    
on_train_batch_begin: 1615688064.947287s

4 step training time: 0.654480s

on_train_batch_end: 1615688065.602231s

10240/50000 [=====>........................] - ETA: 1:32 - loss: 11.1807 - accuracy: 0.0035
on_train_batch_begin: 1615688065.602540s

5 step training time: 0.655254s

on_train_batch_end: 1615688066.262764s

12288/50000 [======>.......................] - ETA: 1:15 - loss: 10.6270 - accuracy: 0.0063
on_train_batch_begin: 1615688066.263062s

6 step training time: 0.660522s

on_train_batch_end: 1615688066.914298s

14336/50000 [=======>......................] - ETA: 1:02 - loss: 10.2324 - accuracy: 0.0111
on_train_batch_begin: 1615688066.914594s

7 step training time: 0.651532s

on_train_batch_end: 1615688067.568377s

16384/50000 [========>.....................] - ETA: 52s - loss: 9.8967 - accuracy: 0.0151  
on_train_batch_begin: 1615688067.568681s

8 step training time: 0.654087s

on_train_batch_end: 1615688068.229808s

18432/50000 [==========>...................] - ETA: 45s - loss: 9.6435 - accuracy: 0.0203
on_train_batch_begin: 1615688068.230115s

9 step training time: 0.661434s

on_train_batch_end: 1615688068.884368s

20480/50000 [===========>..................] - ETA: 39s - loss: 9.4344 - accuracy: 0.0242
on_train_batch_begin: 1615688068.884672s

10 step training time: 0.654557s

on_train_batch_end: 1615688069.539728s

22528/50000 [============>.................] - ETA: 33s - loss: 9.2418 - accuracy: 0.0273
on_train_batch_begin: 1615688069.540027s

11 step training time: 0.655356s

on_train_batch_end: 1615688070.199422s

24576/50000 [=============>................] - ETA: 29s - loss: 9.0747 - accuracy: 0.0303
on_train_batch_begin: 1615688070.199720s

12 step training time: 0.659693s

on_train_batch_end: 1615688070.854741s

26624/50000 [==============>...............] - ETA: 25s - loss: 8.9213 - accuracy: 0.0328
on_train_batch_begin: 1615688070.855046s

13 step training time: 0.655325s

on_train_batch_end: 1615688071.508003s

28672/50000 [================>.............] - ETA: 22s - loss: 8.7875 - accuracy: 0.0357
on_train_batch_begin: 1615688071.508301s

14 step training time: 0.653256s

on_train_batch_end: 1615688072.168800s

30720/50000 [=================>............] - ETA: 19s - loss: 8.6757 - accuracy: 0.0377
on_train_batch_begin: 1615688072.169102s

15 step training time: 0.660801s

on_train_batch_end: 1615688072.820787s

32768/50000 [==================>...........] - ETA: 16s - loss: 8.5592 - accuracy: 0.0399
on_train_batch_begin: 1615688072.821089s

16 step training time: 0.651987s

on_train_batch_end: 1615688073.476629s

34816/50000 [===================>..........] - ETA: 13s - loss: 8.4576 - accuracy: 0.0418
on_train_batch_begin: 1615688073.476928s

17 step training time: 0.655839s

on_train_batch_end: 1615688074.131368s

36864/50000 [=====================>........] - ETA: 11s - loss: 8.3607 - accuracy: 0.0434
on_train_batch_begin: 1615688074.131671s

18 step training time: 0.654743s

on_train_batch_end: 1615688074.788736s

38912/50000 [======================>.......] - ETA: 9s - loss: 8.2702 - accuracy: 0.0447 
on_train_batch_begin: 1615688074.789038s

19 step training time: 0.657367s

on_train_batch_end: 1615688075.444596s

40960/50000 [=======================>......] - ETA: 7s - loss: 8.1890 - accuracy: 0.0461
on_train_batch_begin: 1615688075.444896s

20 step training time: 0.655859s

on_train_batch_end: 1615688076.097245s

43008/50000 [========================>.....] - ETA: 5s - loss: 8.1136 - accuracy: 0.0477
on_train_batch_begin: 1615688076.097543s

21 step training time: 0.652647s

on_train_batch_end: 1615688076.760070s

45056/50000 [==========================>...] - ETA: 3s - loss: 8.0420 - accuracy: 0.0489
on_train_batch_begin: 1615688076.760372s

22 step training time: 0.662829s

on_train_batch_end: 1615688077.414543s

47104/50000 [===========================>..] - ETA: 2s - loss: 7.9700 - accuracy: 0.0501
on_train_batch_begin: 1615688077.414843s

23 step training time: 0.654471s

on_train_batch_end: 1615688078.063722s

49152/50000 [============================>.] - ETA: 0s - loss: 7.8980 - accuracy: 0.0515
on_train_batch_begin: 1615688078.064027s

24 step training time: 0.649184s

on_train_batch_end: 1615688083.942665s

on_test_batch_begin: 1615688084.130979s

25 step training time: 6.066952s

on_epoch_end: 1615688089.314376s

Validation time: 5.183380s

Real time: 1615688089.314376s

Epoch time: 47.574246644973755s

50000/50000 [==============================] - 48s 951us/sample - loss: 7.8680 - accuracy: 0.0517 - val_loss: 142.6094 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615688089.314581s

Real time: 1615688089.3145864
Epoch 2/5

on_train_batch_begin: 1615688089.318031s

on_train_batch_end: 1615688089.973721s

 2048/50000 [>.............................] - ETA: 15s - loss: 6.0130 - accuracy: 0.0738
on_train_batch_begin: 1615688089.974029s

1 step training time: 0.655998s

on_train_batch_end: 1615688090.641397s

 4096/50000 [=>............................] - ETA: 14s - loss: 6.0176 - accuracy: 0.0786
on_train_batch_begin: 1615688090.641701s

2 step training time: 0.667672s

on_train_batch_end: 1615688091.298981s

 6144/50000 [==>...........................] - ETA: 14s - loss: 6.0300 - accuracy: 0.0762
on_train_batch_begin: 1615688091.299306s

3 step training time: 0.657605s

on_train_batch_end: 1615688091.963531s

 8192/50000 [===>..........................] - ETA: 13s - loss: 5.9783 - accuracy: 0.0763
on_train_batch_begin: 1615688091.963835s

4 step training time: 0.664528s

on_train_batch_end: 1615688092.622505s

10240/50000 [=====>........................] - ETA: 12s - loss: 5.9564 - accuracy: 0.0772
on_train_batch_begin: 1615688092.622806s

5 step training time: 0.658971s

on_train_batch_end: 1615688093.281544s

12288/50000 [======>.......................] - ETA: 12s - loss: 5.9370 - accuracy: 0.0765
on_train_batch_begin: 1615688093.281854s

6 step training time: 0.659048s

on_train_batch_end: 1615688093.943347s

14336/50000 [=======>......................] - ETA: 11s - loss: 5.9018 - accuracy: 0.0762
on_train_batch_begin: 1615688093.943655s

7 step training time: 0.661801s

on_train_batch_end: 1615688094.600042s

16384/50000 [========>.....................] - ETA: 10s - loss: 5.8607 - accuracy: 0.0768
on_train_batch_begin: 1615688094.600347s

8 step training time: 0.656692s

on_train_batch_end: 1615688095.266635s

18432/50000 [==========>...................] - ETA: 10s - loss: 5.8315 - accuracy: 0.0764
on_train_batch_begin: 1615688095.266934s

9 step training time: 0.666587s

on_train_batch_end: 1615688095.921843s

20480/50000 [===========>..................] - ETA: 9s - loss: 5.8128 - accuracy: 0.0760 
on_train_batch_begin: 1615688095.922148s

10 step training time: 0.655214s

on_train_batch_end: 1615688096.586374s

22528/50000 [============>.................] - ETA: 8s - loss: 5.8020 - accuracy: 0.0760
on_train_batch_begin: 1615688096.586676s

11 step training time: 0.664528s

on_train_batch_end: 1615688097.246872s

24576/50000 [=============>................] - ETA: 8s - loss: 5.7816 - accuracy: 0.0750
on_train_batch_begin: 1615688097.247172s

12 step training time: 0.660496s

on_train_batch_end: 1615688097.908711s

26624/50000 [==============>...............] - ETA: 7s - loss: 5.7583 - accuracy: 0.0750
on_train_batch_begin: 1615688097.909009s

13 step training time: 0.661837s

on_train_batch_end: 1615688098.572813s

28672/50000 [================>.............] - ETA: 6s - loss: 5.7324 - accuracy: 0.0752
on_train_batch_begin: 1615688098.573113s

14 step training time: 0.664104s

on_train_batch_end: 1615688099.238692s

30720/50000 [=================>............] - ETA: 6s - loss: 5.7041 - accuracy: 0.0748
on_train_batch_begin: 1615688099.238999s

15 step training time: 0.665885s

on_train_batch_end: 1615688099.899024s

32768/50000 [==================>...........] - ETA: 5s - loss: 5.6833 - accuracy: 0.0750
on_train_batch_begin: 1615688099.899354s

16 step training time: 0.660355s

on_train_batch_end: 1615688100.556566s

34816/50000 [===================>..........] - ETA: 4s - loss: 5.6631 - accuracy: 0.0749
on_train_batch_begin: 1615688100.556867s

17 step training time: 0.657513s

on_train_batch_end: 1615688101.220735s

36864/50000 [=====================>........] - ETA: 4s - loss: 5.6365 - accuracy: 0.0749
on_train_batch_begin: 1615688101.221038s

18 step training time: 0.664171s

on_train_batch_end: 1615688101.878080s

38912/50000 [======================>.......] - ETA: 3s - loss: 5.6194 - accuracy: 0.0746
on_train_batch_begin: 1615688101.878389s

19 step training time: 0.657351s

on_train_batch_end: 1615688102.542733s

40960/50000 [=======================>......] - ETA: 2s - loss: 5.6031 - accuracy: 0.0742
on_train_batch_begin: 1615688102.543037s

20 step training time: 0.664648s

on_train_batch_end: 1615688103.206536s

43008/50000 [========================>.....] - ETA: 2s - loss: 5.5827 - accuracy: 0.0741
on_train_batch_begin: 1615688103.206837s

21 step training time: 0.663800s

on_train_batch_end: 1615688103.867768s

45056/50000 [==========================>...] - ETA: 1s - loss: 5.5665 - accuracy: 0.0735
on_train_batch_begin: 1615688103.868070s

22 step training time: 0.661233s

on_train_batch_end: 1615688104.533297s

47104/50000 [===========================>..] - ETA: 0s - loss: 5.5501 - accuracy: 0.0732
on_train_batch_begin: 1615688104.533618s

23 step training time: 0.665548s

on_train_batch_end: 1615688105.196941s

49152/50000 [============================>.] - ETA: 0s - loss: 5.5366 - accuracy: 0.0727
on_train_batch_begin: 1615688105.197247s

24 step training time: 0.663630s

on_train_batch_end: 1615688105.473902s

on_test_batch_begin: 1615688105.538103s

25 step training time: 0.340856s

on_epoch_end: 1615688106.379844s

Validation time: 0.841726s

Real time: 1615688106.379844s

Epoch time: 17.065274238586426s

50000/50000 [==============================] - 17s 341us/sample - loss: 5.5311 - accuracy: 0.0726 - val_loss: 7.9548 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615688106.380034s

Real time: 1615688106.3800395
Epoch 3/5

on_train_batch_begin: 1615688106.383480s

on_train_batch_end: 1615688107.038221s

 2048/50000 [>.............................] - ETA: 15s - loss: 5.0967 - accuracy: 0.0580
on_train_batch_begin: 1615688107.038523s

1 step training time: 0.655043s

on_train_batch_end: 1615688107.704535s

 4096/50000 [=>............................] - ETA: 14s - loss: 5.0197 - accuracy: 0.0590
on_train_batch_begin: 1615688107.704836s

2 step training time: 0.666313s

on_train_batch_end: 1615688108.362266s

 6144/50000 [==>...........................] - ETA: 14s - loss: 5.0089 - accuracy: 0.0601
on_train_batch_begin: 1615688108.362570s

3 step training time: 0.657734s

on_train_batch_end: 1615688109.027339s

 8192/50000 [===>..........................] - ETA: 13s - loss: 5.0150 - accuracy: 0.0611
on_train_batch_begin: 1615688109.027643s

4 step training time: 0.665073s

on_train_batch_end: 1615688109.684711s

10240/50000 [=====>........................] - ETA: 12s - loss: 4.9865 - accuracy: 0.0619
on_train_batch_begin: 1615688109.685013s

5 step training time: 0.657370s

on_train_batch_end: 1615688110.349986s

12288/50000 [======>.......................] - ETA: 12s - loss: 4.9725 - accuracy: 0.0619
on_train_batch_begin: 1615688110.350292s

6 step training time: 0.665280s

on_train_batch_end: 1615688111.003628s

14336/50000 [=======>......................] - ETA: 11s - loss: 4.9704 - accuracy: 0.0613
on_train_batch_begin: 1615688111.003930s

7 step training time: 0.653638s

on_train_batch_end: 1615688111.668770s

16384/50000 [========>.....................] - ETA: 10s - loss: 4.9476 - accuracy: 0.0608
on_train_batch_begin: 1615688111.669071s

8 step training time: 0.665141s

on_train_batch_end: 1615688112.329118s

18432/50000 [==========>...................] - ETA: 10s - loss: 4.9313 - accuracy: 0.0602
on_train_batch_begin: 1615688112.329415s

9 step training time: 0.660343s

on_train_batch_end: 1615688112.991035s

20480/50000 [===========>..................] - ETA: 9s - loss: 4.9225 - accuracy: 0.0596 
on_train_batch_begin: 1615688112.991360s

10 step training time: 0.661945s

on_train_batch_end: 1615688113.653573s

22528/50000 [============>.................] - ETA: 8s - loss: 4.9091 - accuracy: 0.0591
on_train_batch_begin: 1615688113.653871s

11 step training time: 0.662511s

on_train_batch_end: 1615688114.318325s

24576/50000 [=============>................] - ETA: 8s - loss: 4.8792 - accuracy: 0.0590
on_train_batch_begin: 1615688114.318628s

12 step training time: 0.664757s

on_train_batch_end: 1615688114.977286s

26624/50000 [==============>...............] - ETA: 7s - loss: 4.8547 - accuracy: 0.0588
on_train_batch_begin: 1615688114.977589s

13 step training time: 0.658961s

on_train_batch_end: 1615688115.642497s

28672/50000 [================>.............] - ETA: 6s - loss: 4.8305 - accuracy: 0.0587
on_train_batch_begin: 1615688115.642804s

14 step training time: 0.665215s

on_train_batch_end: 1615688116.304699s

30720/50000 [=================>............] - ETA: 6s - loss: 4.8068 - accuracy: 0.0588
on_train_batch_begin: 1615688116.305003s

15 step training time: 0.662199s

on_train_batch_end: 1615688116.965239s

32768/50000 [==================>...........] - ETA: 5s - loss: 4.7773 - accuracy: 0.0588
on_train_batch_begin: 1615688116.965543s

16 step training time: 0.660539s

on_train_batch_end: 1615688117.630187s

34816/50000 [===================>..........] - ETA: 4s - loss: 4.7471 - accuracy: 0.0590
on_train_batch_begin: 1615688117.630494s

17 step training time: 0.664951s

on_train_batch_end: 1615688118.292318s

36864/50000 [=====================>........] - ETA: 4s - loss: 4.7197 - accuracy: 0.0592
on_train_batch_begin: 1615688118.292617s

18 step training time: 0.662123s

on_train_batch_end: 1615688118.956925s

38912/50000 [======================>.......] - ETA: 3s - loss: 4.6939 - accuracy: 0.0593
on_train_batch_begin: 1615688118.957225s

19 step training time: 0.664609s

on_train_batch_end: 1615688119.617002s

40960/50000 [=======================>......] - ETA: 2s - loss: 4.6658 - accuracy: 0.0595
on_train_batch_begin: 1615688119.617307s

20 step training time: 0.660082s

on_train_batch_end: 1615688120.284258s

43008/50000 [========================>.....] - ETA: 2s - loss: 4.6431 - accuracy: 0.0597
on_train_batch_begin: 1615688120.284559s

21 step training time: 0.667251s

on_train_batch_end: 1615688120.940674s

45056/50000 [==========================>...] - ETA: 1s - loss: 4.6273 - accuracy: 0.0598
on_train_batch_begin: 1615688120.941015s

22 step training time: 0.656457s

on_train_batch_end: 1615688121.605715s

47104/50000 [===========================>..] - ETA: 0s - loss: 4.5997 - accuracy: 0.0602
on_train_batch_begin: 1615688121.606013s

23 step training time: 0.664997s

on_train_batch_end: 1615688122.258831s

49152/50000 [============================>.] - ETA: 0s - loss: 4.5765 - accuracy: 0.0604
on_train_batch_begin: 1615688122.259130s

24 step training time: 0.653118s

on_train_batch_end: 1615688122.545364s

on_test_batch_begin: 1615688122.609916s

25 step training time: 0.350786s

on_epoch_end: 1615688123.456440s

Validation time: 0.846508s

Real time: 1615688123.456440s

Epoch time: 17.076417207717896s

50000/50000 [==============================] - 17s 342us/sample - loss: 4.5661 - accuracy: 0.0604 - val_loss: 7.6216 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615688123.456632s

Real time: 1615688123.456637
Epoch 4/5

on_train_batch_begin: 1615688123.460100s

on_train_batch_end: 1615688124.121350s

 2048/50000 [>.............................] - ETA: 15s - loss: 3.8404 - accuracy: 0.0694
on_train_batch_begin: 1615688124.121653s

1 step training time: 0.661553s

on_train_batch_end: 1615688124.790316s

 4096/50000 [=>............................] - ETA: 14s - loss: 3.7897 - accuracy: 0.0698
on_train_batch_begin: 1615688124.790617s

2 step training time: 0.668964s

on_train_batch_end: 1615688125.458817s

 6144/50000 [==>...........................] - ETA: 14s - loss: 3.7634 - accuracy: 0.0698
on_train_batch_begin: 1615688125.459120s

3 step training time: 0.668504s

on_train_batch_end: 1615688126.127497s

 8192/50000 [===>..........................] - ETA: 13s - loss: 3.7195 - accuracy: 0.0699
on_train_batch_begin: 1615688126.127797s

4 step training time: 0.668677s

on_train_batch_end: 1615688126.789573s

10240/50000 [=====>........................] - ETA: 12s - loss: 3.6881 - accuracy: 0.0703
on_train_batch_begin: 1615688126.789870s

5 step training time: 0.662074s

on_train_batch_end: 1615688127.459925s

12288/50000 [======>.......................] - ETA: 12s - loss: 3.6279 - accuracy: 0.0712
on_train_batch_begin: 1615688127.460225s

6 step training time: 0.670354s

on_train_batch_end: 1615688128.118472s

14336/50000 [=======>......................] - ETA: 11s - loss: 3.6058 - accuracy: 0.0716
on_train_batch_begin: 1615688128.118778s

7 step training time: 0.658553s

on_train_batch_end: 1615688128.789809s

16384/50000 [========>.....................] - ETA: 10s - loss: 3.5674 - accuracy: 0.0722
on_train_batch_begin: 1615688128.790108s

8 step training time: 0.671331s

on_train_batch_end: 1615688129.456391s

18432/50000 [==========>...................] - ETA: 10s - loss: 3.5298 - accuracy: 0.0729
on_train_batch_begin: 1615688129.456689s

9 step training time: 0.666581s

on_train_batch_end: 1615688130.125302s

20480/50000 [===========>..................] - ETA: 9s - loss: 3.4981 - accuracy: 0.0736 
on_train_batch_begin: 1615688130.125601s

10 step training time: 0.668912s

on_train_batch_end: 1615688130.791740s

22528/50000 [============>.................] - ETA: 8s - loss: 3.4603 - accuracy: 0.0744
on_train_batch_begin: 1615688130.792037s

11 step training time: 0.666435s

on_train_batch_end: 1615688131.458813s

24576/50000 [=============>................] - ETA: 8s - loss: 3.4140 - accuracy: 0.0752
on_train_batch_begin: 1615688131.459115s

12 step training time: 0.667078s

on_train_batch_end: 1615688132.126151s

26624/50000 [==============>...............] - ETA: 7s - loss: 3.3698 - accuracy: 0.0760
on_train_batch_begin: 1615688132.126451s

13 step training time: 0.667336s

on_train_batch_end: 1615688132.792542s

28672/50000 [================>.............] - ETA: 6s - loss: 3.3325 - accuracy: 0.0766
on_train_batch_begin: 1615688132.792844s

14 step training time: 0.666393s

on_train_batch_end: 1615688133.460420s

30720/50000 [=================>............] - ETA: 6s - loss: 3.2943 - accuracy: 0.0775
on_train_batch_begin: 1615688133.460720s

15 step training time: 0.667876s

on_train_batch_end: 1615688134.130799s

32768/50000 [==================>...........] - ETA: 5s - loss: 3.2675 - accuracy: 0.0781
on_train_batch_begin: 1615688134.131102s

16 step training time: 0.670382s

on_train_batch_end: 1615688134.796282s

34816/50000 [===================>..........] - ETA: 4s - loss: 3.2424 - accuracy: 0.0786
on_train_batch_begin: 1615688134.796586s

17 step training time: 0.665485s

on_train_batch_end: 1615688135.471004s

36864/50000 [=====================>........] - ETA: 4s - loss: 3.2060 - accuracy: 0.0793
on_train_batch_begin: 1615688135.471326s

18 step training time: 0.674740s

on_train_batch_end: 1615688136.136233s

38912/50000 [======================>.......] - ETA: 3s - loss: 3.1730 - accuracy: 0.0799
on_train_batch_begin: 1615688136.136536s

19 step training time: 0.665210s

on_train_batch_end: 1615688136.807863s

40960/50000 [=======================>......] - ETA: 2s - loss: 3.1411 - accuracy: 0.0806
on_train_batch_begin: 1615688136.808164s

20 step training time: 0.671628s

on_train_batch_end: 1615688137.472245s

43008/50000 [========================>.....] - ETA: 2s - loss: 3.1010 - accuracy: 0.0813
on_train_batch_begin: 1615688137.472562s

21 step training time: 0.664398s

on_train_batch_end: 1615688138.139096s

45056/50000 [==========================>...] - ETA: 1s - loss: 3.0685 - accuracy: 0.0820
on_train_batch_begin: 1615688138.139424s

22 step training time: 0.666862s

on_train_batch_end: 1615688138.806176s

47104/50000 [===========================>..] - ETA: 0s - loss: 3.0427 - accuracy: 0.0827
on_train_batch_begin: 1615688138.806476s

23 step training time: 0.667052s

on_train_batch_end: 1615688139.468657s

49152/50000 [============================>.] - ETA: 0s - loss: 3.0114 - accuracy: 0.0833
on_train_batch_begin: 1615688139.468958s

24 step training time: 0.662482s

on_train_batch_end: 1615688139.750793s

on_test_batch_begin: 1615688139.815252s

25 step training time: 0.346294s

on_epoch_end: 1615688140.662191s

Validation time: 0.846924s

Real time: 1615688140.662191s

Epoch time: 17.205570220947266s

50000/50000 [==============================] - 17s 344us/sample - loss: 2.9989 - accuracy: 0.0834 - val_loss: 8.5730 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615688140.662381s

Real time: 1615688140.6623867
Epoch 5/5

on_train_batch_begin: 1615688140.665740s

on_train_batch_end: 1615688141.330031s

 2048/50000 [>.............................] - ETA: 15s - loss: 2.2018 - accuracy: 0.0991
on_train_batch_begin: 1615688141.330334s

1 step training time: 0.664595s

on_train_batch_end: 1615688141.995708s

 4096/50000 [=>............................] - ETA: 14s - loss: 2.0842 - accuracy: 0.0991
on_train_batch_begin: 1615688141.996006s

2 step training time: 0.665672s

on_train_batch_end: 1615688142.661357s

 6144/50000 [==>...........................] - ETA: 14s - loss: 2.1275 - accuracy: 0.0993
on_train_batch_begin: 1615688142.661651s

3 step training time: 0.665644s

on_train_batch_end: 1615688143.331586s

 8192/50000 [===>..........................] - ETA: 13s - loss: 2.0953 - accuracy: 0.0993
on_train_batch_begin: 1615688143.331882s

4 step training time: 0.670231s

on_train_batch_end: 1615688143.999888s

10240/50000 [=====>........................] - ETA: 12s - loss: 2.0708 - accuracy: 0.0993
on_train_batch_begin: 1615688144.000190s

5 step training time: 0.668308s

on_train_batch_end: 1615688144.668909s

12288/50000 [======>.......................] - ETA: 12s - loss: 2.0516 - accuracy: 0.0994
on_train_batch_begin: 1615688144.669210s

6 step training time: 0.669020s

on_train_batch_end: 1615688145.339795s

14336/50000 [=======>......................] - ETA: 11s - loss: 2.0392 - accuracy: 0.0995
on_train_batch_begin: 1615688145.340090s

7 step training time: 0.670881s

on_train_batch_end: 1615688146.005920s

16384/50000 [========>.....................] - ETA: 10s - loss: 2.0195 - accuracy: 0.0995
on_train_batch_begin: 1615688146.006222s

8 step training time: 0.666131s

on_train_batch_end: 1615688146.678561s

18432/50000 [==========>...................] - ETA: 10s - loss: 2.0009 - accuracy: 0.0995
on_train_batch_begin: 1615688146.678858s

9 step training time: 0.672637s

on_train_batch_end: 1615688147.346097s

20480/50000 [===========>..................] - ETA: 9s - loss: 1.9686 - accuracy: 0.0995 
on_train_batch_begin: 1615688147.346393s

10 step training time: 0.667535s

on_train_batch_end: 1615688148.016535s

22528/50000 [============>.................] - ETA: 8s - loss: 1.9474 - accuracy: 0.0995
on_train_batch_begin: 1615688148.016833s

11 step training time: 0.670440s

on_train_batch_end: 1615688148.685559s

24576/50000 [=============>................] - ETA: 8s - loss: 1.9261 - accuracy: 0.0996
on_train_batch_begin: 1615688148.685861s

12 step training time: 0.669028s

on_train_batch_end: 1615688149.356482s

26624/50000 [==============>...............] - ETA: 7s - loss: 1.9115 - accuracy: 0.0996
on_train_batch_begin: 1615688149.356788s

13 step training time: 0.670927s

on_train_batch_end: 1615688150.024512s

28672/50000 [================>.............] - ETA: 6s - loss: 1.8879 - accuracy: 0.0996
on_train_batch_begin: 1615688150.024809s

14 step training time: 0.668021s

on_train_batch_end: 1615688150.691028s

30720/50000 [=================>............] - ETA: 6s - loss: 1.8615 - accuracy: 0.0997
on_train_batch_begin: 1615688150.691351s

15 step training time: 0.666542s

on_train_batch_end: 1615688151.363918s

32768/50000 [==================>...........] - ETA: 5s - loss: 1.8400 - accuracy: 0.0997
on_train_batch_begin: 1615688151.364217s

16 step training time: 0.672865s

on_train_batch_end: 1615688152.030411s

34816/50000 [===================>..........] - ETA: 4s - loss: 1.8321 - accuracy: 0.0997
on_train_batch_begin: 1615688152.030714s

17 step training time: 0.666497s

on_train_batch_end: 1615688152.695048s

36864/50000 [=====================>........] - ETA: 4s - loss: 1.8222 - accuracy: 0.0998
on_train_batch_begin: 1615688152.695365s

18 step training time: 0.664652s

on_train_batch_end: 1615688153.365390s

38912/50000 [======================>.......] - ETA: 3s - loss: 1.8009 - accuracy: 0.0998
on_train_batch_begin: 1615688153.365690s

19 step training time: 0.670324s

on_train_batch_end: 1615688154.034423s

40960/50000 [=======================>......] - ETA: 2s - loss: 1.7901 - accuracy: 0.0998
on_train_batch_begin: 1615688154.034716s

20 step training time: 0.669027s

on_train_batch_end: 1615688154.705253s

43008/50000 [========================>.....] - ETA: 2s - loss: 1.7796 - accuracy: 0.0998
on_train_batch_begin: 1615688154.705550s

21 step training time: 0.670834s

on_train_batch_end: 1615688155.375101s

45056/50000 [==========================>...] - ETA: 1s - loss: 1.7661 - accuracy: 0.0998
on_train_batch_begin: 1615688155.375417s

22 step training time: 0.669867s

on_train_batch_end: 1615688156.044217s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.7558 - accuracy: 0.0998
on_train_batch_begin: 1615688156.044512s

23 step training time: 0.669095s

on_train_batch_end: 1615688156.708617s

49152/50000 [============================>.] - ETA: 0s - loss: 1.7428 - accuracy: 0.0998
on_train_batch_begin: 1615688156.708918s

24 step training time: 0.664407s

on_train_batch_end: 1615688156.995448s

on_test_batch_begin: 1615688157.059336s

25 step training time: 0.350418s

on_epoch_end: 1615688157.921988s

Validation time: 0.862637s

Real time: 1615688157.921988s

Epoch time: 17.2596173286438s

50000/50000 [==============================] - 17s 345us/sample - loss: 1.7357 - accuracy: 0.0998 - val_loss: 8.1300 - val_accuracy: 0.1001
Tempo do fit: 119.7107617855072