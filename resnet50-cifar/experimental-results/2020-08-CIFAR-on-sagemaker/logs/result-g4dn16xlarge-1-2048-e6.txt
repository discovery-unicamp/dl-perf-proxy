wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:34
   204800/170498071 [..............................] - ETA: 1:17
  1138688/170498071 [..............................] - ETA: 21s 
  3743744/170498071 [..............................] - ETA: 8s 
  7053312/170498071 [>.............................] - ETA: 5s
 10379264/170498071 [>.............................] - ETA: 4s
 13688832/170498071 [=>............................] - ETA: 3s
 16982016/170498071 [=>............................] - ETA: 3s
 20307968/170498071 [==>...........................] - ETA: 3s
 23552000/170498071 [===>..........................] - ETA: 3s
 26697728/170498071 [===>..........................] - ETA: 2s
 29876224/170498071 [====>.........................] - ETA: 2s
 33120256/170498071 [====>.........................] - ETA: 2s
 36413440/170498071 [=====>........................] - ETA: 2s
 39690240/170498071 [=====>........................] - ETA: 2s
 42999808/170498071 [======>.......................] - ETA: 2s
 46292992/170498071 [=======>......................] - ETA: 2s
 49602560/170498071 [=======>......................] - ETA: 2s
 52879360/170498071 [========>.....................] - ETA: 2s
 56205312/170498071 [========>.....................] - ETA: 2s
 59482112/170498071 [=========>....................] - ETA: 1s
 62676992/170498071 [==========>...................] - ETA: 1s
 65921024/170498071 [==========>...................] - ETA: 1s
 69230592/170498071 [===========>..................] - ETA: 1s
 72556544/170498071 [===========>..................] - ETA: 1s
 75882496/170498071 [============>.................] - ETA: 1s
 79159296/170498071 [============>.................] - ETA: 1s
 82452480/170498071 [=============>................] - ETA: 1s
 85712896/170498071 [==============>...............] - ETA: 1s
 88891392/170498071 [==============>...............] - ETA: 1s
 91807744/170498071 [===============>..............] - ETA: 1s
 94396416/170498071 [===============>..............] - ETA: 1s
 96886784/170498071 [================>.............] - ETA: 1s
 99491840/170498071 [================>.............] - ETA: 1s
102031360/170498071 [================>.............] - ETA: 1s
104701952/170498071 [=================>............] - ETA: 1s
107372544/170498071 [=================>............] - ETA: 1s
109912064/170498071 [==================>...........] - ETA: 1s
112517120/170498071 [==================>...........] - ETA: 1s
115040256/170498071 [===================>..........] - ETA: 0s
117645312/170498071 [===================>..........] - ETA: 0s
120250368/170498071 [====================>.........] - ETA: 0s
122855424/170498071 [====================>.........] - ETA: 0s
125460480/170498071 [=====================>........] - ETA: 0s
128065536/170498071 [=====================>........] - ETA: 0s
130670592/170498071 [=====================>........] - ETA: 0s
133275648/170498071 [======================>.......] - ETA: 0s
135864320/170498071 [======================>.......] - ETA: 0s
138469376/170498071 [=======================>......] - ETA: 0s
141074432/170498071 [=======================>......] - ETA: 0s
143761408/170498071 [========================>.....] - ETA: 0s
146415616/170498071 [========================>.....] - ETA: 0s
149020672/170498071 [=========================>....] - ETA: 0s
151625728/170498071 [=========================>....] - ETA: 0s
154107904/170498071 [==========================>...] - ETA: 0s
156688384/170498071 [==========================>...] - ETA: 0s
159424512/170498071 [===========================>..] - ETA: 0s
162111488/170498071 [===========================>..] - ETA: 0s
164634624/170498071 [===========================>..] - ETA: 0s
167124992/170498071 [============================>.] - ETA: 0s
169582592/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 1s
 5267456/94765736 [>.............................] - ETA: 0s
 9388032/94765736 [=>............................] - ETA: 1s
14712832/94765736 [===>..........................] - ETA: 0s
18841600/94765736 [====>.........................] - ETA: 1s
21594112/94765736 [=====>........................] - ETA: 2s
23961600/94765736 [======>.......................] - ETA: 1s
28286976/94765736 [=======>......................] - ETA: 2s
35921920/94765736 [==========>...................] - ETA: 1s
37683200/94765736 [==========>...................] - ETA: 1s
42082304/94765736 [============>.................] - ETA: 1s
47136768/94765736 [=============>................] - ETA: 1s
53477376/94765736 [===============>..............] - ETA: 1s
56598528/94765736 [================>.............] - ETA: 0s
63807488/94765736 [===================>..........] - ETA: 0s
65986560/94765736 [===================>..........] - ETA: 0s
73850880/94765736 [======================>.......] - ETA: 0s
75440128/94765736 [======================>.......] - ETA: 0s
83918848/94765736 [=========================>....] - ETA: 0s
84893696/94765736 [=========================>....] - ETA: 0s
93691904/94765736 [============================>.] - ETA: 0s
94281728/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 2s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 14.538599252700806
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1615860206.228050s

Real time: 1615860206.2280712
Epoch 1/5

on_train_batch_begin: 1615860206.993749s

on_train_batch_end: 1615860227.425367s

 2048/50000 [>.............................] - ETA: 8:16 - loss: 17.7087 - accuracy: 1.0204e-04
on_train_batch_begin: 1615860227.426066s

1 step training time: 20.432317s

on_train_batch_end: 1615860228.068513s

 4096/50000 [=>............................] - ETA: 4:04 - loss: 14.7106 - accuracy: 2.3484e-04
on_train_batch_begin: 1615860228.068882s

2 step training time: 0.642816s

on_train_batch_end: 1615860228.716120s

 6144/50000 [==>...........................] - ETA: 2:40 - loss: 12.7766 - accuracy: 4.4282e-04
on_train_batch_begin: 1615860228.716441s

3 step training time: 0.647559s

on_train_batch_end: 1615860229.354111s

 8192/50000 [===>..........................] - ETA: 1:58 - loss: 11.6575 - accuracy: 0.0016    
on_train_batch_begin: 1615860229.354429s

4 step training time: 0.637988s

on_train_batch_end: 1615860230.002302s

10240/50000 [=====>........................] - ETA: 1:32 - loss: 10.9331 - accuracy: 0.0037
on_train_batch_begin: 1615860230.002614s

5 step training time: 0.648185s

on_train_batch_end: 1615860230.645958s

12288/50000 [======>.......................] - ETA: 1:14 - loss: 10.4025 - accuracy: 0.0081
on_train_batch_begin: 1615860230.646265s

6 step training time: 0.643651s

on_train_batch_end: 1615860231.293945s

14336/50000 [=======>......................] - ETA: 1:02 - loss: 10.0266 - accuracy: 0.0129
on_train_batch_begin: 1615860231.294257s

7 step training time: 0.647992s

on_train_batch_end: 1615860231.939809s

16384/50000 [========>.....................] - ETA: 52s - loss: 9.7235 - accuracy: 0.0178  
on_train_batch_begin: 1615860231.940124s

8 step training time: 0.645867s

on_train_batch_end: 1615860232.583875s

18432/50000 [==========>...................] - ETA: 45s - loss: 9.4929 - accuracy: 0.0224
on_train_batch_begin: 1615860232.584191s

9 step training time: 0.644066s

on_train_batch_end: 1615860233.227808s

20480/50000 [===========>..................] - ETA: 38s - loss: 9.3008 - accuracy: 0.0261
on_train_batch_begin: 1615860233.228125s

10 step training time: 0.643934s

on_train_batch_end: 1615860233.876191s

22528/50000 [============>.................] - ETA: 33s - loss: 9.1329 - accuracy: 0.0296
on_train_batch_begin: 1615860233.876501s

11 step training time: 0.648376s

on_train_batch_end: 1615860234.521195s

24576/50000 [=============>................] - ETA: 29s - loss: 8.9857 - accuracy: 0.0324
on_train_batch_begin: 1615860234.521501s

12 step training time: 0.645000s

on_train_batch_end: 1615860235.170912s

26624/50000 [==============>...............] - ETA: 25s - loss: 8.8632 - accuracy: 0.0351
on_train_batch_begin: 1615860235.171227s

13 step training time: 0.649726s

on_train_batch_end: 1615860235.812261s

28672/50000 [================>.............] - ETA: 22s - loss: 8.7468 - accuracy: 0.0383
on_train_batch_begin: 1615860235.812571s

14 step training time: 0.641344s

on_train_batch_end: 1615860236.458348s

30720/50000 [=================>............] - ETA: 18s - loss: 8.6553 - accuracy: 0.0401
on_train_batch_begin: 1615860236.458656s

15 step training time: 0.646085s

on_train_batch_end: 1615860237.106481s

32768/50000 [==================>...........] - ETA: 16s - loss: 8.5617 - accuracy: 0.0420
on_train_batch_begin: 1615860237.106791s

16 step training time: 0.648135s

on_train_batch_end: 1615860237.752776s

34816/50000 [===================>..........] - ETA: 13s - loss: 8.4720 - accuracy: 0.0432
on_train_batch_begin: 1615860237.753089s

17 step training time: 0.646297s

on_train_batch_end: 1615860238.402394s

36864/50000 [=====================>........] - ETA: 11s - loss: 8.3902 - accuracy: 0.0449
on_train_batch_begin: 1615860238.402705s

18 step training time: 0.649616s

on_train_batch_end: 1615860239.044861s

38912/50000 [======================>.......] - ETA: 9s - loss: 8.3157 - accuracy: 0.0462 
on_train_batch_begin: 1615860239.045199s

19 step training time: 0.642494s

on_train_batch_end: 1615860239.687678s

40960/50000 [=======================>......] - ETA: 7s - loss: 8.2503 - accuracy: 0.0477
on_train_batch_begin: 1615860239.687994s

20 step training time: 0.642795s

on_train_batch_end: 1615860240.336503s

43008/50000 [========================>.....] - ETA: 5s - loss: 8.1877 - accuracy: 0.0484
on_train_batch_begin: 1615860240.336815s

21 step training time: 0.648821s

on_train_batch_end: 1615860240.983218s

45056/50000 [==========================>...] - ETA: 3s - loss: 8.1293 - accuracy: 0.0491
on_train_batch_begin: 1615860240.983531s

22 step training time: 0.646717s

on_train_batch_end: 1615860241.632435s

47104/50000 [===========================>..] - ETA: 2s - loss: 8.0755 - accuracy: 0.0502
on_train_batch_begin: 1615860241.632746s

23 step training time: 0.649215s

on_train_batch_end: 1615860242.274741s

49152/50000 [============================>.] - ETA: 0s - loss: 8.0242 - accuracy: 0.0515
on_train_batch_begin: 1615860242.275057s

24 step training time: 0.642310s

on_train_batch_end: 1615860248.023280s

on_test_batch_begin: 1615860248.215387s

25 step training time: 5.940330s

on_epoch_end: 1615860253.562649s

Validation time: 5.347246s

Real time: 1615860253.562649s

Epoch time: 47.33459663391113s

50000/50000 [==============================] - 47s 947us/sample - loss: 8.0045 - accuracy: 0.0516 - val_loss: 505752.6256 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615860253.562869s

Real time: 1615860253.562875
Epoch 2/5

on_train_batch_begin: 1615860253.566517s

on_train_batch_end: 1615860254.213702s

 2048/50000 [>.............................] - ETA: 15s - loss: 6.8128 - accuracy: 0.0849
on_train_batch_begin: 1615860254.214025s

1 step training time: 0.647508s

on_train_batch_end: 1615860254.863150s

 4096/50000 [=>............................] - ETA: 14s - loss: 6.8263 - accuracy: 0.0860
on_train_batch_begin: 1615860254.863455s

2 step training time: 0.649429s

on_train_batch_end: 1615860255.515128s

 6144/50000 [==>...........................] - ETA: 13s - loss: 6.8278 - accuracy: 0.0869
on_train_batch_begin: 1615860255.515433s

3 step training time: 0.651978s

on_train_batch_end: 1615860256.165074s

 8192/50000 [===>..........................] - ETA: 13s - loss: 6.8313 - accuracy: 0.0899
on_train_batch_begin: 1615860256.165400s

4 step training time: 0.649967s

on_train_batch_end: 1615860256.814208s

10240/50000 [=====>........................] - ETA: 12s - loss: 6.8130 - accuracy: 0.0897
on_train_batch_begin: 1615860256.814512s

5 step training time: 0.649112s

on_train_batch_end: 1615860257.464068s

12288/50000 [======>.......................] - ETA: 11s - loss: 6.7949 - accuracy: 0.0901
on_train_batch_begin: 1615860257.464379s

6 step training time: 0.649867s

on_train_batch_end: 1615860258.112567s

14336/50000 [=======>......................] - ETA: 11s - loss: 6.7968 - accuracy: 0.0909
on_train_batch_begin: 1615860258.112871s

7 step training time: 0.648492s

on_train_batch_end: 1615860258.761057s

16384/50000 [========>.....................] - ETA: 10s - loss: 6.7813 - accuracy: 0.0919
on_train_batch_begin: 1615860258.761385s

8 step training time: 0.648514s

on_train_batch_end: 1615860259.408024s

18432/50000 [==========>...................] - ETA: 10s - loss: 6.7647 - accuracy: 0.0920
on_train_batch_begin: 1615860259.408334s

9 step training time: 0.646950s

on_train_batch_end: 1615860260.056488s

20480/50000 [===========>..................] - ETA: 9s - loss: 6.7700 - accuracy: 0.0920 
on_train_batch_begin: 1615860260.056794s

10 step training time: 0.648459s

on_train_batch_end: 1615860260.704484s

22528/50000 [============>.................] - ETA: 8s - loss: 6.7641 - accuracy: 0.0924
on_train_batch_begin: 1615860260.704794s

11 step training time: 0.648000s

on_train_batch_end: 1615860261.352760s

24576/50000 [=============>................] - ETA: 8s - loss: 6.7498 - accuracy: 0.0931
on_train_batch_begin: 1615860261.353063s

12 step training time: 0.648269s

on_train_batch_end: 1615860262.002797s

26624/50000 [==============>...............] - ETA: 7s - loss: 6.7372 - accuracy: 0.0936
on_train_batch_begin: 1615860262.003110s

13 step training time: 0.650047s

on_train_batch_end: 1615860262.650682s

28672/50000 [================>.............] - ETA: 6s - loss: 6.7282 - accuracy: 0.0946
on_train_batch_begin: 1615860262.650987s

14 step training time: 0.647877s

on_train_batch_end: 1615860263.301686s

30720/50000 [=================>............] - ETA: 6s - loss: 6.7150 - accuracy: 0.0945
on_train_batch_begin: 1615860263.302008s

15 step training time: 0.651021s

on_train_batch_end: 1615860263.950638s

32768/50000 [==================>...........] - ETA: 5s - loss: 6.7111 - accuracy: 0.0950
on_train_batch_begin: 1615860263.950955s

16 step training time: 0.648947s

on_train_batch_end: 1615860264.580473s

34816/50000 [===================>..........] - ETA: 4s - loss: 6.7048 - accuracy: 0.0943
on_train_batch_begin: 1615860264.580798s

17 step training time: 0.629843s

on_train_batch_end: 1615860265.237895s

36864/50000 [=====================>........] - ETA: 4s - loss: 6.6970 - accuracy: 0.0940
on_train_batch_begin: 1615860265.238202s

18 step training time: 0.657404s

on_train_batch_end: 1615860265.883976s

38912/50000 [======================>.......] - ETA: 3s - loss: 6.6869 - accuracy: 0.0941
on_train_batch_begin: 1615860265.884289s

19 step training time: 0.646086s

on_train_batch_end: 1615860266.534745s

40960/50000 [=======================>......] - ETA: 2s - loss: 6.6810 - accuracy: 0.0940
on_train_batch_begin: 1615860266.535053s

20 step training time: 0.650765s

on_train_batch_end: 1615860267.183444s

43008/50000 [========================>.....] - ETA: 2s - loss: 6.6705 - accuracy: 0.0938
on_train_batch_begin: 1615860267.183753s

21 step training time: 0.648700s

on_train_batch_end: 1615860267.832804s

45056/50000 [==========================>...] - ETA: 1s - loss: 6.6670 - accuracy: 0.0939
on_train_batch_begin: 1615860267.833132s

22 step training time: 0.649378s

on_train_batch_end: 1615860268.480836s

47104/50000 [===========================>..] - ETA: 0s - loss: 6.6584 - accuracy: 0.0940
on_train_batch_begin: 1615860268.481170s

23 step training time: 0.648039s

on_train_batch_end: 1615860269.129270s

49152/50000 [============================>.] - ETA: 0s - loss: 6.6532 - accuracy: 0.0939
on_train_batch_begin: 1615860269.129574s

24 step training time: 0.648404s

on_train_batch_end: 1615860269.407577s

on_test_batch_begin: 1615860269.532231s

25 step training time: 0.402657s

on_epoch_end: 1615860270.469404s

Validation time: 0.937158s

Real time: 1615860270.469404s

Epoch time: 16.906546115875244s

50000/50000 [==============================] - 17s 338us/sample - loss: 6.6501 - accuracy: 0.0939 - val_loss: 101334.9029 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615860270.469602s

Real time: 1615860270.469607
Epoch 3/5

on_train_batch_begin: 1615860270.473182s

on_train_batch_end: 1615860271.128097s

 2048/50000 [>.............................] - ETA: 15s - loss: 6.3448 - accuracy: 0.0903
on_train_batch_begin: 1615860271.128408s

1 step training time: 0.655226s

on_train_batch_end: 1615860271.783518s

 4096/50000 [=>............................] - ETA: 14s - loss: 6.4020 - accuracy: 0.0908
on_train_batch_begin: 1615860271.783833s

2 step training time: 0.655425s

on_train_batch_end: 1615860272.436953s

 6144/50000 [==>...........................] - ETA: 14s - loss: 6.4139 - accuracy: 0.0904
on_train_batch_begin: 1615860272.437283s

3 step training time: 0.653451s

on_train_batch_end: 1615860273.096932s

 8192/50000 [===>..........................] - ETA: 13s - loss: 6.4233 - accuracy: 0.0897
on_train_batch_begin: 1615860273.097264s

4 step training time: 0.659981s

on_train_batch_end: 1615860273.750280s

10240/50000 [=====>........................] - ETA: 12s - loss: 6.4199 - accuracy: 0.0907
on_train_batch_begin: 1615860273.750583s

5 step training time: 0.653319s

on_train_batch_end: 1615860274.400226s

12288/50000 [======>.......................] - ETA: 12s - loss: 6.4089 - accuracy: 0.0905
on_train_batch_begin: 1615860274.400539s

6 step training time: 0.649956s

on_train_batch_end: 1615860275.051813s

14336/50000 [=======>......................] - ETA: 11s - loss: 6.4043 - accuracy: 0.0888
on_train_batch_begin: 1615860275.052124s

7 step training time: 0.651586s

on_train_batch_end: 1615860275.706943s

16384/50000 [========>.....................] - ETA: 10s - loss: 6.4046 - accuracy: 0.0880
on_train_batch_begin: 1615860275.707252s

8 step training time: 0.655127s

on_train_batch_end: 1615860276.362202s

18432/50000 [==========>...................] - ETA: 10s - loss: 6.3951 - accuracy: 0.0878
on_train_batch_begin: 1615860276.362509s

9 step training time: 0.655258s

on_train_batch_end: 1615860277.018314s

20480/50000 [===========>..................] - ETA: 9s - loss: 6.3921 - accuracy: 0.0871 
on_train_batch_begin: 1615860277.018625s

10 step training time: 0.656116s

on_train_batch_end: 1615860277.669245s

22528/50000 [============>.................] - ETA: 8s - loss: 6.3865 - accuracy: 0.0868
on_train_batch_begin: 1615860277.669557s

11 step training time: 0.650931s

on_train_batch_end: 1615860278.327375s

24576/50000 [=============>................] - ETA: 8s - loss: 6.3833 - accuracy: 0.0863
on_train_batch_begin: 1615860278.327682s

12 step training time: 0.658126s

on_train_batch_end: 1615860278.979986s

26624/50000 [==============>...............] - ETA: 7s - loss: 6.3789 - accuracy: 0.0860
on_train_batch_begin: 1615860278.980293s

13 step training time: 0.652611s

on_train_batch_end: 1615860279.634568s

28672/50000 [================>.............] - ETA: 6s - loss: 6.3758 - accuracy: 0.0857
on_train_batch_begin: 1615860279.634870s

14 step training time: 0.654577s

on_train_batch_end: 1615860280.291806s

30720/50000 [=================>............] - ETA: 6s - loss: 6.3755 - accuracy: 0.0858
on_train_batch_begin: 1615860280.292117s

15 step training time: 0.657246s

on_train_batch_end: 1615860280.946450s

32768/50000 [==================>...........] - ETA: 5s - loss: 6.3696 - accuracy: 0.0858
on_train_batch_begin: 1615860280.946754s

16 step training time: 0.654638s

on_train_batch_end: 1615860281.600533s

34816/50000 [===================>..........] - ETA: 4s - loss: 6.3613 - accuracy: 0.0855
on_train_batch_begin: 1615860281.600851s

17 step training time: 0.654097s

on_train_batch_end: 1615860282.260401s

36864/50000 [=====================>........] - ETA: 4s - loss: 6.3559 - accuracy: 0.0849
on_train_batch_begin: 1615860282.260711s

18 step training time: 0.659860s

on_train_batch_end: 1615860282.912186s

38912/50000 [======================>.......] - ETA: 3s - loss: 6.3535 - accuracy: 0.0846
on_train_batch_begin: 1615860282.912494s

19 step training time: 0.651783s

on_train_batch_end: 1615860283.572577s

40960/50000 [=======================>......] - ETA: 2s - loss: 6.3498 - accuracy: 0.0848
on_train_batch_begin: 1615860283.572886s

20 step training time: 0.660392s

on_train_batch_end: 1615860284.228395s

43008/50000 [========================>.....] - ETA: 2s - loss: 6.3447 - accuracy: 0.0841
on_train_batch_begin: 1615860284.228707s

21 step training time: 0.655821s

on_train_batch_end: 1615860284.887518s

45056/50000 [==========================>...] - ETA: 1s - loss: 6.3416 - accuracy: 0.0834
on_train_batch_begin: 1615860284.887826s

22 step training time: 0.659119s

on_train_batch_end: 1615860285.546514s

47104/50000 [===========================>..] - ETA: 0s - loss: 6.3337 - accuracy: 0.0826
on_train_batch_begin: 1615860285.546819s

23 step training time: 0.658993s

on_train_batch_end: 1615860286.197931s

49152/50000 [============================>.] - ETA: 0s - loss: 6.3281 - accuracy: 0.0817
on_train_batch_begin: 1615860286.198236s

24 step training time: 0.651417s

on_train_batch_end: 1615860286.481163s

on_test_batch_begin: 1615860286.613391s

25 step training time: 0.415155s

on_epoch_end: 1615860287.560786s

Validation time: 0.947381s

Real time: 1615860287.560786s

Epoch time: 17.091195106506348s

50000/50000 [==============================] - 17s 342us/sample - loss: 6.3251 - accuracy: 0.0815 - val_loss: 1772.7945 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615860287.560983s

Real time: 1615860287.5609887
Epoch 4/5

on_train_batch_begin: 1615860287.564561s

on_train_batch_end: 1615860288.217630s

 2048/50000 [>.............................] - ETA: 15s - loss: 6.1173 - accuracy: 0.0578
on_train_batch_begin: 1615860288.217937s

1 step training time: 0.653377s

on_train_batch_end: 1615860288.877156s

 4096/50000 [=>............................] - ETA: 14s - loss: 6.1172 - accuracy: 0.0582
on_train_batch_begin: 1615860288.877461s

2 step training time: 0.659524s

on_train_batch_end: 1615860289.548931s

 6144/50000 [==>...........................] - ETA: 14s - loss: 6.1147 - accuracy: 0.0536
on_train_batch_begin: 1615860289.549258s

3 step training time: 0.671797s

on_train_batch_end: 1615860290.207782s

 8192/50000 [===>..........................] - ETA: 13s - loss: 6.0847 - accuracy: 0.0515
on_train_batch_begin: 1615860290.208085s

4 step training time: 0.658827s

on_train_batch_end: 1615860290.870545s

10240/50000 [=====>........................] - ETA: 12s - loss: 6.0527 - accuracy: 0.0504
on_train_batch_begin: 1615860290.870860s

5 step training time: 0.662775s

on_train_batch_end: 1615860291.526937s

12288/50000 [======>.......................] - ETA: 12s - loss: 6.0378 - accuracy: 0.0494
on_train_batch_begin: 1615860291.527244s

6 step training time: 0.656384s

on_train_batch_end: 1615860292.194358s

14336/50000 [=======>......................] - ETA: 11s - loss: 6.0005 - accuracy: 0.0489
on_train_batch_begin: 1615860292.194671s

7 step training time: 0.667427s

on_train_batch_end: 1615860292.850528s

16384/50000 [========>.....................] - ETA: 10s - loss: 5.9563 - accuracy: 0.0481
on_train_batch_begin: 1615860292.850850s

8 step training time: 0.656179s

on_train_batch_end: 1615860293.515635s

18432/50000 [==========>...................] - ETA: 10s - loss: 5.9191 - accuracy: 0.0478
on_train_batch_begin: 1615860293.515942s

9 step training time: 0.665092s

on_train_batch_end: 1615860294.173951s

20480/50000 [===========>..................] - ETA: 9s - loss: 5.8641 - accuracy: 0.0482 
on_train_batch_begin: 1615860294.174258s

10 step training time: 0.658316s

on_train_batch_end: 1615860294.838284s

22528/50000 [============>.................] - ETA: 8s - loss: 5.8178 - accuracy: 0.0486
on_train_batch_begin: 1615860294.838590s

11 step training time: 0.664332s

on_train_batch_end: 1615860295.497978s

24576/50000 [=============>................] - ETA: 8s - loss: 5.7673 - accuracy: 0.0490
on_train_batch_begin: 1615860295.498285s

12 step training time: 0.659695s

on_train_batch_end: 1615860296.159012s

26624/50000 [==============>...............] - ETA: 7s - loss: 5.7151 - accuracy: 0.0496
on_train_batch_begin: 1615860296.159313s

13 step training time: 0.661028s

on_train_batch_end: 1615860296.822409s

28672/50000 [================>.............] - ETA: 6s - loss: 5.6627 - accuracy: 0.0499
on_train_batch_begin: 1615860296.822718s

14 step training time: 0.663405s

on_train_batch_end: 1615860297.483266s

30720/50000 [=================>............] - ETA: 6s - loss: 5.6066 - accuracy: 0.0508
on_train_batch_begin: 1615860297.483574s

15 step training time: 0.660855s

on_train_batch_end: 1615860298.149909s

32768/50000 [==================>...........] - ETA: 5s - loss: 5.5440 - accuracy: 0.0516
on_train_batch_begin: 1615860298.150215s

16 step training time: 0.666641s

on_train_batch_end: 1615860298.810261s

34816/50000 [===================>..........] - ETA: 4s - loss: 5.4823 - accuracy: 0.0525
on_train_batch_begin: 1615860298.810575s

17 step training time: 0.660360s

on_train_batch_end: 1615860299.472423s

36864/50000 [=====================>........] - ETA: 4s - loss: 5.4274 - accuracy: 0.0535
on_train_batch_begin: 1615860299.472731s

18 step training time: 0.662156s

on_train_batch_end: 1615860300.134671s

38912/50000 [======================>.......] - ETA: 3s - loss: 5.3684 - accuracy: 0.0547
on_train_batch_begin: 1615860300.134981s

19 step training time: 0.662250s

on_train_batch_end: 1615860300.800502s

40960/50000 [=======================>......] - ETA: 2s - loss: 5.3089 - accuracy: 0.0558
on_train_batch_begin: 1615860300.800816s

20 step training time: 0.665835s

on_train_batch_end: 1615860301.462730s

43008/50000 [========================>.....] - ETA: 2s - loss: 5.2542 - accuracy: 0.0570
on_train_batch_begin: 1615860301.463038s

21 step training time: 0.662222s

on_train_batch_end: 1615860302.130941s

45056/50000 [==========================>...] - ETA: 1s - loss: 5.1923 - accuracy: 0.0583
on_train_batch_begin: 1615860302.131253s

22 step training time: 0.668214s

on_train_batch_end: 1615860302.793519s

47104/50000 [===========================>..] - ETA: 0s - loss: 5.1352 - accuracy: 0.0596
on_train_batch_begin: 1615860302.793827s

23 step training time: 0.662575s

on_train_batch_end: 1615860303.454926s

49152/50000 [============================>.] - ETA: 0s - loss: 5.0732 - accuracy: 0.0609
on_train_batch_begin: 1615860303.455247s

24 step training time: 0.661420s

on_train_batch_end: 1615860303.742812s

on_test_batch_begin: 1615860303.878596s

25 step training time: 0.423349s

on_epoch_end: 1615860304.819778s

Validation time: 0.941167s

Real time: 1615860304.819778s

Epoch time: 17.258806943893433s

50000/50000 [==============================] - 17s 345us/sample - loss: 5.0485 - accuracy: 0.0612 - val_loss: 7.9543 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615860304.819974s

Real time: 1615860304.8199792
Epoch 5/5

on_train_batch_begin: 1615860304.823473s

on_train_batch_end: 1615860305.484594s

 2048/50000 [>.............................] - ETA: 15s - loss: 3.3538 - accuracy: 0.0937
on_train_batch_begin: 1615860305.484906s

1 step training time: 0.661433s

on_train_batch_end: 1615860306.149076s

 4096/50000 [=>............................] - ETA: 14s - loss: 3.2860 - accuracy: 0.0953
on_train_batch_begin: 1615860306.149416s

2 step training time: 0.664511s

on_train_batch_end: 1615860306.813687s

 6144/50000 [==>...........................] - ETA: 14s - loss: 3.2092 - accuracy: 0.0950
on_train_batch_begin: 1615860306.813994s

3 step training time: 0.664578s

on_train_batch_end: 1615860307.473405s

 8192/50000 [===>..........................] - ETA: 13s - loss: 3.1596 - accuracy: 0.0951
on_train_batch_begin: 1615860307.473711s

4 step training time: 0.659716s

on_train_batch_end: 1615860308.141378s

10240/50000 [=====>........................] - ETA: 12s - loss: 3.1251 - accuracy: 0.0951
on_train_batch_begin: 1615860308.141682s

5 step training time: 0.667972s

on_train_batch_end: 1615860308.808943s

12288/50000 [======>.......................] - ETA: 12s - loss: 3.0926 - accuracy: 0.0952
on_train_batch_begin: 1615860308.809281s

6 step training time: 0.667598s

on_train_batch_end: 1615860309.469600s

14336/50000 [=======>......................] - ETA: 11s - loss: 3.0665 - accuracy: 0.0955
on_train_batch_begin: 1615860309.469909s

7 step training time: 0.660628s

on_train_batch_end: 1615860310.131446s

16384/50000 [========>.....................] - ETA: 10s - loss: 3.0358 - accuracy: 0.0959
on_train_batch_begin: 1615860310.131754s

8 step training time: 0.661846s

on_train_batch_end: 1615860310.799030s

18432/50000 [==========>...................] - ETA: 10s - loss: 3.0366 - accuracy: 0.0961
on_train_batch_begin: 1615860310.799340s

9 step training time: 0.667585s

on_train_batch_end: 1615860311.466986s

20480/50000 [===========>..................] - ETA: 9s - loss: 3.0131 - accuracy: 0.0963 
on_train_batch_begin: 1615860311.467289s

10 step training time: 0.667949s

on_train_batch_end: 1615860312.136499s

22528/50000 [============>.................] - ETA: 8s - loss: 2.9951 - accuracy: 0.0965
on_train_batch_begin: 1615860312.136808s

11 step training time: 0.669519s

on_train_batch_end: 1615860312.803352s

24576/50000 [=============>................] - ETA: 8s - loss: 2.9674 - accuracy: 0.0967
on_train_batch_begin: 1615860312.803663s

12 step training time: 0.666855s

on_train_batch_end: 1615860313.471023s

26624/50000 [==============>...............] - ETA: 7s - loss: 2.9464 - accuracy: 0.0970
on_train_batch_begin: 1615860313.471332s

13 step training time: 0.667669s

on_train_batch_end: 1615860314.141660s

28672/50000 [================>.............] - ETA: 6s - loss: 2.9274 - accuracy: 0.0972
on_train_batch_begin: 1615860314.141972s

14 step training time: 0.670640s

on_train_batch_end: 1615860314.814582s

30720/50000 [=================>............] - ETA: 6s - loss: 2.9041 - accuracy: 0.0974
on_train_batch_begin: 1615860314.814886s

15 step training time: 0.672914s

on_train_batch_end: 1615860315.486922s

32768/50000 [==================>...........] - ETA: 5s - loss: 2.8879 - accuracy: 0.0976
on_train_batch_begin: 1615860315.487227s

16 step training time: 0.672341s

on_train_batch_end: 1615860316.151013s

34816/50000 [===================>..........] - ETA: 4s - loss: 2.8754 - accuracy: 0.0978
on_train_batch_begin: 1615860316.151321s

17 step training time: 0.664095s

on_train_batch_end: 1615860316.819287s

36864/50000 [=====================>........] - ETA: 4s - loss: 2.8687 - accuracy: 0.0979
on_train_batch_begin: 1615860316.819596s

18 step training time: 0.668274s

on_train_batch_end: 1615860317.483547s

38912/50000 [======================>.......] - ETA: 3s - loss: 2.8499 - accuracy: 0.0979
on_train_batch_begin: 1615860317.483857s

19 step training time: 0.664262s

on_train_batch_end: 1615860318.149544s

40960/50000 [=======================>......] - ETA: 2s - loss: 2.8459 - accuracy: 0.0980
on_train_batch_begin: 1615860318.149854s

20 step training time: 0.665997s

on_train_batch_end: 1615860318.820194s

43008/50000 [========================>.....] - ETA: 2s - loss: 2.8346 - accuracy: 0.0982
on_train_batch_begin: 1615860318.820507s

21 step training time: 0.670653s

on_train_batch_end: 1615860319.483057s

45056/50000 [==========================>...] - ETA: 1s - loss: 2.8242 - accuracy: 0.0983
on_train_batch_begin: 1615860319.483358s

22 step training time: 0.662852s

on_train_batch_end: 1615860320.158545s

47104/50000 [===========================>..] - ETA: 0s - loss: 2.8160 - accuracy: 0.0983
on_train_batch_begin: 1615860320.158849s

23 step training time: 0.675491s

on_train_batch_end: 1615860320.819849s

49152/50000 [============================>.] - ETA: 0s - loss: 2.8027 - accuracy: 0.0984
on_train_batch_begin: 1615860320.820153s

24 step training time: 0.661304s

on_train_batch_end: 1615860321.108924s

on_test_batch_begin: 1615860321.245346s

25 step training time: 0.425193s

on_epoch_end: 1615860322.194613s

Validation time: 0.949252s

Real time: 1615860322.194613s

Epoch time: 17.37464928627014s

50000/50000 [==============================] - 17s 347us/sample - loss: 2.8033 - accuracy: 0.0984 - val_loss: 7.6227 - val_accuracy: 0.0000e+00
Tempo do fit: 119.62147235870361