wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:16
   122880/170498071 [..............................] - ETA: 1:41
   532480/170498071 [..............................] - ETA: 39s 
  1810432/170498071 [..............................] - ETA: 16s
  3350528/170498071 [..............................] - ETA: 11s
  5038080/170498071 [..............................] - ETA: 9s 
  6692864/170498071 [>.............................] - ETA: 7s
  8298496/170498071 [>.............................] - ETA: 7s
 10657792/170498071 [>.............................] - ETA: 6s
 13262848/170498071 [=>............................] - ETA: 5s
 15966208/170498071 [=>............................] - ETA: 5s
 18931712/170498071 [==>...........................] - ETA: 4s
 22192128/170498071 [==>...........................] - ETA: 4s
 25501696/170498071 [===>..........................] - ETA: 3s
 28794880/170498071 [====>.........................] - ETA: 3s
 31940608/170498071 [====>.........................] - ETA: 3s
 35201024/170498071 [=====>........................] - ETA: 3s
 38510592/170498071 [=====>........................] - ETA: 3s
 41803776/170498071 [======>.......................] - ETA: 2s
 45129728/170498071 [======>.......................] - ETA: 2s
 48455680/170498071 [=======>......................] - ETA: 2s
 51798016/170498071 [========>.....................] - ETA: 2s
 55042048/170498071 [========>.....................] - ETA: 2s
 58302464/170498071 [=========>....................] - ETA: 2s
 61628416/170498071 [=========>....................] - ETA: 2s
 64806912/170498071 [==========>...................] - ETA: 2s
 68001792/170498071 [==========>...................] - ETA: 2s
 71294976/170498071 [===========>..................] - ETA: 1s
 74571776/170498071 [============>.................] - ETA: 1s
 77897728/170498071 [============>.................] - ETA: 1s
 81190912/170498071 [=============>................] - ETA: 1s
 84516864/170498071 [=============>................] - ETA: 1s
 87859200/170498071 [==============>...............] - ETA: 1s
 91152384/170498071 [===============>..............] - ETA: 1s
 94445568/170498071 [===============>..............] - ETA: 1s
 97624064/170498071 [================>.............] - ETA: 1s
100851712/170498071 [================>.............] - ETA: 1s
104185856/170498071 [=================>............] - ETA: 1s
107454464/170498071 [=================>............] - ETA: 1s
110747648/170498071 [==================>...........] - ETA: 1s
114106368/170498071 [===================>..........] - ETA: 1s
117448704/170498071 [===================>..........] - ETA: 0s
120807424/170498071 [====================>.........] - ETA: 0s
124018688/170498071 [====================>.........] - ETA: 0s
127344640/170498071 [=====================>........] - ETA: 0s
130523136/170498071 [=====================>........] - ETA: 0s
133718016/170498071 [======================>.......] - ETA: 0s
137027584/170498071 [=======================>......] - ETA: 0s
140222464/170498071 [=======================>......] - ETA: 0s
143286272/170498071 [========================>.....] - ETA: 0s
146276352/170498071 [========================>.....] - ETA: 0s
149200896/170498071 [=========================>....] - ETA: 0s
152141824/170498071 [=========================>....] - ETA: 0s
155066368/170498071 [==========================>...] - ETA: 0s
157982720/170498071 [==========================>...] - ETA: 0s
160948224/170498071 [===========================>..] - ETA: 0s
163856384/170498071 [===========================>..] - ETA: 0s
166846464/170498071 [============================>.] - ETA: 0s
169795584/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 1s
 2162688/94765736 [..............................] - ETA: 2s
 5300224/94765736 [>.............................] - ETA: 1s
 8642560/94765736 [=>............................] - ETA: 1s
11001856/94765736 [==>...........................] - ETA: 1s
14409728/94765736 [===>..........................] - ETA: 1s
18382848/94765736 [====>.........................] - ETA: 1s
19677184/94765736 [=====>........................] - ETA: 1s
21757952/94765736 [=====>........................] - ETA: 1s
23617536/94765736 [======>.......................] - ETA: 1s
27688960/94765736 [=======>......................] - ETA: 1s
30425088/94765736 [========>.....................] - ETA: 1s
31342592/94765736 [========>.....................] - ETA: 1s
33021952/94765736 [=========>....................] - ETA: 1s
36216832/94765736 [==========>...................] - ETA: 1s
38141952/94765736 [===========>..................] - ETA: 1s
41426944/94765736 [============>.................] - ETA: 1s
44908544/94765736 [=============>................] - ETA: 1s
47136768/94765736 [=============>................] - ETA: 1s
49840128/94765736 [==============>...............] - ETA: 1s
49963008/94765736 [==============>...............] - ETA: 1s
52436992/94765736 [===============>..............] - ETA: 1s
55541760/94765736 [================>.............] - ETA: 0s
57319424/94765736 [=================>............] - ETA: 0s
59400192/94765736 [=================>............] - ETA: 0s
61390848/94765736 [==================>...........] - ETA: 0s
64954368/94765736 [===================>..........] - ETA: 0s
66510848/94765736 [====================>.........] - ETA: 0s
67231744/94765736 [====================>.........] - ETA: 0s
68214784/94765736 [====================>.........] - ETA: 0s
70893568/94765736 [=====================>........] - ETA: 0s
73490432/94765736 [======================>.......] - ETA: 0s
74440704/94765736 [======================>.......] - ETA: 0s
76840960/94765736 [=======================>......] - ETA: 0s
82444288/94765736 [=========================>....] - ETA: 0s
84893696/94765736 [=========================>....] - ETA: 0s
84951040/94765736 [=========================>....] - ETA: 0s
87670784/94765736 [==========================>...] - ETA: 0s
89096192/94765736 [===========================>..] - ETA: 0s
90398720/94765736 [===========================>..] - ETA: 0s
92725248/94765736 [============================>.] - ETA: 0s
93700096/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 3s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 16.975399494171143
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1615686133.328522s

Real time: 1615686133.3285413
Epoch 1/5

on_train_batch_begin: 1615686134.097779s

on_train_batch_end: 1615686154.431753s

 2048/50000 [>.............................] - ETA: 8:14 - loss: 18.0304 - accuracy: 4.3297e-04
on_train_batch_begin: 1615686154.432366s

1 step training time: 20.334587s

on_train_batch_end: 1615686155.072335s

 4096/50000 [=>............................] - ETA: 4:03 - loss: 14.2988 - accuracy: 4.7135e-04
on_train_batch_begin: 1615686155.072655s

2 step training time: 0.640289s

on_train_batch_end: 1615686155.712443s

 6144/50000 [==>...........................] - ETA: 2:39 - loss: 12.3747 - accuracy: 7.4760e-04
on_train_batch_begin: 1615686155.712749s

3 step training time: 0.640094s

on_train_batch_end: 1615686156.358610s

 8192/50000 [===>..........................] - ETA: 1:57 - loss: 11.3147 - accuracy: 0.0018    
on_train_batch_begin: 1615686156.358916s

4 step training time: 0.646167s

on_train_batch_end: 1615686156.999656s

10240/50000 [=====>........................] - ETA: 1:31 - loss: 10.6778 - accuracy: 0.0047
on_train_batch_begin: 1615686156.999960s

5 step training time: 0.641044s

on_train_batch_end: 1615686157.642696s

12288/50000 [======>.......................] - ETA: 1:14 - loss: 10.2046 - accuracy: 0.0095
on_train_batch_begin: 1615686157.643002s

6 step training time: 0.643042s

on_train_batch_end: 1615686158.284395s

14336/50000 [=======>......................] - ETA: 1:02 - loss: 9.8689 - accuracy: 0.0139 
on_train_batch_begin: 1615686158.284695s

7 step training time: 0.641693s

on_train_batch_end: 1615686158.930456s

16384/50000 [========>.....................] - ETA: 52s - loss: 9.5824 - accuracy: 0.0183 
on_train_batch_begin: 1615686158.930764s

8 step training time: 0.646069s

on_train_batch_end: 1615686159.568679s

18432/50000 [==========>...................] - ETA: 44s - loss: 9.3608 - accuracy: 0.0217
on_train_batch_begin: 1615686159.568984s

9 step training time: 0.638221s

on_train_batch_end: 1615686160.214303s

20480/50000 [===========>..................] - ETA: 38s - loss: 9.1740 - accuracy: 0.0244
on_train_batch_begin: 1615686160.214604s

10 step training time: 0.645619s

on_train_batch_end: 1615686160.854330s

22528/50000 [============>.................] - ETA: 33s - loss: 9.0109 - accuracy: 0.0274
on_train_batch_begin: 1615686160.854637s

11 step training time: 0.640034s

on_train_batch_end: 1615686161.505610s

24576/50000 [=============>................] - ETA: 29s - loss: 8.8651 - accuracy: 0.0294
on_train_batch_begin: 1615686161.505920s

12 step training time: 0.651283s

on_train_batch_end: 1615686162.146107s

26624/50000 [==============>...............] - ETA: 25s - loss: 8.7465 - accuracy: 0.0312
on_train_batch_begin: 1615686162.146415s

13 step training time: 0.640495s

on_train_batch_end: 1615686162.793109s

28672/50000 [================>.............] - ETA: 21s - loss: 8.6320 - accuracy: 0.0333
on_train_batch_begin: 1615686162.793412s

14 step training time: 0.646996s

on_train_batch_end: 1615686163.435077s

30720/50000 [=================>............] - ETA: 18s - loss: 8.5333 - accuracy: 0.0353
on_train_batch_begin: 1615686163.435373s

15 step training time: 0.641961s

on_train_batch_end: 1615686164.080961s

32768/50000 [==================>...........] - ETA: 16s - loss: 8.4442 - accuracy: 0.0372
on_train_batch_begin: 1615686164.081265s

16 step training time: 0.645892s

on_train_batch_end: 1615686164.692996s

34816/50000 [===================>..........] - ETA: 13s - loss: 8.3618 - accuracy: 0.0393
on_train_batch_begin: 1615686164.693301s

17 step training time: 0.612036s

on_train_batch_end: 1615686165.346425s

36864/50000 [=====================>........] - ETA: 11s - loss: 8.2869 - accuracy: 0.0411
on_train_batch_begin: 1615686165.346724s

18 step training time: 0.653423s

on_train_batch_end: 1615686165.982264s

38912/50000 [======================>.......] - ETA: 9s - loss: 8.2098 - accuracy: 0.0427 
on_train_batch_begin: 1615686165.982564s

19 step training time: 0.635840s

on_train_batch_end: 1615686166.624768s

40960/50000 [=======================>......] - ETA: 7s - loss: 8.1422 - accuracy: 0.0443
on_train_batch_begin: 1615686166.625064s

20 step training time: 0.642500s

on_train_batch_end: 1615686167.270823s

43008/50000 [========================>.....] - ETA: 5s - loss: 8.0746 - accuracy: 0.0457
on_train_batch_begin: 1615686167.271129s

21 step training time: 0.646065s

on_train_batch_end: 1615686167.906839s

45056/50000 [==========================>...] - ETA: 3s - loss: 8.0113 - accuracy: 0.0470
on_train_batch_begin: 1615686167.907148s

22 step training time: 0.636019s

on_train_batch_end: 1615686168.552609s

47104/50000 [===========================>..] - ETA: 2s - loss: 7.9505 - accuracy: 0.0484
on_train_batch_begin: 1615686168.552914s

23 step training time: 0.645766s

on_train_batch_end: 1615686169.186121s

49152/50000 [============================>.] - ETA: 0s - loss: 7.8913 - accuracy: 0.0498
on_train_batch_begin: 1615686169.186424s

24 step training time: 0.633510s

on_train_batch_end: 1615686174.828728s

on_test_batch_begin: 1615686175.017138s

25 step training time: 5.830714s

on_epoch_end: 1615686180.073361s

Validation time: 5.056208s

Real time: 1615686180.073361s

Epoch time: 46.74483680725098s

50000/50000 [==============================] - 47s 935us/sample - loss: 7.8669 - accuracy: 0.0500 - val_loss: 2849.1356 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615686180.073573s

Real time: 1615686180.073579
Epoch 2/5

on_train_batch_begin: 1615686180.077008s

on_train_batch_end: 1615686180.712672s

 2048/50000 [>.............................] - ETA: 14s - loss: 6.4140 - accuracy: 0.0785
on_train_batch_begin: 1615686180.712980s

1 step training time: 0.635972s

on_train_batch_end: 1615686181.359862s

 4096/50000 [=>............................] - ETA: 14s - loss: 6.3497 - accuracy: 0.0778
on_train_batch_begin: 1615686181.360163s

2 step training time: 0.647183s

on_train_batch_end: 1615686182.003808s

 6144/50000 [==>...........................] - ETA: 13s - loss: 6.3082 - accuracy: 0.0746
on_train_batch_begin: 1615686182.004118s

3 step training time: 0.643955s

on_train_batch_end: 1615686182.653328s

 8192/50000 [===>..........................] - ETA: 13s - loss: 6.2513 - accuracy: 0.0736
on_train_batch_begin: 1615686182.653629s

4 step training time: 0.649510s

on_train_batch_end: 1615686183.297439s

10240/50000 [=====>........................] - ETA: 12s - loss: 6.2204 - accuracy: 0.0739
on_train_batch_begin: 1615686183.297738s

5 step training time: 0.644110s

on_train_batch_end: 1615686183.935857s

12288/50000 [======>.......................] - ETA: 11s - loss: 6.1861 - accuracy: 0.0747
on_train_batch_begin: 1615686183.936158s

6 step training time: 0.638419s

on_train_batch_end: 1615686184.579235s

14336/50000 [=======>......................] - ETA: 11s - loss: 6.1396 - accuracy: 0.0754
on_train_batch_begin: 1615686184.579559s

7 step training time: 0.643402s

on_train_batch_end: 1615686185.224433s

16384/50000 [========>.....................] - ETA: 10s - loss: 6.1001 - accuracy: 0.0761
on_train_batch_begin: 1615686185.224742s

8 step training time: 0.645183s

on_train_batch_end: 1615686185.864950s

18432/50000 [==========>...................] - ETA: 9s - loss: 6.0541 - accuracy: 0.0758 
on_train_batch_begin: 1615686185.865359s

9 step training time: 0.640616s

on_train_batch_end: 1615686186.507611s

20480/50000 [===========>..................] - ETA: 9s - loss: 6.0146 - accuracy: 0.0757
on_train_batch_begin: 1615686186.507915s

10 step training time: 0.642557s

on_train_batch_end: 1615686187.155607s

22528/50000 [============>.................] - ETA: 8s - loss: 5.9698 - accuracy: 0.0752
on_train_batch_begin: 1615686187.155918s

11 step training time: 0.648002s

on_train_batch_end: 1615686187.802685s

24576/50000 [=============>................] - ETA: 7s - loss: 5.9367 - accuracy: 0.0746
on_train_batch_begin: 1615686187.802987s

12 step training time: 0.647069s

on_train_batch_end: 1615686188.452519s

26624/50000 [==============>...............] - ETA: 7s - loss: 5.9029 - accuracy: 0.0738
on_train_batch_begin: 1615686188.452833s

13 step training time: 0.649846s

on_train_batch_end: 1615686189.098243s

28672/50000 [================>.............] - ETA: 6s - loss: 5.8577 - accuracy: 0.0736
on_train_batch_begin: 1615686189.098547s

14 step training time: 0.645714s

on_train_batch_end: 1615686189.743392s

30720/50000 [=================>............] - ETA: 6s - loss: 5.8180 - accuracy: 0.0728
on_train_batch_begin: 1615686189.743729s

15 step training time: 0.645182s

on_train_batch_end: 1615686190.389898s

32768/50000 [==================>...........] - ETA: 5s - loss: 5.7756 - accuracy: 0.0720
on_train_batch_begin: 1615686190.390205s

16 step training time: 0.646476s

on_train_batch_end: 1615686191.033396s

34816/50000 [===================>..........] - ETA: 4s - loss: 5.7308 - accuracy: 0.0709
on_train_batch_begin: 1615686191.033702s

17 step training time: 0.643497s

on_train_batch_end: 1615686191.678110s

36864/50000 [=====================>........] - ETA: 4s - loss: 5.6824 - accuracy: 0.0700
on_train_batch_begin: 1615686191.678427s

18 step training time: 0.644725s

on_train_batch_end: 1615686192.326272s

38912/50000 [======================>.......] - ETA: 3s - loss: 5.6325 - accuracy: 0.0692
on_train_batch_begin: 1615686192.326590s

19 step training time: 0.648163s

on_train_batch_end: 1615686192.974390s

40960/50000 [=======================>......] - ETA: 2s - loss: 5.5930 - accuracy: 0.0684
on_train_batch_begin: 1615686192.974699s

20 step training time: 0.648109s

on_train_batch_end: 1615686193.622728s

43008/50000 [========================>.....] - ETA: 2s - loss: 5.5470 - accuracy: 0.0680
on_train_batch_begin: 1615686193.623033s

21 step training time: 0.648333s

on_train_batch_end: 1615686194.269822s

45056/50000 [==========================>...] - ETA: 1s - loss: 5.4990 - accuracy: 0.0677
on_train_batch_begin: 1615686194.270136s

22 step training time: 0.647103s

on_train_batch_end: 1615686194.917201s

47104/50000 [===========================>..] - ETA: 0s - loss: 5.4485 - accuracy: 0.0674
on_train_batch_begin: 1615686194.917515s

23 step training time: 0.647379s

on_train_batch_end: 1615686195.562136s

49152/50000 [============================>.] - ETA: 0s - loss: 5.4055 - accuracy: 0.0672
on_train_batch_begin: 1615686195.562446s

24 step training time: 0.644931s

on_train_batch_end: 1615686195.841645s

on_test_batch_begin: 1615686195.880858s

25 step training time: 0.318413s

on_epoch_end: 1615686196.702201s

Validation time: 0.821326s

Real time: 1615686196.702201s

Epoch time: 16.628638982772827s

50000/50000 [==============================] - 17s 333us/sample - loss: 5.3882 - accuracy: 0.0672 - val_loss: 13.2359 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615686196.702397s

Real time: 1615686196.702402
Epoch 3/5

on_train_batch_begin: 1615686196.705785s

on_train_batch_end: 1615686197.346099s

 2048/50000 [>.............................] - ETA: 15s - loss: 3.9296 - accuracy: 0.0698
on_train_batch_begin: 1615686197.346408s

1 step training time: 0.640624s

on_train_batch_end: 1615686197.993662s

 4096/50000 [=>............................] - ETA: 14s - loss: 3.8612 - accuracy: 0.0700
on_train_batch_begin: 1615686197.993975s

2 step training time: 0.647566s

on_train_batch_end: 1615686198.639600s

 6144/50000 [==>...........................] - ETA: 13s - loss: 3.7991 - accuracy: 0.0707
on_train_batch_begin: 1615686198.639911s

3 step training time: 0.645936s

on_train_batch_end: 1615686199.285587s

 8192/50000 [===>..........................] - ETA: 13s - loss: 3.7883 - accuracy: 0.0704
on_train_batch_begin: 1615686199.285894s

4 step training time: 0.645983s

on_train_batch_end: 1615686199.933504s

10240/50000 [=====>........................] - ETA: 12s - loss: 3.7770 - accuracy: 0.0706
on_train_batch_begin: 1615686199.933804s

5 step training time: 0.647910s

on_train_batch_end: 1615686200.580045s

12288/50000 [======>.......................] - ETA: 11s - loss: 3.7152 - accuracy: 0.0717
on_train_batch_begin: 1615686200.580360s

6 step training time: 0.646556s

on_train_batch_end: 1615686201.227835s

14336/50000 [=======>......................] - ETA: 11s - loss: 3.6711 - accuracy: 0.0729
on_train_batch_begin: 1615686201.228137s

7 step training time: 0.647778s

on_train_batch_end: 1615686201.875566s

16384/50000 [========>.....................] - ETA: 10s - loss: 3.6343 - accuracy: 0.0739
on_train_batch_begin: 1615686201.875870s

8 step training time: 0.647732s

on_train_batch_end: 1615686202.522844s

18432/50000 [==========>...................] - ETA: 9s - loss: 3.5964 - accuracy: 0.0750 
on_train_batch_begin: 1615686202.523146s

9 step training time: 0.647276s

on_train_batch_end: 1615686203.171978s

20480/50000 [===========>..................] - ETA: 9s - loss: 3.5691 - accuracy: 0.0758
on_train_batch_begin: 1615686203.172291s

10 step training time: 0.649145s

on_train_batch_end: 1615686203.817610s

22528/50000 [============>.................] - ETA: 8s - loss: 3.5418 - accuracy: 0.0765
on_train_batch_begin: 1615686203.817910s

11 step training time: 0.645619s

on_train_batch_end: 1615686204.467410s

24576/50000 [=============>................] - ETA: 8s - loss: 3.4930 - accuracy: 0.0772
on_train_batch_begin: 1615686204.467745s

12 step training time: 0.649835s

on_train_batch_end: 1615686205.112425s

26624/50000 [==============>...............] - ETA: 7s - loss: 3.4622 - accuracy: 0.0777
on_train_batch_begin: 1615686205.112722s

13 step training time: 0.644977s

on_train_batch_end: 1615686205.757267s

28672/50000 [================>.............] - ETA: 6s - loss: 3.4307 - accuracy: 0.0783
on_train_batch_begin: 1615686205.757564s

14 step training time: 0.644842s

on_train_batch_end: 1615686206.404153s

30720/50000 [=================>............] - ETA: 6s - loss: 3.3955 - accuracy: 0.0788
on_train_batch_begin: 1615686206.404456s

15 step training time: 0.646892s

on_train_batch_end: 1615686207.052611s

32768/50000 [==================>...........] - ETA: 5s - loss: 3.3727 - accuracy: 0.0793
on_train_batch_begin: 1615686207.052915s

16 step training time: 0.648458s

on_train_batch_end: 1615686207.701111s

34816/50000 [===================>..........] - ETA: 4s - loss: 3.3456 - accuracy: 0.0799
on_train_batch_begin: 1615686207.701418s

17 step training time: 0.648504s

on_train_batch_end: 1615686208.349849s

36864/50000 [=====================>........] - ETA: 4s - loss: 3.3113 - accuracy: 0.0806
on_train_batch_begin: 1615686208.350159s

18 step training time: 0.648741s

on_train_batch_end: 1615686209.000991s

38912/50000 [======================>.......] - ETA: 3s - loss: 3.2751 - accuracy: 0.0813
on_train_batch_begin: 1615686209.001300s

19 step training time: 0.651141s

on_train_batch_end: 1615686209.649121s

40960/50000 [=======================>......] - ETA: 2s - loss: 3.2382 - accuracy: 0.0820
on_train_batch_begin: 1615686209.649424s

20 step training time: 0.648123s

on_train_batch_end: 1615686210.296589s

43008/50000 [========================>.....] - ETA: 2s - loss: 3.2111 - accuracy: 0.0827
on_train_batch_begin: 1615686210.296892s

21 step training time: 0.647468s

on_train_batch_end: 1615686210.943034s

45056/50000 [==========================>...] - ETA: 1s - loss: 3.1810 - accuracy: 0.0834
on_train_batch_begin: 1615686210.943338s

22 step training time: 0.646446s

on_train_batch_end: 1615686211.584890s

47104/50000 [===========================>..] - ETA: 0s - loss: 3.1508 - accuracy: 0.0840
on_train_batch_begin: 1615686211.585196s

23 step training time: 0.641857s

on_train_batch_end: 1615686212.229718s

49152/50000 [============================>.] - ETA: 0s - loss: 3.1278 - accuracy: 0.0846
on_train_batch_begin: 1615686212.230021s

24 step training time: 0.644826s

on_train_batch_end: 1615686212.503838s

on_test_batch_begin: 1615686212.543191s

25 step training time: 0.313169s

on_epoch_end: 1615686213.361046s

Validation time: 0.817838s

Real time: 1615686213.361046s

Epoch time: 16.658661603927612s

50000/50000 [==============================] - 17s 333us/sample - loss: 3.1166 - accuracy: 0.0847 - val_loss: 13.9245 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615686213.361241s

Real time: 1615686213.3612468
Epoch 4/5

on_train_batch_begin: 1615686213.364699s

on_train_batch_end: 1615686214.009387s

 2048/50000 [>.............................] - ETA: 15s - loss: 2.1459 - accuracy: 0.0999
on_train_batch_begin: 1615686214.009704s

1 step training time: 0.645005s

on_train_batch_end: 1615686214.660599s

 4096/50000 [=>............................] - ETA: 14s - loss: 2.1784 - accuracy: 0.0997
on_train_batch_begin: 1615686214.660909s

2 step training time: 0.651205s

on_train_batch_end: 1615686215.302846s

 6144/50000 [==>...........................] - ETA: 13s - loss: 2.1780 - accuracy: 0.0996
on_train_batch_begin: 1615686215.303154s

3 step training time: 0.642245s

on_train_batch_end: 1615686215.948168s

 8192/50000 [===>..........................] - ETA: 13s - loss: 2.1628 - accuracy: 0.0997
on_train_batch_begin: 1615686215.948479s

4 step training time: 0.645325s

on_train_batch_end: 1615686216.598386s

10240/50000 [=====>........................] - ETA: 12s - loss: 2.1556 - accuracy: 0.0996
on_train_batch_begin: 1615686216.598692s

5 step training time: 0.650213s

on_train_batch_end: 1615686217.243850s

12288/50000 [======>.......................] - ETA: 11s - loss: 2.1363 - accuracy: 0.0994
on_train_batch_begin: 1615686217.244165s

6 step training time: 0.645473s

on_train_batch_end: 1615686217.892510s

14336/50000 [=======>......................] - ETA: 11s - loss: 2.1071 - accuracy: 0.0994
on_train_batch_begin: 1615686217.892823s

7 step training time: 0.648658s

on_train_batch_end: 1615686218.536733s

16384/50000 [========>.....................] - ETA: 10s - loss: 2.0997 - accuracy: 0.0994
on_train_batch_begin: 1615686218.537042s

8 step training time: 0.644218s

on_train_batch_end: 1615686219.181257s

18432/50000 [==========>...................] - ETA: 9s - loss: 2.0824 - accuracy: 0.0995 
on_train_batch_begin: 1615686219.181557s

9 step training time: 0.644515s

on_train_batch_end: 1615686219.826003s

20480/50000 [===========>..................] - ETA: 9s - loss: 2.0695 - accuracy: 0.0995
on_train_batch_begin: 1615686219.826305s

10 step training time: 0.644748s

on_train_batch_end: 1615686220.475648s

22528/50000 [============>.................] - ETA: 8s - loss: 2.0534 - accuracy: 0.0995
on_train_batch_begin: 1615686220.475949s

11 step training time: 0.649643s

on_train_batch_end: 1615686221.121824s

24576/50000 [=============>................] - ETA: 8s - loss: 2.0554 - accuracy: 0.0996
on_train_batch_begin: 1615686221.122117s

12 step training time: 0.646168s

on_train_batch_end: 1615686221.769534s

26624/50000 [==============>...............] - ETA: 7s - loss: 2.0368 - accuracy: 0.0996
on_train_batch_begin: 1615686221.769842s

13 step training time: 0.647725s

on_train_batch_end: 1615686222.411764s

28672/50000 [================>.............] - ETA: 6s - loss: 2.0174 - accuracy: 0.0997
on_train_batch_begin: 1615686222.412067s

14 step training time: 0.642225s

on_train_batch_end: 1615686223.057495s

30720/50000 [=================>............] - ETA: 6s - loss: 2.0135 - accuracy: 0.0997
on_train_batch_begin: 1615686223.057798s

15 step training time: 0.645731s

on_train_batch_end: 1615686223.707078s

32768/50000 [==================>...........] - ETA: 5s - loss: 1.9972 - accuracy: 0.0997
on_train_batch_begin: 1615686223.707375s

16 step training time: 0.649577s

on_train_batch_end: 1615686224.357119s

34816/50000 [===================>..........] - ETA: 4s - loss: 1.9809 - accuracy: 0.0997
on_train_batch_begin: 1615686224.357427s

17 step training time: 0.650052s

on_train_batch_end: 1615686225.005677s

36864/50000 [=====================>........] - ETA: 4s - loss: 1.9686 - accuracy: 0.0997
on_train_batch_begin: 1615686225.005977s

18 step training time: 0.648550s

on_train_batch_end: 1615686225.654980s

38912/50000 [======================>.......] - ETA: 3s - loss: 1.9578 - accuracy: 0.0997
on_train_batch_begin: 1615686225.655282s

19 step training time: 0.649305s

on_train_batch_end: 1615686226.307313s

40960/50000 [=======================>......] - ETA: 2s - loss: 1.9440 - accuracy: 0.0998
on_train_batch_begin: 1615686226.307668s

20 step training time: 0.652386s

on_train_batch_end: 1615686226.953499s

43008/50000 [========================>.....] - ETA: 2s - loss: 1.9380 - accuracy: 0.0998
on_train_batch_begin: 1615686226.953808s

21 step training time: 0.646140s

on_train_batch_end: 1615686227.604387s

45056/50000 [==========================>...] - ETA: 1s - loss: 1.9268 - accuracy: 0.0998
on_train_batch_begin: 1615686227.604707s

22 step training time: 0.650899s

on_train_batch_end: 1615686228.251168s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.9146 - accuracy: 0.0998
on_train_batch_begin: 1615686228.251485s

23 step training time: 0.646777s

on_train_batch_end: 1615686228.896297s

49152/50000 [============================>.] - ETA: 0s - loss: 1.9060 - accuracy: 0.0998
on_train_batch_begin: 1615686228.896608s

24 step training time: 0.645124s

on_train_batch_end: 1615686229.173863s

on_test_batch_begin: 1615686229.213230s

25 step training time: 0.316622s

on_epoch_end: 1615686230.036390s

Validation time: 0.823143s

Real time: 1615686230.036390s

Epoch time: 16.675161123275757s

50000/50000 [==============================] - 17s 334us/sample - loss: 1.9026 - accuracy: 0.0998 - val_loss: 7.8191 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615686230.036584s

Real time: 1615686230.0365903
Epoch 5/5

on_train_batch_begin: 1615686230.039994s

on_train_batch_end: 1615686230.681466s

 2048/50000 [>.............................] - ETA: 15s - loss: 1.5000 - accuracy: 0.0999
on_train_batch_begin: 1615686230.681775s

1 step training time: 0.641781s

on_train_batch_end: 1615686231.333513s

 4096/50000 [=>............................] - ETA: 14s - loss: 1.5417 - accuracy: 0.1001
on_train_batch_begin: 1615686231.333836s

2 step training time: 0.652061s

on_train_batch_end: 1615686231.981887s

 6144/50000 [==>...........................] - ETA: 13s - loss: 1.5038 - accuracy: 0.1003
on_train_batch_begin: 1615686231.982190s

3 step training time: 0.648354s

on_train_batch_end: 1615686232.633135s

 8192/50000 [===>..........................] - ETA: 13s - loss: 1.4917 - accuracy: 0.1002
on_train_batch_begin: 1615686232.633454s

4 step training time: 0.651264s

on_train_batch_end: 1615686233.282023s

10240/50000 [=====>........................] - ETA: 12s - loss: 1.4842 - accuracy: 0.1002
on_train_batch_begin: 1615686233.282337s

5 step training time: 0.648883s

on_train_batch_end: 1615686233.932490s

12288/50000 [======>.......................] - ETA: 11s - loss: 1.4804 - accuracy: 0.1002
on_train_batch_begin: 1615686233.932791s

6 step training time: 0.650454s

on_train_batch_end: 1615686234.579814s

14336/50000 [=======>......................] - ETA: 11s - loss: 1.4706 - accuracy: 0.1002
on_train_batch_begin: 1615686234.580137s

7 step training time: 0.647347s

on_train_batch_end: 1615686235.231129s

16384/50000 [========>.....................] - ETA: 10s - loss: 1.4701 - accuracy: 0.1002
on_train_batch_begin: 1615686235.231446s

8 step training time: 0.651309s

on_train_batch_end: 1615686235.877511s

18432/50000 [==========>...................] - ETA: 10s - loss: 1.4655 - accuracy: 0.1002
on_train_batch_begin: 1615686235.877825s

9 step training time: 0.646379s

on_train_batch_end: 1615686236.525778s

20480/50000 [===========>..................] - ETA: 9s - loss: 1.4676 - accuracy: 0.1002 
on_train_batch_begin: 1615686236.526091s

10 step training time: 0.648266s

on_train_batch_end: 1615686237.174043s

22528/50000 [============>.................] - ETA: 8s - loss: 1.4621 - accuracy: 0.1002
on_train_batch_begin: 1615686237.174351s

11 step training time: 0.648260s

on_train_batch_end: 1615686237.824015s

24576/50000 [=============>................] - ETA: 8s - loss: 1.4624 - accuracy: 0.1002
on_train_batch_begin: 1615686237.824321s

12 step training time: 0.649970s

on_train_batch_end: 1615686238.474002s

26624/50000 [==============>...............] - ETA: 7s - loss: 1.4595 - accuracy: 0.1001
on_train_batch_begin: 1615686238.474308s

13 step training time: 0.649987s

on_train_batch_end: 1615686239.122222s

28672/50000 [================>.............] - ETA: 6s - loss: 1.4514 - accuracy: 0.1001
on_train_batch_begin: 1615686239.122528s

14 step training time: 0.648220s

on_train_batch_end: 1615686239.771610s

30720/50000 [=================>............] - ETA: 6s - loss: 1.4473 - accuracy: 0.1001
on_train_batch_begin: 1615686239.771914s

15 step training time: 0.649387s

on_train_batch_end: 1615686240.420923s

32768/50000 [==================>...........] - ETA: 5s - loss: 1.4468 - accuracy: 0.1001
on_train_batch_begin: 1615686240.421223s

16 step training time: 0.649309s

on_train_batch_end: 1615686241.068261s

34816/50000 [===================>..........] - ETA: 4s - loss: 1.4449 - accuracy: 0.1001
on_train_batch_begin: 1615686241.068561s

17 step training time: 0.647338s

on_train_batch_end: 1615686241.717573s

36864/50000 [=====================>........] - ETA: 4s - loss: 1.4433 - accuracy: 0.1002
on_train_batch_begin: 1615686241.717881s

18 step training time: 0.649319s

on_train_batch_end: 1615686242.366575s

38912/50000 [======================>.......] - ETA: 3s - loss: 1.4377 - accuracy: 0.1001
on_train_batch_begin: 1615686242.366883s

19 step training time: 0.649003s

on_train_batch_end: 1615686243.015062s

40960/50000 [=======================>......] - ETA: 2s - loss: 1.4398 - accuracy: 0.1001
on_train_batch_begin: 1615686243.015371s

20 step training time: 0.648488s

on_train_batch_end: 1615686243.663614s

43008/50000 [========================>.....] - ETA: 2s - loss: 1.4350 - accuracy: 0.1001
on_train_batch_begin: 1615686243.663920s

21 step training time: 0.648549s

on_train_batch_end: 1615686244.310442s

45056/50000 [==========================>...] - ETA: 1s - loss: 1.4270 - accuracy: 0.1002
on_train_batch_begin: 1615686244.310756s

22 step training time: 0.646836s

on_train_batch_end: 1615686244.960290s

47104/50000 [===========================>..] - ETA: 0s - loss: 1.4275 - accuracy: 0.1001
on_train_batch_begin: 1615686244.960601s

23 step training time: 0.649845s

on_train_batch_end: 1615686245.605147s

49152/50000 [============================>.] - ETA: 0s - loss: 1.4205 - accuracy: 0.1001
on_train_batch_begin: 1615686245.605550s

24 step training time: 0.644949s

on_train_batch_end: 1615686245.883944s

on_test_batch_begin: 1615686245.941857s

25 step training time: 0.336306s

on_epoch_end: 1615686246.768126s

Validation time: 0.826252s

Real time: 1615686246.768126s

Epoch time: 16.731553316116333s

50000/50000 [==============================] - 17s 335us/sample - loss: 1.4210 - accuracy: 0.1001 - val_loss: 7.6328 - val_accuracy: 0.0000e+00
Tempo do fit: 116.94376492500305