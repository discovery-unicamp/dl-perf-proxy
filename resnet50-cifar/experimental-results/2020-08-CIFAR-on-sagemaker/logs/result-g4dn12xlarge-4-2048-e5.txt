wnloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 7:45
   204800/170498071 [..............................] - ETA: 1:17
  1073152/170498071 [..............................] - ETA: 22s 
  3678208/170498071 [..............................] - ETA: 8s 
  7004160/170498071 [>.............................] - ETA: 5s
 10321920/170498071 [>.............................] - ETA: 4s
 13639680/170498071 [=>............................] - ETA: 3s
 16949248/170498071 [=>............................] - ETA: 3s
 19701760/170498071 [==>...........................] - ETA: 3s
 22847488/170498071 [===>..........................] - ETA: 3s
 25993216/170498071 [===>..........................] - ETA: 3s
 29138944/170498071 [====>.........................] - ETA: 3s
 32284672/170498071 [====>.........................] - ETA: 3s
 35201024/170498071 [=====>........................] - ETA: 2s
 38002688/170498071 [=====>........................] - ETA: 2s
 40919040/170498071 [======>.......................] - ETA: 2s
 43327488/170498071 [======>.......................] - ETA: 2s
 46440448/170498071 [=======>......................] - ETA: 2s
 49569792/170498071 [=======>......................] - ETA: 2s
 52928512/170498071 [========>.....................] - ETA: 2s
 56213504/170498071 [========>.....................] - ETA: 2s
 59514880/170498071 [=========>....................] - ETA: 2s
 62840832/170498071 [==========>...................] - ETA: 2s
 64806912/170498071 [==========>...................] - ETA: 2s
 67952640/170498071 [==========>...................] - ETA: 2s
 71278592/170498071 [===========>..................] - ETA: 2s
 74571776/170498071 [============>.................] - ETA: 1s
 77922304/170498071 [============>.................] - ETA: 1s
 81240064/170498071 [=============>................] - ETA: 1s
 84549632/170498071 [=============>................] - ETA: 1s
 87842816/170498071 [==============>...............] - ETA: 1s
 91193344/170498071 [===============>..............] - ETA: 1s
 94461952/170498071 [===============>..............] - ETA: 1s
 97656832/170498071 [================>.............] - ETA: 1s
100917248/170498071 [================>.............] - ETA: 1s
104226816/170498071 [=================>............] - ETA: 1s
107528192/170498071 [=================>............] - ETA: 1s
110845952/170498071 [==================>...........] - ETA: 1s
114122752/170498071 [===================>..........] - ETA: 1s
117448704/170498071 [===================>..........] - ETA: 0s
120774656/170498071 [====================>.........] - ETA: 0s
124084224/170498071 [====================>.........] - ETA: 0s
127328256/170498071 [=====================>........] - ETA: 0s
130588672/170498071 [=====================>........] - ETA: 0s
133849088/170498071 [======================>.......] - ETA: 0s
137142272/170498071 [=======================>......] - ETA: 0s
140419072/170498071 [=======================>......] - ETA: 0s
143728640/170498071 [========================>.....] - ETA: 0s
146989056/170498071 [========================>.....] - ETA: 0s
150298624/170498071 [=========================>....] - ETA: 0s
153608192/170498071 [==========================>...] - ETA: 0s
156917760/170498071 [==========================>...] - ETA: 0s
160129024/170498071 [===========================>..] - ETA: 0s
163373056/170498071 [===========================>..] - ETA: 0s
166567936/170498071 [============================>.] - ETA: 0s
169828352/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 3s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 1s
 7938048/94765736 [=>............................] - ETA: 0s
 9388032/94765736 [=>............................] - ETA: 0s
14548992/94765736 [===>..........................] - ETA: 0s
18841600/94765736 [====>.........................] - ETA: 0s
28286976/94765736 [=======>......................] - ETA: 0s
37314560/94765736 [==========>...................] - ETA: 0s
37683200/94765736 [==========>...................] - ETA: 0s
45727744/94765736 [=============>................] - ETA: 0s
48324608/94765736 [==============>...............] - ETA: 0s
55664640/94765736 [================>.............] - ETA: 0s
57647104/94765736 [=================>............] - ETA: 0s
63930368/94765736 [===================>..........] - ETA: 0s
68952064/94765736 [====================>.........] - ETA: 0s
77799424/94765736 [=======================>......] - ETA: 0s
84893696/94765736 [=========================>....] - ETA: 0s
87367680/94765736 [==========================>...] - ETA: 0s
94281728/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 1s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 16.998652935028076
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1615761469.440094s

Real time: 1615761469.4401093
Epoch 1/5

on_train_batch_begin: 1615761470.229484s

on_train_batch_end: 1615761517.462598s

 2048/50000 [>.............................] - ETA: 18:44 - loss: 17.6978 - accuracy: 4.1580e-04
on_train_batch_begin: 1615761517.463201s

1 step training time: 47.233717s

on_train_batch_end: 1615761517.671811s

 4096/50000 [=>............................] - ETA: 9:00 - loss: 13.9670 - accuracy: 3.5381e-04 
on_train_batch_begin: 1615761517.672122s

2 step training time: 0.208921s

on_train_batch_end: 1615761517.878488s

 6144/50000 [==>...........................] - ETA: 5:45 - loss: 12.1506 - accuracy: 5.5822e-04
on_train_batch_begin: 1615761517.878809s

3 step training time: 0.206687s

on_train_batch_end: 1615761518.086226s

 8192/50000 [===>..........................] - ETA: 4:08 - loss: 11.1957 - accuracy: 0.0024    
on_train_batch_begin: 1615761518.086531s

4 step training time: 0.207723s

on_train_batch_end: 1615761518.292037s

10240/50000 [=====>........................] - ETA: 3:09 - loss: 10.5674 - accuracy: 0.0060
on_train_batch_begin: 1615761518.292346s

5 step training time: 0.205815s

on_train_batch_end: 1615761518.500771s

12288/50000 [======>.......................] - ETA: 2:30 - loss: 10.1364 - accuracy: 0.0099
on_train_batch_begin: 1615761518.501094s

6 step training time: 0.208748s

on_train_batch_end: 1615761518.710386s

14336/50000 [=======>......................] - ETA: 2:02 - loss: 9.8029 - accuracy: 0.0157 
on_train_batch_begin: 1615761518.710685s

7 step training time: 0.209591s

on_train_batch_end: 1615761518.917338s

16384/50000 [========>.....................] - ETA: 1:41 - loss: 9.5453 - accuracy: 0.0210
on_train_batch_begin: 1615761518.917629s

8 step training time: 0.206944s

on_train_batch_end: 1615761519.124068s

18432/50000 [==========>...................] - ETA: 1:25 - loss: 9.3328 - accuracy: 0.0259
on_train_batch_begin: 1615761519.124358s

9 step training time: 0.206729s

on_train_batch_end: 1615761519.331490s

20480/50000 [===========>..................] - ETA: 1:11 - loss: 9.1627 - accuracy: 0.0311
on_train_batch_begin: 1615761519.331823s

10 step training time: 0.207465s

on_train_batch_end: 1615761519.537788s

22528/50000 [============>.................] - ETA: 1:01 - loss: 9.0123 - accuracy: 0.0352
on_train_batch_begin: 1615761519.538080s

11 step training time: 0.206257s

on_train_batch_end: 1615761519.743655s

24576/50000 [=============>................] - ETA: 52s - loss: 8.8778 - accuracy: 0.0391 
on_train_batch_begin: 1615761519.743982s

12 step training time: 0.205902s

on_train_batch_end: 1615761519.949045s

26624/50000 [==============>...............] - ETA: 44s - loss: 8.7717 - accuracy: 0.0429
on_train_batch_begin: 1615761519.949350s

13 step training time: 0.205369s

on_train_batch_end: 1615761520.154975s

28672/50000 [================>.............] - ETA: 37s - loss: 8.6763 - accuracy: 0.0461
on_train_batch_begin: 1615761520.155267s

14 step training time: 0.205917s

on_train_batch_end: 1615761520.362151s

30720/50000 [=================>............] - ETA: 31s - loss: 8.5925 - accuracy: 0.0493
on_train_batch_begin: 1615761520.362474s

15 step training time: 0.207207s

on_train_batch_end: 1615761520.568839s

32768/50000 [==================>...........] - ETA: 26s - loss: 8.5116 - accuracy: 0.0518
on_train_batch_begin: 1615761520.569125s

16 step training time: 0.206651s

on_train_batch_end: 1615761520.774567s

34816/50000 [===================>..........] - ETA: 22s - loss: 8.4434 - accuracy: 0.0544
on_train_batch_begin: 1615761520.774869s

17 step training time: 0.205744s

on_train_batch_end: 1615761520.979196s

36864/50000 [=====================>........] - ETA: 18s - loss: 8.3766 - accuracy: 0.0561
on_train_batch_begin: 1615761520.979484s

18 step training time: 0.204615s

on_train_batch_end: 1615761521.186302s

38912/50000 [======================>.......] - ETA: 14s - loss: 8.3162 - accuracy: 0.0576
on_train_batch_begin: 1615761521.186585s

19 step training time: 0.207101s

on_train_batch_end: 1615761521.393508s

40960/50000 [=======================>......] - ETA: 11s - loss: 8.2631 - accuracy: 0.0590
on_train_batch_begin: 1615761521.393797s

20 step training time: 0.207212s

on_train_batch_end: 1615761521.600681s

43008/50000 [========================>.....] - ETA: 8s - loss: 8.2131 - accuracy: 0.0605 
on_train_batch_begin: 1615761521.600982s

21 step training time: 0.207185s

on_train_batch_end: 1615761521.809041s

45056/50000 [==========================>...] - ETA: 5s - loss: 8.1670 - accuracy: 0.0618
on_train_batch_begin: 1615761521.809327s

22 step training time: 0.208345s

on_train_batch_end: 1615761522.015584s

47104/50000 [===========================>..] - ETA: 3s - loss: 8.1260 - accuracy: 0.0630
on_train_batch_begin: 1615761522.015896s

23 step training time: 0.206568s

on_train_batch_end: 1615761522.219540s

49152/50000 [============================>.] - ETA: 0s - loss: 8.0851 - accuracy: 0.0640
on_train_batch_begin: 1615761522.219850s

24 step training time: 0.203954s

on_train_batch_end: 1615761524.763252s

on_test_batch_begin: 1615761524.999904s

25 step training time: 2.780055s

on_epoch_end: 1615761530.355334s

Validation time: 5.355413s

Real time: 1615761530.355334s

Epoch time: 60.91524386405945s

50000/50000 [==============================] - 61s 1ms/sample - loss: 8.0689 - accuracy: 0.0642 - val_loss: 31256.6969 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615761530.355521s

Real time: 1615761530.3555262
Epoch 2/5

on_train_batch_begin: 1615761530.359904s

on_train_batch_end: 1615761530.566220s

 2048/50000 [>.............................] - ETA: 4s - loss: 6.9596 - accuracy: 0.0925
on_train_batch_begin: 1615761530.566513s

1 step training time: 0.206609s

on_train_batch_end: 1615761530.770935s

 4096/50000 [=>............................] - ETA: 4s - loss: 6.8885 - accuracy: 0.0898
on_train_batch_begin: 1615761530.771223s

2 step training time: 0.204710s

on_train_batch_end: 1615761530.978296s

 6144/50000 [==>...........................] - ETA: 4s - loss: 6.8723 - accuracy: 0.0923
on_train_batch_begin: 1615761530.978591s

3 step training time: 0.207368s

on_train_batch_end: 1615761531.185338s

 8192/50000 [===>..........................] - ETA: 4s - loss: 6.8528 - accuracy: 0.0917
on_train_batch_begin: 1615761531.185625s

4 step training time: 0.207034s

on_train_batch_end: 1615761531.391517s

10240/50000 [=====>........................] - ETA: 4s - loss: 6.8482 - accuracy: 0.0917
on_train_batch_begin: 1615761531.391826s

5 step training time: 0.206202s

on_train_batch_end: 1615761531.598628s

12288/50000 [======>.......................] - ETA: 3s - loss: 6.8195 - accuracy: 0.0907
on_train_batch_begin: 1615761531.598917s

6 step training time: 0.207090s

on_train_batch_end: 1615761531.805515s

14336/50000 [=======>......................] - ETA: 3s - loss: 6.8021 - accuracy: 0.0900
on_train_batch_begin: 1615761531.805798s

7 step training time: 0.206881s

on_train_batch_end: 1615761532.012861s

16384/50000 [========>.....................] - ETA: 3s - loss: 6.7810 - accuracy: 0.0899
on_train_batch_begin: 1615761532.013147s

8 step training time: 0.207350s

on_train_batch_end: 1615761532.219984s

18432/50000 [==========>...................] - ETA: 3s - loss: 6.7649 - accuracy: 0.0888
on_train_batch_begin: 1615761532.220268s

9 step training time: 0.207120s

on_train_batch_end: 1615761532.427932s

20480/50000 [===========>..................] - ETA: 2s - loss: 6.7504 - accuracy: 0.0886
on_train_batch_begin: 1615761532.428225s

10 step training time: 0.207957s

on_train_batch_end: 1615761532.634401s

22528/50000 [============>.................] - ETA: 2s - loss: 6.7316 - accuracy: 0.0882
on_train_batch_begin: 1615761532.634697s

11 step training time: 0.206473s

on_train_batch_end: 1615761532.841857s

24576/50000 [=============>................] - ETA: 2s - loss: 6.7157 - accuracy: 0.0884
on_train_batch_begin: 1615761532.842198s

12 step training time: 0.207501s

on_train_batch_end: 1615761533.048549s

26624/50000 [==============>...............] - ETA: 2s - loss: 6.7063 - accuracy: 0.0892
on_train_batch_begin: 1615761533.048837s

13 step training time: 0.206638s

on_train_batch_end: 1615761533.256219s

28672/50000 [================>.............] - ETA: 2s - loss: 6.6888 - accuracy: 0.0892
on_train_batch_begin: 1615761533.256510s

14 step training time: 0.207674s

on_train_batch_end: 1615761533.464117s

30720/50000 [=================>............] - ETA: 1s - loss: 6.6764 - accuracy: 0.0889
on_train_batch_begin: 1615761533.464405s

15 step training time: 0.207895s

on_train_batch_end: 1615761533.671139s

32768/50000 [==================>...........] - ETA: 1s - loss: 6.6621 - accuracy: 0.0889
on_train_batch_begin: 1615761533.671434s

16 step training time: 0.207028s

on_train_batch_end: 1615761533.877123s

34816/50000 [===================>..........] - ETA: 1s - loss: 6.6488 - accuracy: 0.0892
on_train_batch_begin: 1615761533.877409s

17 step training time: 0.205975s

on_train_batch_end: 1615761534.082543s

36864/50000 [=====================>........] - ETA: 1s - loss: 6.6332 - accuracy: 0.0894
on_train_batch_begin: 1615761534.082855s

18 step training time: 0.205447s

on_train_batch_end: 1615761534.288455s

38912/50000 [======================>.......] - ETA: 1s - loss: 6.6143 - accuracy: 0.0897
on_train_batch_begin: 1615761534.288748s

19 step training time: 0.205892s

on_train_batch_end: 1615761534.493431s

40960/50000 [=======================>......] - ETA: 0s - loss: 6.5939 - accuracy: 0.0896
on_train_batch_begin: 1615761534.493720s

20 step training time: 0.204972s

on_train_batch_end: 1615761534.699347s

43008/50000 [========================>.....] - ETA: 0s - loss: 6.5755 - accuracy: 0.0900
on_train_batch_begin: 1615761534.699634s

21 step training time: 0.205914s

on_train_batch_end: 1615761534.906467s

45056/50000 [==========================>...] - ETA: 0s - loss: 6.5617 - accuracy: 0.0901
on_train_batch_begin: 1615761534.906756s

22 step training time: 0.207121s

on_train_batch_end: 1615761535.116307s

47104/50000 [===========================>..] - ETA: 0s - loss: 6.5452 - accuracy: 0.0902
on_train_batch_begin: 1615761535.116600s

23 step training time: 0.209844s

on_train_batch_end: 1615761535.323014s

49152/50000 [============================>.] - ETA: 0s - loss: 6.5270 - accuracy: 0.0900
on_train_batch_begin: 1615761535.323300s

24 step training time: 0.206700s

on_train_batch_end: 1615761535.445381s

on_test_batch_begin: 1615761535.537528s

25 step training time: 0.214227s

on_epoch_end: 1615761535.824219s

Validation time: 0.286675s

Real time: 1615761535.824219s

Epoch time: 5.4687089920043945s

50000/50000 [==============================] - 5s 109us/sample - loss: 6.5208 - accuracy: 0.0901 - val_loss: 10340.7572 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615761535.824404s

Real time: 1615761535.8244088
Epoch 3/5

on_train_batch_begin: 1615761535.828613s

on_train_batch_end: 1615761536.038769s

 2048/50000 [>.............................] - ETA: 5s - loss: 5.9886 - accuracy: 0.1068
on_train_batch_begin: 1615761536.039050s

1 step training time: 0.210438s

on_train_batch_end: 1615761536.244701s

 4096/50000 [=>............................] - ETA: 4s - loss: 5.9599 - accuracy: 0.0996
on_train_batch_begin: 1615761536.244987s

2 step training time: 0.205937s

on_train_batch_end: 1615761536.453450s

 6144/50000 [==>...........................] - ETA: 4s - loss: 5.9503 - accuracy: 0.0962
on_train_batch_begin: 1615761536.453748s

3 step training time: 0.208760s

on_train_batch_end: 1615761536.658724s

 8192/50000 [===>..........................] - ETA: 4s - loss: 5.9737 - accuracy: 0.0967
on_train_batch_begin: 1615761536.659016s

4 step training time: 0.205268s

on_train_batch_end: 1615761536.865813s

10240/50000 [=====>........................] - ETA: 4s - loss: 5.9607 - accuracy: 0.0952
on_train_batch_begin: 1615761536.866097s

5 step training time: 0.207081s

on_train_batch_end: 1615761537.071450s

12288/50000 [======>.......................] - ETA: 3s - loss: 5.9483 - accuracy: 0.0966
on_train_batch_begin: 1615761537.071747s

6 step training time: 0.205650s

on_train_batch_end: 1615761537.278864s

14336/50000 [=======>......................] - ETA: 3s - loss: 5.9219 - accuracy: 0.0956
on_train_batch_begin: 1615761537.279169s

7 step training time: 0.207422s

on_train_batch_end: 1615761537.486731s

16384/50000 [========>.....................] - ETA: 3s - loss: 5.9188 - accuracy: 0.0955
on_train_batch_begin: 1615761537.487016s

8 step training time: 0.207847s

on_train_batch_end: 1615761537.695253s

18432/50000 [==========>...................] - ETA: 3s - loss: 5.9214 - accuracy: 0.0954
on_train_batch_begin: 1615761537.695614s

9 step training time: 0.208598s

on_train_batch_end: 1615761537.902539s

20480/50000 [===========>..................] - ETA: 2s - loss: 5.9117 - accuracy: 0.0955
on_train_batch_begin: 1615761537.902850s

10 step training time: 0.207236s

on_train_batch_end: 1615761538.109961s

22528/50000 [============>.................] - ETA: 2s - loss: 5.8976 - accuracy: 0.0954
on_train_batch_begin: 1615761538.110249s

11 step training time: 0.207399s

on_train_batch_end: 1615761538.317244s

24576/50000 [=============>................] - ETA: 2s - loss: 5.8917 - accuracy: 0.0950
on_train_batch_begin: 1615761538.317536s

12 step training time: 0.207287s

on_train_batch_end: 1615761538.522755s

26624/50000 [==============>...............] - ETA: 2s - loss: 5.8793 - accuracy: 0.0951
on_train_batch_begin: 1615761538.523046s

13 step training time: 0.205510s

on_train_batch_end: 1615761538.731530s

28672/50000 [================>.............] - ETA: 2s - loss: 5.8759 - accuracy: 0.0951
on_train_batch_begin: 1615761538.731841s

14 step training time: 0.208795s

on_train_batch_end: 1615761538.939523s

30720/50000 [=================>............] - ETA: 1s - loss: 5.8696 - accuracy: 0.0952
on_train_batch_begin: 1615761538.939835s

15 step training time: 0.207994s

on_train_batch_end: 1615761539.146479s

32768/50000 [==================>...........] - ETA: 1s - loss: 5.8647 - accuracy: 0.0949
on_train_batch_begin: 1615761539.146765s

16 step training time: 0.206930s

on_train_batch_end: 1615761539.353197s

34816/50000 [===================>..........] - ETA: 1s - loss: 5.8601 - accuracy: 0.0948
on_train_batch_begin: 1615761539.353486s

17 step training time: 0.206721s

on_train_batch_end: 1615761539.561403s

36864/50000 [=====================>........] - ETA: 1s - loss: 5.8475 - accuracy: 0.0947
on_train_batch_begin: 1615761539.561697s

18 step training time: 0.208211s

on_train_batch_end: 1615761539.767952s

38912/50000 [======================>.......] - ETA: 1s - loss: 5.8391 - accuracy: 0.0952
on_train_batch_begin: 1615761539.768252s

19 step training time: 0.206555s

on_train_batch_end: 1615761539.974592s

40960/50000 [=======================>......] - ETA: 0s - loss: 5.8288 - accuracy: 0.0951
on_train_batch_begin: 1615761539.974882s

20 step training time: 0.206630s

on_train_batch_end: 1615761540.181587s

43008/50000 [========================>.....] - ETA: 0s - loss: 5.8198 - accuracy: 0.0946
on_train_batch_begin: 1615761540.181885s

21 step training time: 0.207003s

on_train_batch_end: 1615761540.388539s

45056/50000 [==========================>...] - ETA: 0s - loss: 5.8127 - accuracy: 0.0943
on_train_batch_begin: 1615761540.388822s

22 step training time: 0.206938s

on_train_batch_end: 1615761540.598401s

47104/50000 [===========================>..] - ETA: 0s - loss: 5.8036 - accuracy: 0.0939
on_train_batch_begin: 1615761540.598702s

23 step training time: 0.209880s

on_train_batch_end: 1615761540.804020s

49152/50000 [============================>.] - ETA: 0s - loss: 5.8005 - accuracy: 0.0936
on_train_batch_begin: 1615761540.804301s

24 step training time: 0.205599s

on_train_batch_end: 1615761540.927098s

on_test_batch_begin: 1615761541.019229s

25 step training time: 0.214928s

on_epoch_end: 1615761541.342034s

Validation time: 0.322789s

Real time: 1615761541.342034s

Epoch time: 5.5176403522491455s

50000/50000 [==============================] - 6s 110us/sample - loss: 5.8016 - accuracy: 0.0936 - val_loss: 10304.5491 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615761541.342216s

Real time: 1615761541.3422208
Epoch 4/5

on_train_batch_begin: 1615761541.346456s

on_train_batch_end: 1615761541.554294s

 2048/50000 [>.............................] - ETA: 4s - loss: 5.5739 - accuracy: 0.0881
on_train_batch_begin: 1615761541.554595s

1 step training time: 0.208139s

on_train_batch_end: 1615761541.763819s

 4096/50000 [=>............................] - ETA: 4s - loss: 5.6388 - accuracy: 0.0914
on_train_batch_begin: 1615761541.764110s

2 step training time: 0.209515s

on_train_batch_end: 1615761541.970057s

 6144/50000 [==>...........................] - ETA: 4s - loss: 5.6640 - accuracy: 0.0890
on_train_batch_begin: 1615761541.970343s

3 step training time: 0.206233s

on_train_batch_end: 1615761542.179571s

 8192/50000 [===>..........................] - ETA: 4s - loss: 5.6236 - accuracy: 0.0883
on_train_batch_begin: 1615761542.179875s

4 step training time: 0.209532s

on_train_batch_end: 1615761542.387508s

10240/50000 [=====>........................] - ETA: 4s - loss: 5.6074 - accuracy: 0.0881
on_train_batch_begin: 1615761542.387809s

5 step training time: 0.207934s

on_train_batch_end: 1615761542.593730s

12288/50000 [======>.......................] - ETA: 3s - loss: 5.6006 - accuracy: 0.0873
on_train_batch_begin: 1615761542.594032s

6 step training time: 0.206223s

on_train_batch_end: 1615761542.801828s

14336/50000 [=======>......................] - ETA: 3s - loss: 5.5789 - accuracy: 0.0872
on_train_batch_begin: 1615761542.802133s

7 step training time: 0.208102s

on_train_batch_end: 1615761543.008591s

16384/50000 [========>.....................] - ETA: 3s - loss: 5.5883 - accuracy: 0.0861
on_train_batch_begin: 1615761543.008908s

8 step training time: 0.206775s

on_train_batch_end: 1615761543.215794s

18432/50000 [==========>...................] - ETA: 3s - loss: 5.5820 - accuracy: 0.0849
on_train_batch_begin: 1615761543.216074s

9 step training time: 0.207166s

on_train_batch_end: 1615761543.423506s

20480/50000 [===========>..................] - ETA: 3s - loss: 5.5742 - accuracy: 0.0835
on_train_batch_begin: 1615761543.423811s

10 step training time: 0.207736s

on_train_batch_end: 1615761543.629687s

22528/50000 [============>.................] - ETA: 2s - loss: 5.5610 - accuracy: 0.0816
on_train_batch_begin: 1615761543.629969s

11 step training time: 0.206158s

on_train_batch_end: 1615761543.836029s

24576/50000 [=============>................] - ETA: 2s - loss: 5.5532 - accuracy: 0.0801
on_train_batch_begin: 1615761543.836316s

12 step training time: 0.206347s

on_train_batch_end: 1615761544.043493s

26624/50000 [==============>...............] - ETA: 2s - loss: 5.5439 - accuracy: 0.0786
on_train_batch_begin: 1615761544.043794s

13 step training time: 0.207479s

on_train_batch_end: 1615761544.249968s

28672/50000 [================>.............] - ETA: 2s - loss: 5.5342 - accuracy: 0.0771
on_train_batch_begin: 1615761544.250255s

14 step training time: 0.206461s

on_train_batch_end: 1615761544.457330s

30720/50000 [=================>............] - ETA: 1s - loss: 5.5160 - accuracy: 0.0757
on_train_batch_begin: 1615761544.457618s

15 step training time: 0.207363s

on_train_batch_end: 1615761544.664316s

32768/50000 [==================>...........] - ETA: 1s - loss: 5.5042 - accuracy: 0.0741
on_train_batch_begin: 1615761544.664604s

16 step training time: 0.206986s

on_train_batch_end: 1615761544.872402s

34816/50000 [===================>..........] - ETA: 1s - loss: 5.4872 - accuracy: 0.0726
on_train_batch_begin: 1615761544.872691s

17 step training time: 0.208087s

on_train_batch_end: 1615761545.080281s

36864/50000 [=====================>........] - ETA: 1s - loss: 5.4612 - accuracy: 0.0712
on_train_batch_begin: 1615761545.080581s

18 step training time: 0.207890s

on_train_batch_end: 1615761545.287980s

38912/50000 [======================>.......] - ETA: 1s - loss: 5.4406 - accuracy: 0.0699
on_train_batch_begin: 1615761545.288281s

19 step training time: 0.207700s

on_train_batch_end: 1615761545.497294s

40960/50000 [=======================>......] - ETA: 0s - loss: 5.4192 - accuracy: 0.0688
on_train_batch_begin: 1615761545.497584s

20 step training time: 0.209303s

on_train_batch_end: 1615761545.704166s

43008/50000 [========================>.....] - ETA: 0s - loss: 5.3970 - accuracy: 0.0679
on_train_batch_begin: 1615761545.704457s

21 step training time: 0.206873s

on_train_batch_end: 1615761545.912714s

45056/50000 [==========================>...] - ETA: 0s - loss: 5.3807 - accuracy: 0.0670
on_train_batch_begin: 1615761545.913006s

22 step training time: 0.208549s

on_train_batch_end: 1615761546.119368s

47104/50000 [===========================>..] - ETA: 0s - loss: 5.3630 - accuracy: 0.0664
on_train_batch_begin: 1615761546.119659s

23 step training time: 0.206654s

on_train_batch_end: 1615761546.324304s

49152/50000 [============================>.] - ETA: 0s - loss: 5.3371 - accuracy: 0.0659
on_train_batch_begin: 1615761546.324596s

24 step training time: 0.204937s

on_train_batch_end: 1615761546.445615s

on_test_batch_begin: 1615761546.537524s

25 step training time: 0.212929s

on_epoch_end: 1615761546.830332s

Validation time: 0.292791s

Real time: 1615761546.830332s

Epoch time: 5.488126993179321s

50000/50000 [==============================] - 5s 110us/sample - loss: 5.3235 - accuracy: 0.0659 - val_loss: 7.6354 - val_accuracy: 0.0000e+00

on_epoch_begin: 1615761546.830517s

Real time: 1615761546.8305216
Epoch 5/5

on_train_batch_begin: 1615761546.835010s

on_train_batch_end: 1615761547.041754s

 2048/50000 [>.............................] - ETA: 4s - loss: 4.7199 - accuracy: 0.0550
on_train_batch_begin: 1615761547.042066s

1 step training time: 0.207056s

on_train_batch_end: 1615761547.251324s

 4096/50000 [=>............................] - ETA: 4s - loss: 4.5913 - accuracy: 0.0558
on_train_batch_begin: 1615761547.251616s

2 step training time: 0.209551s

on_train_batch_end: 1615761547.459367s

 6144/50000 [==>...........................] - ETA: 4s - loss: 4.5510 - accuracy: 0.0564
on_train_batch_begin: 1615761547.459657s

3 step training time: 0.208041s

on_train_batch_end: 1615761547.667898s

 8192/50000 [===>..........................] - ETA: 4s - loss: 4.5202 - accuracy: 0.0569
on_train_batch_begin: 1615761547.668190s

4 step training time: 0.208534s

on_train_batch_end: 1615761547.874733s

10240/50000 [=====>........................] - ETA: 4s - loss: 4.4651 - accuracy: 0.0580
on_train_batch_begin: 1615761547.875023s

5 step training time: 0.206833s

on_train_batch_end: 1615761548.082874s

12288/50000 [======>.......................] - ETA: 3s - loss: 4.4160 - accuracy: 0.0586
on_train_batch_begin: 1615761548.083166s

6 step training time: 0.208143s

on_train_batch_end: 1615761548.293558s

14336/50000 [=======>......................] - ETA: 3s - loss: 4.3894 - accuracy: 0.0593
on_train_batch_begin: 1615761548.293845s

7 step training time: 0.210679s

on_train_batch_end: 1615761548.503453s

16384/50000 [========>.....................] - ETA: 3s - loss: 4.3390 - accuracy: 0.0598
on_train_batch_begin: 1615761548.503743s

8 step training time: 0.209898s

on_train_batch_end: 1615761548.712011s

18432/50000 [==========>...................] - ETA: 3s - loss: 4.2994 - accuracy: 0.0603
on_train_batch_begin: 1615761548.712309s

9 step training time: 0.208566s

on_train_batch_end: 1615761548.919948s

20480/50000 [===========>..................] - ETA: 3s - loss: 4.2457 - accuracy: 0.0612
on_train_batch_begin: 1615761548.920236s

10 step training time: 0.207927s

on_train_batch_end: 1615761549.127267s

22528/50000 [============>.................] - ETA: 2s - loss: 4.2046 - accuracy: 0.0619
on_train_batch_begin: 1615761549.127555s

11 step training time: 0.207319s

on_train_batch_end: 1615761549.335539s

24576/50000 [=============>................] - ETA: 2s - loss: 4.1559 - accuracy: 0.0627
on_train_batch_begin: 1615761549.335854s

12 step training time: 0.208300s

on_train_batch_end: 1615761549.544066s

26624/50000 [==============>...............] - ETA: 2s - loss: 4.1075 - accuracy: 0.0635
on_train_batch_begin: 1615761549.544355s

13 step training time: 0.208500s

on_train_batch_end: 1615761549.751633s

28672/50000 [================>.............] - ETA: 2s - loss: 4.0528 - accuracy: 0.0644
on_train_batch_begin: 1615761549.751970s

14 step training time: 0.207615s

on_train_batch_end: 1615761549.960538s

30720/50000 [=================>............] - ETA: 1s - loss: 3.9976 - accuracy: 0.0655
on_train_batch_begin: 1615761549.960862s

15 step training time: 0.208892s

on_train_batch_end: 1615761550.166786s

32768/50000 [==================>...........] - ETA: 1s - loss: 3.9414 - accuracy: 0.0667
on_train_batch_begin: 1615761550.167074s

16 step training time: 0.206213s

on_train_batch_end: 1615761550.374650s

34816/50000 [===================>..........] - ETA: 1s - loss: 3.8939 - accuracy: 0.0677
on_train_batch_begin: 1615761550.374944s

17 step training time: 0.207870s

on_train_batch_end: 1615761550.581648s

36864/50000 [=====================>........] - ETA: 1s - loss: 3.8404 - accuracy: 0.0688
on_train_batch_begin: 1615761550.581961s

18 step training time: 0.207016s

on_train_batch_end: 1615761550.791139s

38912/50000 [======================>.......] - ETA: 1s - loss: 3.7945 - accuracy: 0.0697
on_train_batch_begin: 1615761550.791452s

19 step training time: 0.209492s

on_train_batch_end: 1615761550.998344s

40960/50000 [=======================>......] - ETA: 0s - loss: 3.7391 - accuracy: 0.0708
on_train_batch_begin: 1615761550.998634s

20 step training time: 0.207182s

on_train_batch_end: 1615761551.208174s

43008/50000 [========================>.....] - ETA: 0s - loss: 3.6852 - accuracy: 0.0719
on_train_batch_begin: 1615761551.208464s

21 step training time: 0.209830s

on_train_batch_end: 1615761551.418945s

45056/50000 [==========================>...] - ETA: 0s - loss: 3.6351 - accuracy: 0.0729
on_train_batch_begin: 1615761551.419236s

22 step training time: 0.210772s

on_train_batch_end: 1615761551.625906s

47104/50000 [===========================>..] - ETA: 0s - loss: 3.5786 - accuracy: 0.0739
on_train_batch_begin: 1615761551.626218s

23 step training time: 0.206982s

on_train_batch_end: 1615761551.832187s

49152/50000 [============================>.] - ETA: 0s - loss: 3.5189 - accuracy: 0.0750
on_train_batch_begin: 1615761551.832498s

24 step training time: 0.206280s

on_train_batch_end: 1615761551.956378s

on_test_batch_begin: 1615761552.049824s

25 step training time: 0.217326s

on_epoch_end: 1615761552.352838s

Validation time: 0.302997s

Real time: 1615761552.352838s

Epoch time: 5.522333383560181s

50000/50000 [==============================] - 6s 110us/sample - loss: 3.4991 - accuracy: 0.0751 - val_loss: 7.6246 - val_accuracy: 0.1001
Tempo do fit: 86.40353560447693