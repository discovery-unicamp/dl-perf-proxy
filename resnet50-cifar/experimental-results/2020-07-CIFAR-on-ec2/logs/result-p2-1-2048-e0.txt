python3 cifar.py --batch-size 2048 --epochs 5
Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

     8192/170498071 [..............................] - ETA: 6:40
   221184/170498071 [..............................] - ETA: 1:02
  1826816/170498071 [..............................] - ETA: 13s 
  5718016/170498071 [>.............................] - ETA: 5s 
  9609216/170498071 [>.............................] - ETA: 4s
 13279232/170498071 [=>............................] - ETA: 3s
 17227776/170498071 [==>...........................] - ETA: 3s
 20602880/170498071 [==>...........................] - ETA: 2s
 24436736/170498071 [===>..........................] - ETA: 2s
 28352512/170498071 [===>..........................] - ETA: 2s
 31891456/170498071 [====>.........................] - ETA: 2s
 35553280/170498071 [=====>........................] - ETA: 2s
 39297024/170498071 [=====>........................] - ETA: 2s
 42967040/170498071 [======>.......................] - ETA: 2s
 46817280/170498071 [=======>......................] - ETA: 1s
 50339840/170498071 [=======>......................] - ETA: 1s
 54083584/170498071 [========>.....................] - ETA: 1s
 57835520/170498071 [=========>....................] - ETA: 1s
 61415424/170498071 [=========>....................] - ETA: 1s
 65183744/170498071 [==========>...................] - ETA: 1s
 69017600/170498071 [===========>..................] - ETA: 1s
 72572928/170498071 [===========>..................] - ETA: 1s
 76333056/170498071 [============>.................] - ETA: 1s
 80093184/170498071 [=============>................] - ETA: 1s
 83730432/170498071 [=============>................] - ETA: 1s
 87449600/170498071 [==============>...............] - ETA: 1s
 91152384/170498071 [===============>..............] - ETA: 1s
 94887936/170498071 [===============>..............] - ETA: 1s
 98557952/170498071 [================>.............] - ETA: 1s
102211584/170498071 [================>.............] - ETA: 1s
105947136/170498071 [=================>............] - ETA: 0s
109723648/170498071 [==================>...........] - ETA: 0s
113336320/170498071 [==================>...........] - ETA: 0s
117121024/170498071 [===================>..........] - ETA: 0s
120823808/170498071 [====================>.........] - ETA: 0s
124510208/170498071 [====================>.........] - ETA: 0s
128245760/170498071 [=====================>........] - ETA: 0s
131883008/170498071 [======================>.......] - ETA: 0s
135651328/170498071 [======================>.......] - ETA: 0s
139370496/170498071 [=======================>......] - ETA: 0s
143024128/170498071 [========================>.....] - ETA: 0s
146792448/170498071 [========================>.....] - ETA: 0s
150568960/170498071 [=========================>....] - ETA: 0s
154230784/170498071 [==========================>...] - ETA: 0s
157917184/170498071 [==========================>...] - ETA: 0s
161619968/170498071 [===========================>..] - ETA: 0s
165322752/170498071 [============================>.] - ETA: 0s
169107456/170498071 [============================>.] - ETA: 0s
170500096/170498071 [==============================] - 2s 0us/step
Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

    8192/94765736 [..............................] - ETA: 1s
 2686976/94765736 [..............................] - ETA: 1s
 6332416/94765736 [=>............................] - ETA: 1s
 9388032/94765736 [=>............................] - ETA: 1s
15327232/94765736 [===>..........................] - ETA: 1s
18841600/94765736 [====>.........................] - ETA: 1s
24756224/94765736 [======>.......................] - ETA: 1s
29409280/94765736 [========>.....................] - ETA: 0s
34398208/94765736 [=========>....................] - ETA: 0s
37683200/94765736 [==========>...................] - ETA: 0s
41648128/94765736 [============>.................] - ETA: 0s
47439872/94765736 [==============>...............] - ETA: 0s
51159040/94765736 [===============>..............] - ETA: 0s
56598528/94765736 [================>.............] - ETA: 0s
60071936/94765736 [==================>...........] - ETA: 0s
62201856/94765736 [==================>...........] - ETA: 0s
66101248/94765736 [===================>..........] - ETA: 0s
69500928/94765736 [=====================>........] - ETA: 0s
75505664/94765736 [======================>.......] - ETA: 0s
80748544/94765736 [========================>.....] - ETA: 0s
86007808/94765736 [==========================>...] - ETA: 0s
87777280/94765736 [==========================>...] - ETA: 0s
92340224/94765736 [============================>.] - ETA: 0s
94765056/94765736 [============================>.] - ETA: 0s
94773248/94765736 [==============================] - 1s 0us/step
Model: "resnet50"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
Tempo de Inicializacao: 12.441866397857666
Train on 50000 samples, validate on 10000 samples

on_epoch_begin: 1594187211.154039s

Real time: 1594187211.1540647
Epoch 1/5

on_train_batch_begin: 1594187211.984108s

on_train_batch_end: 1594187238.184312s

 2048/50000 [>.............................] - ETA: 10:32 - loss: 17.9298 - accuracy: 2.4629e-04
on_train_batch_begin: 1594187238.185746s

1 step training time: 26.201638s

on_train_batch_end: 1594187239.618082s

 4096/50000 [=>............................] - ETA: 5:18 - loss: 14.0846 - accuracy: 3.0863e-04 
on_train_batch_begin: 1594187239.618497s

2 step training time: 1.432751s

on_train_batch_end: 1594187241.065147s

 6144/50000 [==>...........................] - ETA: 3:33 - loss: 12.2121 - accuracy: 6.7250e-04
on_train_batch_begin: 1594187241.065572s

3 step training time: 1.447075s

on_train_batch_end: 1594187242.499537s

 8192/50000 [===>..........................] - ETA: 2:39 - loss: 11.1841 - accuracy: 0.0021    
on_train_batch_begin: 1594187242.499945s

4 step training time: 1.434373s

on_train_batch_end: 1594187243.934809s

10240/50000 [=====>........................] - ETA: 2:07 - loss: 10.5276 - accuracy: 0.0060
on_train_batch_begin: 1594187243.935204s

5 step training time: 1.435258s

on_train_batch_end: 1594187245.369170s

12288/50000 [======>.......................] - ETA: 1:45 - loss: 10.0811 - accuracy: 0.0092
on_train_batch_begin: 1594187245.369583s

6 step training time: 1.434380s

on_train_batch_end: 1594187246.798623s

14336/50000 [=======>......................] - ETA: 1:28 - loss: 9.7746 - accuracy: 0.0103 
on_train_batch_begin: 1594187246.799038s

7 step training time: 1.429455s

on_train_batch_end: 1594187248.213028s

16384/50000 [========>.....................] - ETA: 1:16 - loss: 9.5076 - accuracy: 0.0127
on_train_batch_begin: 1594187248.213438s

8 step training time: 1.414400s

on_train_batch_end: 1594187249.641680s

18432/50000 [==========>...................] - ETA: 1:05 - loss: 9.2900 - accuracy: 0.0150
on_train_batch_begin: 1594187249.642092s

9 step training time: 1.428653s

on_train_batch_end: 1594187251.077535s

20480/50000 [===========>..................] - ETA: 57s - loss: 9.1099 - accuracy: 0.0175 
on_train_batch_begin: 1594187251.077945s

10 step training time: 1.435854s

on_train_batch_end: 1594187252.544159s

22528/50000 [============>.................] - ETA: 50s - loss: 8.9517 - accuracy: 0.0197
on_train_batch_begin: 1594187252.544579s

11 step training time: 1.466634s

on_train_batch_end: 1594187253.973465s

24576/50000 [=============>................] - ETA: 44s - loss: 8.8119 - accuracy: 0.0221
on_train_batch_begin: 1594187253.973881s

12 step training time: 1.429302s

on_train_batch_end: 1594187255.443107s

26624/50000 [==============>...............] - ETA: 38s - loss: 8.6804 - accuracy: 0.0249
on_train_batch_begin: 1594187255.443544s

13 step training time: 1.469663s

on_train_batch_end: 1594187256.878201s

28672/50000 [================>.............] - ETA: 34s - loss: 8.5593 - accuracy: 0.0280
on_train_batch_begin: 1594187256.878610s

14 step training time: 1.435066s

on_train_batch_end: 1594187258.322647s

30720/50000 [=================>............] - ETA: 29s - loss: 8.4478 - accuracy: 0.0305
on_train_batch_begin: 1594187258.323056s

15 step training time: 1.444445s

on_train_batch_end: 1594187259.774881s

32768/50000 [==================>...........] - ETA: 25s - loss: 8.3553 - accuracy: 0.0329
on_train_batch_begin: 1594187259.775278s

16 step training time: 1.452222s

on_train_batch_end: 1594187261.236561s

34816/50000 [===================>..........] - ETA: 21s - loss: 8.2616 - accuracy: 0.0347
on_train_batch_begin: 1594187261.236965s

17 step training time: 1.461688s

on_train_batch_end: 1594187262.673985s

36864/50000 [=====================>........] - ETA: 18s - loss: 8.1795 - accuracy: 0.0365
on_train_batch_begin: 1594187262.674382s

18 step training time: 1.437417s

on_train_batch_end: 1594187264.110277s

38912/50000 [======================>.......] - ETA: 15s - loss: 8.1008 - accuracy: 0.0384
on_train_batch_begin: 1594187264.110678s

19 step training time: 1.436296s

on_train_batch_end: 1594187265.543428s

40960/50000 [=======================>......] - ETA: 12s - loss: 8.0275 - accuracy: 0.0398
on_train_batch_begin: 1594187265.543860s

20 step training time: 1.433182s

on_train_batch_end: 1594187266.998013s

43008/50000 [========================>.....] - ETA: 9s - loss: 7.9590 - accuracy: 0.0414 
on_train_batch_begin: 1594187266.998427s

21 step training time: 1.454567s

on_train_batch_end: 1594187268.429255s

45056/50000 [==========================>...] - ETA: 6s - loss: 7.8919 - accuracy: 0.0430
on_train_batch_begin: 1594187268.429660s

22 step training time: 1.431233s

on_train_batch_end: 1594187269.875500s

47104/50000 [===========================>..] - ETA: 3s - loss: 7.8286 - accuracy: 0.0442
on_train_batch_begin: 1594187269.875904s

23 step training time: 1.446244s

on_train_batch_end: 1594187271.331831s

49152/50000 [============================>.] - ETA: 1s - loss: 7.7704 - accuracy: 0.0455
on_train_batch_begin: 1594187271.332237s

24 step training time: 1.456333s

on_train_batch_end: 1594187279.394915s

on_test_batch_begin: 1594187279.611272s

25 step training time: 8.279035s

on_epoch_end: 1594187286.471233s

Validation time: 6.859941s

Real time: 1594187286.471233s

Epoch time: 75.3171899318695s

50000/50000 [==============================] - 75s 2ms/sample - loss: 7.7445 - accuracy: 0.0457 - val_loss: 18131.6983 - val_accuracy: 0.0000e+00

on_epoch_begin: 1594187286.471531s

Real time: 1594187286.4715405
Epoch 2/5

on_train_batch_begin: 1594187286.475768s

on_train_batch_end: 1594187287.936217s

 2048/50000 [>.............................] - ETA: 34s - loss: 6.1016 - accuracy: 0.0769
on_train_batch_begin: 1594187287.936632s

1 step training time: 1.460864s

on_train_batch_end: 1594187289.371492s

 4096/50000 [=>............................] - ETA: 32s - loss: 6.0599 - accuracy: 0.0756
on_train_batch_begin: 1594187289.371902s

2 step training time: 1.435270s

on_train_batch_end: 1594187290.834780s

 6144/50000 [==>...........................] - ETA: 31s - loss: 6.0092 - accuracy: 0.0776
on_train_batch_begin: 1594187290.835228s

3 step training time: 1.463326s

on_train_batch_end: 1594187292.304337s

 8192/50000 [===>..........................] - ETA: 29s - loss: 6.0158 - accuracy: 0.0751
on_train_batch_begin: 1594187292.304739s

4 step training time: 1.469510s

on_train_batch_end: 1594187293.775127s

10240/50000 [=====>........................] - ETA: 28s - loss: 5.9770 - accuracy: 0.0742
on_train_batch_begin: 1594187293.775557s

5 step training time: 1.470818s

on_train_batch_end: 1594187295.211048s

12288/50000 [======>.......................] - ETA: 26s - loss: 5.9372 - accuracy: 0.0747
on_train_batch_begin: 1594187295.211445s

6 step training time: 1.435888s

on_train_batch_end: 1594187296.661977s

14336/50000 [=======>......................] - ETA: 25s - loss: 5.9015 - accuracy: 0.0731
on_train_batch_begin: 1594187296.662379s

7 step training time: 1.450934s

on_train_batch_end: 1594187298.115381s

16384/50000 [========>.....................] - ETA: 23s - loss: 5.8569 - accuracy: 0.0719
on_train_batch_begin: 1594187298.115831s

8 step training time: 1.453452s

on_train_batch_end: 1594187299.572814s

18432/50000 [==========>...................] - ETA: 22s - loss: 5.8324 - accuracy: 0.0713
on_train_batch_begin: 1594187299.573232s

9 step training time: 1.457401s

on_train_batch_end: 1594187301.027075s

20480/50000 [===========>..................] - ETA: 20s - loss: 5.8095 - accuracy: 0.0716
on_train_batch_begin: 1594187301.027530s

10 step training time: 1.454298s

on_train_batch_end: 1594187302.470582s

22528/50000 [============>.................] - ETA: 19s - loss: 5.7786 - accuracy: 0.0723
on_train_batch_begin: 1594187302.470995s

11 step training time: 1.443465s

on_train_batch_end: 1594187303.929844s

24576/50000 [=============>................] - ETA: 18s - loss: 5.7515 - accuracy: 0.0711
on_train_batch_begin: 1594187303.930244s

12 step training time: 1.459249s

on_train_batch_end: 1594187305.409034s

26624/50000 [==============>...............] - ETA: 16s - loss: 5.7215 - accuracy: 0.0698
on_train_batch_begin: 1594187305.409440s

13 step training time: 1.479196s

on_train_batch_end: 1594187306.862971s

28672/50000 [================>.............] - ETA: 15s - loss: 5.6931 - accuracy: 0.0699
on_train_batch_begin: 1594187306.863376s

14 step training time: 1.453936s

on_train_batch_end: 1594187308.320416s

30720/50000 [=================>............] - ETA: 13s - loss: 5.6508 - accuracy: 0.0699
on_train_batch_begin: 1594187308.320824s

15 step training time: 1.457448s

on_train_batch_end: 1594187309.766186s

32768/50000 [==================>...........] - ETA: 12s - loss: 5.6176 - accuracy: 0.0689
on_train_batch_begin: 1594187309.766586s

16 step training time: 1.445762s

on_train_batch_end: 1594187311.224330s

34816/50000 [===================>..........] - ETA: 10s - loss: 5.5739 - accuracy: 0.0684
on_train_batch_begin: 1594187311.224779s

17 step training time: 1.458192s

on_train_batch_end: 1594187312.667935s

36864/50000 [=====================>........] - ETA: 9s - loss: 5.5288 - accuracy: 0.0681 
on_train_batch_begin: 1594187312.668340s

18 step training time: 1.443562s

on_train_batch_end: 1594187314.132059s

38912/50000 [======================>.......] - ETA: 7s - loss: 5.4887 - accuracy: 0.0678
on_train_batch_begin: 1594187314.132461s

19 step training time: 1.464121s

on_train_batch_end: 1594187315.594898s

40960/50000 [=======================>......] - ETA: 6s - loss: 5.4456 - accuracy: 0.0676
on_train_batch_begin: 1594187315.595297s

20 step training time: 1.462836s

on_train_batch_end: 1594187317.072018s

43008/50000 [========================>.....] - ETA: 4s - loss: 5.4063 - accuracy: 0.0673
on_train_batch_begin: 1594187317.072417s

21 step training time: 1.477120s

on_train_batch_end: 1594187318.510161s

45056/50000 [==========================>...] - ETA: 3s - loss: 5.3656 - accuracy: 0.0673
on_train_batch_begin: 1594187318.510559s

22 step training time: 1.438143s

on_train_batch_end: 1594187319.983710s

47104/50000 [===========================>..] - ETA: 2s - loss: 5.3209 - accuracy: 0.0673
on_train_batch_begin: 1594187319.984110s

23 step training time: 1.473551s

on_train_batch_end: 1594187321.442549s

49152/50000 [============================>.] - ETA: 0s - loss: 5.2764 - accuracy: 0.0675
on_train_batch_begin: 1594187321.442952s

24 step training time: 1.458843s

on_train_batch_end: 1594187322.077980s

on_test_batch_begin: 1594187322.103062s

25 step training time: 0.660109s

on_epoch_end: 1594187324.046865s

Validation time: 1.943784s

Real time: 1594187324.046865s

Epoch time: 37.57534623146057s

50000/50000 [==============================] - 38s 752us/sample - loss: 5.2601 - accuracy: 0.0675 - val_loss: 108.7260 - val_accuracy: 0.0000e+00

on_epoch_begin: 1594187324.047134s

Real time: 1594187324.0471437
Epoch 3/5

on_train_batch_begin: 1594187324.051432s

on_train_batch_end: 1594187325.516049s

 2048/50000 [>.............................] - ETA: 34s - loss: 4.1227 - accuracy: 0.0734
on_train_batch_begin: 1594187325.516455s

1 step training time: 1.465023s

on_train_batch_end: 1594187326.970168s

 4096/50000 [=>............................] - ETA: 32s - loss: 4.0585 - accuracy: 0.0758
on_train_batch_begin: 1594187326.970562s

2 step training time: 1.454107s

on_train_batch_end: 1594187328.428216s

 6144/50000 [==>...........................] - ETA: 31s - loss: 3.9933 - accuracy: 0.0772
on_train_batch_begin: 1594187328.428631s

3 step training time: 1.458070s

on_train_batch_end: 1594187329.892929s

 8192/50000 [===>..........................] - ETA: 29s - loss: 3.9392 - accuracy: 0.0786
on_train_batch_begin: 1594187329.893322s

4 step training time: 1.464691s

on_train_batch_end: 1594187331.363162s

10240/50000 [=====>........................] - ETA: 28s - loss: 3.8727 - accuracy: 0.0799
on_train_batch_begin: 1594187331.363620s

5 step training time: 1.470297s

on_train_batch_end: 1594187332.824659s

12288/50000 [======>.......................] - ETA: 26s - loss: 3.8054 - accuracy: 0.0810
on_train_batch_begin: 1594187332.825055s

6 step training time: 1.461435s

on_train_batch_end: 1594187334.280604s

14336/50000 [=======>......................] - ETA: 25s - loss: 3.7659 - accuracy: 0.0821
on_train_batch_begin: 1594187334.280995s

7 step training time: 1.455940s

on_train_batch_end: 1594187335.746790s

16384/50000 [========>.....................] - ETA: 24s - loss: 3.7281 - accuracy: 0.0830
on_train_batch_begin: 1594187335.747182s

8 step training time: 1.466187s

on_train_batch_end: 1594187337.198065s

18432/50000 [==========>...................] - ETA: 22s - loss: 3.6714 - accuracy: 0.0838
on_train_batch_begin: 1594187337.198462s

9 step training time: 1.451281s

on_train_batch_end: 1594187338.644883s

20480/50000 [===========>..................] - ETA: 21s - loss: 3.6325 - accuracy: 0.0846
on_train_batch_begin: 1594187338.645273s

10 step training time: 1.446810s

on_train_batch_end: 1594187340.093168s

22528/50000 [============>.................] - ETA: 19s - loss: 3.5852 - accuracy: 0.0854
on_train_batch_begin: 1594187340.093557s

11 step training time: 1.448285s

on_train_batch_end: 1594187341.574242s

24576/50000 [=============>................] - ETA: 18s - loss: 3.5314 - accuracy: 0.0864
on_train_batch_begin: 1594187341.574663s

12 step training time: 1.481106s

on_train_batch_end: 1594187343.022631s

26624/50000 [==============>...............] - ETA: 16s - loss: 3.4827 - accuracy: 0.0871
on_train_batch_begin: 1594187343.023041s

13 step training time: 1.448379s

on_train_batch_end: 1594187344.489476s

28672/50000 [================>.............] - ETA: 15s - loss: 3.4307 - accuracy: 0.0878
on_train_batch_begin: 1594187344.489884s

14 step training time: 1.466842s

on_train_batch_end: 1594187345.955945s

30720/50000 [=================>............] - ETA: 13s - loss: 3.3847 - accuracy: 0.0884
on_train_batch_begin: 1594187345.956344s

15 step training time: 1.466460s

on_train_batch_end: 1594187347.408901s

32768/50000 [==================>...........] - ETA: 12s - loss: 3.3411 - accuracy: 0.0891
on_train_batch_begin: 1594187347.409299s

16 step training time: 1.452954s

on_train_batch_end: 1594187348.869344s

34816/50000 [===================>..........] - ETA: 10s - loss: 3.2989 - accuracy: 0.0896
on_train_batch_begin: 1594187348.869742s

17 step training time: 1.460443s

on_train_batch_end: 1594187350.320503s

36864/50000 [=====================>........] - ETA: 9s - loss: 3.2661 - accuracy: 0.0901 
on_train_batch_begin: 1594187350.320902s

18 step training time: 1.451160s

on_train_batch_end: 1594187351.794185s

38912/50000 [======================>.......] - ETA: 7s - loss: 3.2216 - accuracy: 0.0906
on_train_batch_begin: 1594187351.794590s

19 step training time: 1.473688s

on_train_batch_end: 1594187353.237420s

40960/50000 [=======================>......] - ETA: 6s - loss: 3.1863 - accuracy: 0.0910
on_train_batch_begin: 1594187353.237828s

20 step training time: 1.443238s

on_train_batch_end: 1594187354.709841s

43008/50000 [========================>.....] - ETA: 4s - loss: 3.1532 - accuracy: 0.0915
on_train_batch_begin: 1594187354.710246s

21 step training time: 1.472418s

on_train_batch_end: 1594187356.162392s

45056/50000 [==========================>...] - ETA: 3s - loss: 3.1174 - accuracy: 0.0918
on_train_batch_begin: 1594187356.162802s

22 step training time: 1.452555s

on_train_batch_end: 1594187357.636173s

47104/50000 [===========================>..] - ETA: 2s - loss: 3.0876 - accuracy: 0.0922
on_train_batch_begin: 1594187357.636575s

23 step training time: 1.473773s

on_train_batch_end: 1594187359.077897s

49152/50000 [============================>.] - ETA: 0s - loss: 3.0579 - accuracy: 0.0925
on_train_batch_begin: 1594187359.078298s

24 step training time: 1.441723s

on_train_batch_end: 1594187359.725392s

on_test_batch_begin: 1594187359.749202s

25 step training time: 0.670905s

on_epoch_end: 1594187361.682641s

Validation time: 1.933424s

Real time: 1594187361.682641s

Epoch time: 37.63551902770996s

50000/50000 [==============================] - 38s 753us/sample - loss: 3.0486 - accuracy: 0.0926 - val_loss: 7.4294 - val_accuracy: 0.0000e+00

on_epoch_begin: 1594187361.682905s

Real time: 1594187361.6829143
Epoch 4/5

on_train_batch_begin: 1594187361.687125s

on_train_batch_end: 1594187363.139993s

 2048/50000 [>.............................] - ETA: 34s - loss: 2.2877 - accuracy: 0.1001
on_train_batch_begin: 1594187363.140391s

1 step training time: 1.453266s

on_train_batch_end: 1594187364.580627s

 4096/50000 [=>............................] - ETA: 32s - loss: 2.2612 - accuracy: 0.0998
on_train_batch_begin: 1594187364.581027s

2 step training time: 1.440636s

on_train_batch_end: 1594187366.049129s

 6144/50000 [==>...........................] - ETA: 31s - loss: 2.2779 - accuracy: 0.0997
on_train_batch_begin: 1594187366.049530s

3 step training time: 1.468503s

on_train_batch_end: 1594187367.495447s

 8192/50000 [===>..........................] - ETA: 29s - loss: 2.3199 - accuracy: 0.0999
on_train_batch_begin: 1594187367.495898s

4 step training time: 1.446368s

on_train_batch_end: 1594187368.958966s

10240/50000 [=====>........................] - ETA: 28s - loss: 2.3249 - accuracy: 0.1000
on_train_batch_begin: 1594187368.959369s

5 step training time: 1.463472s

on_train_batch_end: 1594187370.410978s

12288/50000 [======>.......................] - ETA: 26s - loss: 2.2986 - accuracy: 0.1000
on_train_batch_begin: 1594187370.411381s

6 step training time: 1.452011s

on_train_batch_end: 1594187371.885508s

14336/50000 [=======>......................] - ETA: 25s - loss: 2.3080 - accuracy: 0.0999
on_train_batch_begin: 1594187371.885915s

7 step training time: 1.474535s

on_train_batch_end: 1594187373.340015s

16384/50000 [========>.....................] - ETA: 23s - loss: 2.3294 - accuracy: 0.0999
on_train_batch_begin: 1594187373.340419s

8 step training time: 1.454504s

on_train_batch_end: 1594187374.810147s

18432/50000 [==========>...................] - ETA: 22s - loss: 2.3122 - accuracy: 0.0999
on_train_batch_begin: 1594187374.810540s

9 step training time: 1.470122s

on_train_batch_end: 1594187376.282715s

20480/50000 [===========>..................] - ETA: 21s - loss: 2.3116 - accuracy: 0.0999
on_train_batch_begin: 1594187376.283121s

10 step training time: 1.472581s

on_train_batch_end: 1594187377.751484s

22528/50000 [============>.................] - ETA: 19s - loss: 2.3195 - accuracy: 0.0999
on_train_batch_begin: 1594187377.751881s

11 step training time: 1.468760s

on_train_batch_end: 1594187379.217823s

24576/50000 [=============>................] - ETA: 18s - loss: 2.3139 - accuracy: 0.1000
on_train_batch_begin: 1594187379.218222s

12 step training time: 1.466341s

on_train_batch_end: 1594187380.703031s

26624/50000 [==============>...............] - ETA: 16s - loss: 2.3130 - accuracy: 0.1000
on_train_batch_begin: 1594187380.703431s

13 step training time: 1.485209s

on_train_batch_end: 1594187382.182973s

28672/50000 [================>.............] - ETA: 15s - loss: 2.2979 - accuracy: 0.1000
on_train_batch_begin: 1594187382.183380s

14 step training time: 1.479949s

on_train_batch_end: 1594187383.661636s

30720/50000 [=================>............] - ETA: 13s - loss: 2.2912 - accuracy: 0.0999
on_train_batch_begin: 1594187383.662037s

15 step training time: 1.478657s

on_train_batch_end: 1594187385.142811s

32768/50000 [==================>...........] - ETA: 12s - loss: 2.2881 - accuracy: 0.0999
on_train_batch_begin: 1594187385.143228s

16 step training time: 1.481191s

on_train_batch_end: 1594187386.611969s

34816/50000 [===================>..........] - ETA: 10s - loss: 2.2691 - accuracy: 0.0999
on_train_batch_begin: 1594187386.612369s

17 step training time: 1.469141s

on_train_batch_end: 1594187388.063870s

36864/50000 [=====================>........] - ETA: 9s - loss: 2.2640 - accuracy: 0.0999 
on_train_batch_begin: 1594187388.064268s

18 step training time: 1.451900s

on_train_batch_end: 1594187389.532886s

38912/50000 [======================>.......] - ETA: 7s - loss: 2.2598 - accuracy: 0.1000
on_train_batch_begin: 1594187389.533286s

19 step training time: 1.469018s

on_train_batch_end: 1594187390.979505s

40960/50000 [=======================>......] - ETA: 6s - loss: 2.2542 - accuracy: 0.1000
on_train_batch_begin: 1594187390.979914s

20 step training time: 1.446628s

on_train_batch_end: 1594187392.431845s

43008/50000 [========================>.....] - ETA: 4s - loss: 2.2476 - accuracy: 0.1000
on_train_batch_begin: 1594187392.432255s

21 step training time: 1.452341s

on_train_batch_end: 1594187393.900203s

45056/50000 [==========================>...] - ETA: 3s - loss: 2.2485 - accuracy: 0.1000
on_train_batch_begin: 1594187393.900603s

22 step training time: 1.468348s

on_train_batch_end: 1594187395.378489s

47104/50000 [===========================>..] - ETA: 2s - loss: 2.2393 - accuracy: 0.1000
on_train_batch_begin: 1594187395.378884s

23 step training time: 1.478281s

on_train_batch_end: 1594187396.823239s

49152/50000 [============================>.] - ETA: 0s - loss: 2.2422 - accuracy: 0.1000
on_train_batch_begin: 1594187396.823668s

24 step training time: 1.444785s

on_train_batch_end: 1594187397.471623s

on_test_batch_begin: 1594187397.495821s

25 step training time: 0.672152s

on_epoch_end: 1594187399.439924s

Validation time: 1.944083s

Real time: 1594187399.439924s

Epoch time: 37.75703167915344s

50000/50000 [==============================] - 38s 755us/sample - loss: 2.2422 - accuracy: 0.1000 - val_loss: 7.3487 - val_accuracy: 0.0720

on_epoch_begin: 1594187399.440186s

Real time: 1594187399.4401958
Epoch 5/5

on_train_batch_begin: 1594187399.444583s

on_train_batch_end: 1594187400.918860s

 2048/50000 [>.............................] - ETA: 34s - loss: 1.9482 - accuracy: 0.1001
on_train_batch_begin: 1594187400.919258s

1 step training time: 1.474675s

on_train_batch_end: 1594187402.396183s

 4096/50000 [=>............................] - ETA: 33s - loss: 1.9367 - accuracy: 0.1002
on_train_batch_begin: 1594187402.396578s

2 step training time: 1.477320s

on_train_batch_end: 1594187403.846405s

 6144/50000 [==>...........................] - ETA: 31s - loss: 1.9950 - accuracy: 0.1001
on_train_batch_begin: 1594187403.846797s

3 step training time: 1.450219s

on_train_batch_end: 1594187405.316962s

 8192/50000 [===>..........................] - ETA: 29s - loss: 2.0056 - accuracy: 0.0999
on_train_batch_begin: 1594187405.317355s

4 step training time: 1.470558s

on_train_batch_end: 1594187406.778014s

10240/50000 [=====>........................] - ETA: 28s - loss: 2.0047 - accuracy: 0.0999
on_train_batch_begin: 1594187406.778416s

5 step training time: 1.461061s

on_train_batch_end: 1594187408.226388s

12288/50000 [======>.......................] - ETA: 26s - loss: 1.9903 - accuracy: 0.0998
on_train_batch_begin: 1594187408.226799s

6 step training time: 1.448382s

on_train_batch_end: 1594187409.699244s

14336/50000 [=======>......................] - ETA: 25s - loss: 1.9892 - accuracy: 0.0999
on_train_batch_begin: 1594187409.699679s

7 step training time: 1.472881s

on_train_batch_end: 1594187411.165067s

16384/50000 [========>.....................] - ETA: 24s - loss: 1.9834 - accuracy: 0.0999
on_train_batch_begin: 1594187411.165456s

8 step training time: 1.465776s

on_train_batch_end: 1594187412.647213s

18432/50000 [==========>...................] - ETA: 22s - loss: 1.9610 - accuracy: 0.0999
on_train_batch_begin: 1594187412.647638s

9 step training time: 1.482182s

on_train_batch_end: 1594187414.121300s

20480/50000 [===========>..................] - ETA: 21s - loss: 1.9386 - accuracy: 0.0999
on_train_batch_begin: 1594187414.121693s

10 step training time: 1.474056s

on_train_batch_end: 1594187415.575547s

22528/50000 [============>.................] - ETA: 19s - loss: 1.9234 - accuracy: 0.0999
on_train_batch_begin: 1594187415.575946s

11 step training time: 1.454253s

on_train_batch_end: 1594187417.051847s

24576/50000 [=============>................] - ETA: 18s - loss: 1.9051 - accuracy: 0.0999
on_train_batch_begin: 1594187417.052254s

12 step training time: 1.476308s

on_train_batch_end: 1594187418.509072s

26624/50000 [==============>...............] - ETA: 16s - loss: 1.8934 - accuracy: 0.0999
on_train_batch_begin: 1594187418.509475s

13 step training time: 1.457221s

on_train_batch_end: 1594187419.966476s

28672/50000 [================>.............] - ETA: 15s - loss: 1.8894 - accuracy: 0.0999
on_train_batch_begin: 1594187419.966883s

14 step training time: 1.457407s

on_train_batch_end: 1594187421.422469s

30720/50000 [=================>............] - ETA: 13s - loss: 1.8712 - accuracy: 0.0999
on_train_batch_begin: 1594187421.422868s

15 step training time: 1.455986s

on_train_batch_end: 1594187422.892584s

32768/50000 [==================>...........] - ETA: 12s - loss: 1.8505 - accuracy: 0.1000
on_train_batch_begin: 1594187422.892994s

16 step training time: 1.470126s

on_train_batch_end: 1594187424.375007s

34816/50000 [===================>..........] - ETA: 10s - loss: 1.8358 - accuracy: 0.1000
on_train_batch_begin: 1594187424.375405s

17 step training time: 1.482410s

on_train_batch_end: 1594187425.838037s

36864/50000 [=====================>........] - ETA: 9s - loss: 1.8191 - accuracy: 0.1000 
on_train_batch_begin: 1594187425.838441s

18 step training time: 1.463036s

on_train_batch_end: 1594187427.304962s

38912/50000 [======================>.......] - ETA: 7s - loss: 1.8092 - accuracy: 0.1000
on_train_batch_begin: 1594187427.305415s

19 step training time: 1.466974s

on_train_batch_end: 1594187428.774118s

40960/50000 [=======================>......] - ETA: 6s - loss: 1.7906 - accuracy: 0.1000
on_train_batch_begin: 1594187428.774525s

20 step training time: 1.469109s

on_train_batch_end: 1594187430.252231s

43008/50000 [========================>.....] - ETA: 5s - loss: 1.7781 - accuracy: 0.1000
on_train_batch_begin: 1594187430.252638s

21 step training time: 1.478113s

on_train_batch_end: 1594187431.733116s

45056/50000 [==========================>...] - ETA: 3s - loss: 1.7631 - accuracy: 0.1000
on_train_batch_begin: 1594187431.733523s

22 step training time: 1.480885s

on_train_batch_end: 1594187433.190178s

47104/50000 [===========================>..] - ETA: 2s - loss: 1.7529 - accuracy: 0.1000
on_train_batch_begin: 1594187433.190577s

23 step training time: 1.457054s

on_train_batch_end: 1594187434.652196s

49152/50000 [============================>.] - ETA: 0s - loss: 1.7422 - accuracy: 0.1000
on_train_batch_begin: 1594187434.652600s

24 step training time: 1.462023s

on_train_batch_end: 1594187435.294173s

on_test_batch_begin: 1594187435.319747s

25 step training time: 0.667147s

on_epoch_end: 1594187437.267738s

Validation time: 1.947972s

Real time: 1594187437.267738s

Epoch time: 37.827563762664795s

50000/50000 [==============================] - 38s 757us/sample - loss: 1.7354 - accuracy: 0.1000 - val_loss: 6.8457 - val_accuracy: 0.0998
Tempo do fit: 229.96958208084106

Training complete.
